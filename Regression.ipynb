{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7973bf18-c881-4e52-b75f-0487b9bb9690",
   "metadata": {},
   "source": [
    "# Regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10a4cf52-0a67-4d9b-9668-f454fc2f03f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # question 1 >> What is Simple Linear Regression\n",
    "\n",
    "# Simple linear regression is a statistical method used to model the relationship between two variables. It assumes that one variable (the dependent variable) is linearly dependent on another (the independent variable). In this context, \"linear\" refers to the fact that the relationship between the variables is represented by a straight line.\n",
    "\n",
    "# ### Key Points:\n",
    "# - **Dependent variable**: This is the variable you're trying to predict or explain (often denoted as \\( Y \\)).\n",
    "# - **Independent variable**: This is the variable you're using to make predictions (often denoted as \\( X \\)).\n",
    "\n",
    "# The goal of simple linear regression is to find the best-fitting straight line (called the \"regression line\") that explains the relationship between the two variables. This line is typically represented by the equation:\n",
    "\n",
    "# \\[\n",
    "# Y = \\beta_0 + \\beta_1X + \\epsilon\n",
    "# \\]\n",
    "\n",
    "# Where:\n",
    "# - \\( Y \\) is the predicted value of the dependent variable.\n",
    "# - \\( \\beta_0 \\) is the **intercept**, which is the value of \\( Y \\) when \\( X = 0 \\).\n",
    "# - \\( \\beta_1 \\) is the **slope**, which shows how much \\( Y \\) changes for a unit change in \\( X \\).\n",
    "# - \\( X \\) is the independent variable.\n",
    "# - \\( \\epsilon \\) is the error term, accounting for variability in \\( Y \\) that can’t be explained by \\( X \\).\n",
    "\n",
    "# ### Purpose:\n",
    "# Simple linear regression is used to:\n",
    "# - **Predict** the value of the dependent variable (\\( Y \\)) given a specific value of the independent variable (\\( X \\)).\n",
    "# - **Understand** the strength and direction of the relationship between the two variables.\n",
    "\n",
    "# ### Example:\n",
    "# If you are studying the relationship between the number of hours studied (independent variable \\( X \\)) and test scores (dependent variable \\( Y \\)), simple linear regression can help you predict test scores based on the hours studied.\n",
    "\n",
    "# The slope (\\( \\beta_1 \\)) tells you how much the test score is expected to increase for each additional hour studied, while the intercept (\\( \\beta_0 \\)) gives the predicted test score when no hours are studied (i.e., \\( X = 0 \\)).\n",
    "\n",
    "# ### Key Assumptions of Simple Linear Regression:\n",
    "# 1. **Linearity**: There is a linear relationship between the independent and dependent variables.\n",
    "# 2. **Independence**: Observations are independent of each other.\n",
    "# 3. **Homoscedasticity**: The variance of the error terms (\\( \\epsilon \\)) is constant across all levels of the independent variable.\n",
    "# 4. **Normality**: The residuals (errors) are normally distributed.\n",
    "\n",
    "# This method is widely used in statistics, machine learning, economics, and many other fields for its simplicity and interpretability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e376f542-e820-4f80-8c41-75b716a9b1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # question 2 >> What are the key assumptions of Simple Linear Regression\n",
    "\n",
    "# In Simple Linear Regression, there are several key assumptions that need to be met for the model to be valid and provide reliable results. These assumptions ensure the quality of the estimates and predictions made by the regression model. The main assumptions are:\n",
    "\n",
    "# ### 1. **Linearity**\n",
    "#    - The relationship between the independent variable (\\(X\\)) and the dependent variable (\\(Y\\)) is assumed to be **linear**. This means the change in \\(Y\\) is directly proportional to the change in \\(X\\), and this relationship can be described by a straight line.\n",
    "#    - If this assumption is violated (i.e., if the relationship is not linear), the regression model may provide inaccurate predictions.\n",
    "\n",
    "# ### 2. **Independence of Errors**\n",
    "#    - The errors (or residuals) — which are the differences between the observed values of \\(Y\\) and the predicted values — must be **independent** of each other.\n",
    "#    - This means the error for one data point should not provide any information about the error for another data point. Violation of this assumption can occur in time-series data or data with clustering, leading to misleading results.\n",
    "\n",
    "# ### 3. **Homoscedasticity (Constant Variance of Errors)**\n",
    "#    - The variance of the errors (or residuals) should be **constant** across all levels of the independent variable \\(X\\). This is known as **homoscedasticity**.\n",
    "#    - In simple terms, the spread or dispersion of residuals should be roughly the same regardless of the value of \\(X\\). If the variance of the errors increases or decreases as \\(X\\) changes, it leads to **heteroscedasticity**, which can affect the reliability of the model.\n",
    "\n",
    "# ### 4. **Normality of Errors**\n",
    "#    - The errors (or residuals) should be **normally distributed**. This assumption is particularly important for hypothesis testing and confidence intervals.\n",
    "#    - If the errors are not normally distributed, statistical tests (such as t-tests) used to assess the significance of the regression parameters may not be valid.\n",
    "\n",
    "# ### 5. **No Multicollinearity**\n",
    "#    - This assumption is more relevant for multiple linear regression, but it's worth mentioning in the context of simple linear regression as well. It refers to the idea that the independent variable \\(X\\) should not be perfectly correlated with any other variables (in the case of simple linear regression, there should be just one independent variable).\n",
    "#    - This assumption ensures that the independent variable truly explains the variation in the dependent variable and is not redundant.\n",
    "\n",
    "# ### 6. **Measurement of Variables**\n",
    "#    - The variables (both independent and dependent) are measured without error. In reality, measurement errors in either the independent or dependent variable can lead to biased or inconsistent estimates.\n",
    "\n",
    "# ### Summary of Assumptions:\n",
    "# 1. **Linearity**: The relationship between \\(X\\) and \\(Y\\) is linear.\n",
    "# 2. **Independence of errors**: Errors are independent of each other.\n",
    "# 3. **Homoscedasticity**: The variance of errors is constant across levels of \\(X\\).\n",
    "# 4. **Normality of errors**: Errors are normally distributed.\n",
    "# 5. **No Multicollinearity**: In simple regression, this isn't a concern since there's only one independent variable.\n",
    "# 6. **No measurement errors**: The variables are measured accurately.\n",
    "\n",
    "# These assumptions help ensure that the linear regression model provides accurate and reliable results. If these assumptions are violated, the model's predictions may be biased or misleading, and statistical tests (like those used to test the significance of the coefficients) may no longer be valid.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "daf2c4a1-4a79-4ebd-863b-ac20bc6543dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # question 3 >> What does the coefficient m represent in the equation Y=mX+c\n",
    "\n",
    "# In the equation \\( Y = mX + c \\), the coefficient \\( m \\) represents the **slope** of the regression line.\n",
    "\n",
    "# ### Interpretation of \\( m \\) (the slope):\n",
    "# - The slope \\( m \\) tells us the **rate of change** of the dependent variable \\( Y \\) with respect to a one-unit change in the independent variable \\( X \\).\n",
    "# - Specifically, for every 1-unit increase in \\( X \\), \\( Y \\) will change by \\( m \\) units. The sign of \\( m \\) (whether positive or negative) indicates the direction of the relationship:\n",
    "#   - If \\( m > 0 \\), it means that \\( Y \\) increases as \\( X \\) increases (positive relationship).\n",
    "#   - If \\( m < 0 \\), it means that \\( Y \\) decreases as \\( X \\) increases (negative relationship).\n",
    "#   - If \\( m = 0 \\), it means there is **no change** in \\( Y \\) as \\( X \\) changes (horizontal line).\n",
    "\n",
    "# ### Example:\n",
    "# - If you are studying the relationship between hours studied (\\( X \\)) and exam scores (\\( Y \\)), and you find that \\( m = 2 \\), it means that for each additional hour of study, the exam score increases by 2 points.\n",
    "\n",
    "# Thus, the coefficient \\( m \\) quantifies the strength and direction of the linear relationship between the two variables.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73c89436-0f67-4360-aec1-60f5618bdfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # question 4 >> What does the intercept c represent in the equation Y=mX+c\n",
    "\n",
    "# In the equation \\( Y = mX + c \\), the **intercept** \\( c \\) represents the **point where the regression line crosses the \\( Y \\)-axis** (i.e., when \\( X = 0 \\)).\n",
    "\n",
    "# ### Interpretation of \\( c \\) (the intercept):\n",
    "# - The intercept \\( c \\) is the predicted value of \\( Y \\) when the independent variable \\( X \\) is equal to 0.\n",
    "# - It gives you the baseline or starting value of \\( Y \\) before any change in \\( X \\).\n",
    "# - The intercept shows where the line starts on the \\( Y \\)-axis when \\( X \\) is at zero.\n",
    "\n",
    "# ### Example:\n",
    "# Let’s say the equation is:\n",
    "# \\[\n",
    "# Y = 2X + 5\n",
    "# \\]\n",
    "# - Here, \\( m = 2 \\) (the slope) means that for each additional hour studied, the exam score increases by 2 points.\n",
    "# - The intercept \\( c = 5 \\) means that when \\( X = 0 \\) (i.e., when no hours are studied), the predicted exam score is 5.\n",
    "\n",
    "# So, the intercept \\( c \\) represents the value of \\( Y \\) when the independent variable \\( X \\) has no influence (or is at its starting point, typically 0).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da20903a-509b-4e6d-be41-0929f824f71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # question 5 >> How do we calculate the slope m in Simple Linear Regression\n",
    "\n",
    "# In Simple Linear Regression, the slope \\( m \\) (also known as \\( \\beta_1 \\)) represents the rate of change in the dependent variable \\( Y \\) for each unit change in the independent variable \\( X \\). It can be calculated using the formula:\n",
    "\n",
    "# \\[\n",
    "# m = \\frac{n \\sum XY - \\sum X \\sum Y}{n \\sum X^2 - (\\sum X)^2}\n",
    "# \\]\n",
    "\n",
    "# Where:\n",
    "# - \\( n \\) = Number of data points\n",
    "# - \\( X \\) = The independent variable\n",
    "# - \\( Y \\) = The dependent variable\n",
    "# - \\( \\sum XY \\) = The sum of the product of each pair of \\( X \\) and \\( Y \\)\n",
    "# - \\( \\sum X \\) = The sum of all \\( X \\) values\n",
    "# - \\( \\sum Y \\) = The sum of all \\( Y \\) values\n",
    "# - \\( \\sum X^2 \\) = The sum of the squares of all \\( X \\) values\n",
    "\n",
    "# ### Step-by-Step Calculation:\n",
    "# 1. **Calculate the sums:**\n",
    "#    - \\( \\sum X \\) (sum of all \\( X \\) values)\n",
    "#    - \\( \\sum Y \\) (sum of all \\( Y \\) values)\n",
    "#    - \\( \\sum X^2 \\) (sum of the squares of all \\( X \\) values)\n",
    "#    - \\( \\sum XY \\) (sum of the product of each corresponding pair of \\( X \\) and \\( Y \\))\n",
    "\n",
    "# 2. **Substitute these sums into the formula** and solve for \\( m \\).\n",
    "\n",
    "# ### Example:\n",
    "# Let’s assume you have the following data:\n",
    "\n",
    "# | \\( X \\) | \\( Y \\) |\n",
    "# |-------|-------|\n",
    "# | 1     | 2     |\n",
    "# | 2     | 3     |\n",
    "# | 3     | 4     |\n",
    "# | 4     | 5     |\n",
    "# | 5     | 6     |\n",
    "\n",
    "# Now, we calculate each sum:\n",
    "# - \\( \\sum X = 1 + 2 + 3 + 4 + 5 = 15 \\)\n",
    "# - \\( \\sum Y = 2 + 3 + 4 + 5 + 6 = 20 \\)\n",
    "# - \\( \\sum X^2 = 1^2 + 2^2 + 3^2 + 4^2 + 5^2 = 1 + 4 + 9 + 16 + 25 = 55 \\)\n",
    "# - \\( \\sum XY = (1 \\times 2) + (2 \\times 3) + (3 \\times 4) + (4 \\times 5) + (5 \\times 6) = 2 + 6 + 12 + 20 + 30 = 70 \\)\n",
    "\n",
    "# Now, plug these values into the formula:\n",
    "\n",
    "# \\[\n",
    "# m = \\frac{5 \\times 70 - 15 \\times 20}{5 \\times 55 - 15^2}\n",
    "# \\]\n",
    "\n",
    "# \\[\n",
    "# m = \\frac{350 - 300}{275 - 225} = \\frac{50}{50} = 1\n",
    "# \\]\n",
    "\n",
    "# Thus, the slope \\( m \\) is **1**.\n",
    "\n",
    "# This means that for every 1 unit increase in \\( X \\), the value of \\( Y \\) increases by 1 unit.\n",
    "\n",
    "# ### Key Takeaway:\n",
    "# The slope \\( m \\) tells you how steep the regression line is and quantifies the relationship between the independent and dependent variables in a linear fashion.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a70e9e11-c9e1-40aa-a8ee-8e5b0833f169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # question 6 >> What is the purpose of the least squares method in Simple Linear Regression\n",
    "\n",
    "# The **least squares method** is used in Simple Linear Regression to find the best-fitting line that minimizes the errors (residuals) between the observed data points and the predicted values of the dependent variable. \n",
    "\n",
    "# ### Purpose of the Least Squares Method:\n",
    "\n",
    "# 1. **Minimize the Sum of Squared Errors**:\n",
    "#    - The primary goal of the least squares method is to minimize the **sum of the squared differences** (errors or residuals) between the observed values of the dependent variable \\( Y \\) and the predicted values of \\( Y \\) (based on the regression model). \n",
    "#    - This ensures that the line fits the data as closely as possible, with the smallest possible deviations.\n",
    "\n",
    "#    The formula for the residual for each data point is:\n",
    "#    \\[\n",
    "#    \\text{Residual} = Y_{\\text{observed}} - Y_{\\text{predicted}}\n",
    "#    \\]\n",
    "#    The least squares method minimizes the sum of the **squared residuals**:\n",
    "#    \\[\n",
    "#    \\text{Sum of squared residuals} = \\sum (Y_{\\text{observed}} - Y_{\\text{predicted}})^2\n",
    "#    \\]\n",
    "\n",
    "# 2. **Find the Best-Fitting Line**:\n",
    "#    - In simple linear regression, we are trying to find the equation of the line \\( Y = mX + c \\), where:\n",
    "#      - \\( m \\) is the slope of the line, and\n",
    "#      - \\( c \\) is the intercept.\n",
    "#    - The least squares method helps determine the values of \\( m \\) and \\( c \\) that minimize the sum of squared errors.\n",
    "\n",
    "# 3. **Optimization**:\n",
    "#    - By minimizing the sum of squared residuals, the least squares method ensures that the line is as close as possible to the actual data points. This leads to better predictions and a model that best represents the relationship between the independent variable \\( X \\) and the dependent variable \\( Y \\).\n",
    "\n",
    "# 4. **Accuracy of the Regression Model**:\n",
    "#    - The smaller the sum of squared residuals, the more accurate the regression model is in predicting the dependent variable \\( Y \\) from the independent variable \\( X \\).\n",
    "#    - The least squares method guarantees the optimal fitting line under the assumption of linearity, and it is the most commonly used method in regression analysis.\n",
    "\n",
    "# ### Why Squared Residuals?\n",
    "\n",
    "# - Squaring the residuals (errors) serves two purposes:\n",
    "#   1. **Eliminating Negative Values**: If the errors are not squared, positive and negative errors would cancel each other out. Squaring ensures that all errors are treated as positive, and they contribute positively to the sum.\n",
    "#   2. **Emphasizing Larger Errors**: Squaring increases the weight of larger errors, ensuring that the model prioritizes minimizing larger discrepancies between predicted and observed values.\n",
    "\n",
    "# ### Visual Understanding:\n",
    "\n",
    "# Imagine a scatter plot of data points and a straight line representing the regression model. The **residuals** are the vertical distances between each data point and the line. The least squares method finds the line that minimizes the total \"squared distance\" (residuals) between the points and the line, ensuring the best possible fit.\n",
    "\n",
    "# ### Conclusion:\n",
    "\n",
    "# The least squares method in Simple Linear Regression is essential because it ensures that the regression line is the \"best fit\" in terms of minimizing the discrepancy between the observed data points and the model’s predictions. This results in a model that can make accurate predictions and represents the underlying relationship between the variables effectively.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1fd31e7-70a2-4157-b686-4f782f8ff7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # question 7 >> How is the coefficient of determination (R²) interpreted in Simple Linear Regression\n",
    "\n",
    "# The **coefficient of determination**, denoted as \\( R^2 \\), is a key metric used to evaluate the performance of a Simple Linear Regression model. It represents the proportion of the variance in the dependent variable \\( Y \\) that is explained by the independent variable \\( X \\) in the regression model.\n",
    "\n",
    "# ### Interpretation of \\( R^2 \\):\n",
    "\n",
    "# 1. **Proportion of Variance Explained**:\n",
    "#    - \\( R^2 \\) tells you **how much of the variation in the dependent variable \\( Y \\)** can be explained by the independent variable \\( X \\). \n",
    "#    - It ranges from **0 to 1**:\n",
    "#      - \\( R^2 = 1 \\): This means that 100% of the variance in \\( Y \\) is explained by \\( X \\). The regression line perfectly fits the data, and there are no residuals (errors).\n",
    "#      - \\( R^2 = 0 \\): This means that the independent variable \\( X \\) does not explain any of the variance in \\( Y \\). The regression model has no predictive power and is not useful for explaining the relationship between \\( X \\) and \\( Y \\).\n",
    "\n",
    "# 2. **Formula**:\n",
    "#    The coefficient of determination is calculated as:\n",
    "#    \\[\n",
    "#    R^2 = 1 - \\frac{\\sum (Y_{\\text{observed}} - Y_{\\text{predicted}})^2}{\\sum (Y_{\\text{observed}} - \\bar{Y})^2}\n",
    "#    \\]\n",
    "#    Where:\n",
    "#    - \\( Y_{\\text{observed}} \\) are the actual values of the dependent variable \\( Y \\).\n",
    "#    - \\( Y_{\\text{predicted}} \\) are the predicted values based on the regression line.\n",
    "#    - \\( \\bar{Y} \\) is the mean of the actual values of \\( Y \\).\n",
    "\n",
    "# 3. **How to Interpret \\( R^2 \\)**:\n",
    "#    - **\\( R^2 = 0.85 \\)**: This means that **85%** of the variance in \\( Y \\) is explained by \\( X \\), and the remaining **15%** is due to factors not captured by the model (or random error).\n",
    "#    - **\\( R^2 = 0.50 \\)**: This indicates that **50%** of the variation in \\( Y \\) is explained by \\( X \\), and the model does not capture the remaining **50%** of the variance.\n",
    "\n",
    "# 4. **Goodness of Fit**:\n",
    "#    - A higher \\( R^2 \\) indicates a **better fit** of the model to the data. However, a high \\( R^2 \\) does not necessarily mean that the model is the best; it may still have issues such as overfitting or poor assumptions.\n",
    "#    - A lower \\( R^2 \\) indicates a **poor fit** and suggests that the independent variable \\( X \\) does not explain much of the variability in \\( Y \\). The model may not be appropriate for the data, or there may be other important variables that are not included in the model.\n",
    "\n",
    "# 5. **Limitations of \\( R^2 \\)**:\n",
    "#    - **Doesn’t Prove Causality**: \\( R^2 \\) measures the strength of the relationship but does not indicate causation between \\( X \\) and \\( Y \\).\n",
    "#    - **Sensitive to Outliers**: \\( R^2 \\) can be overly influenced by outliers or extreme values, which may lead to an exaggerated value that doesn’t reflect the true model fit.\n",
    "#    - **Doesn’t Address Model Assumptions**: A high \\( R^2 \\) does not guarantee that the model meets the key assumptions of linear regression (e.g., linearity, independence, homoscedasticity, etc.).\n",
    "\n",
    "# ### Example:\n",
    "# Suppose you are studying the relationship between hours studied (\\( X \\)) and exam scores (\\( Y \\)) and get an \\( R^2 \\) value of **0.92**. \n",
    "\n",
    "# - This means that 92% of the variance in exam scores is explained by the number of hours studied. The remaining 8% of the variance could be due to other factors not captured by the model, such as study methods, test anxiety, or other personal factors.\n",
    "# - With such a high \\( R^2 \\), you could say that the model provides a good fit and explains most of the variability in exam scores based on study hours.\n",
    "\n",
    "# ### In summary:\n",
    "# - \\( R^2 \\) gives you an idea of how well your regression model explains the variability in the dependent variable.\n",
    "# - A higher \\( R^2 \\) means a better fit, but it is not a definitive measure of the model’s quality, and it should be interpreted with caution.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76eb638d-5b82-4ad9-9668-6a1dd0c96d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # question 8 >> What is Multiple Linear Regression\n",
    "\n",
    "# **Multiple Linear Regression (MLR)** is an extension of simple linear regression that models the relationship between two or more independent variables (predictors) and a dependent variable. The goal of MLR is to explain how the dependent variable changes when any one of the independent variables changes, while keeping the other variables constant.\n",
    "\n",
    "# ### Key Concepts of Multiple Linear Regression:\n",
    "\n",
    "# In **simple linear regression**, we have just one independent variable \\(X\\), and the relationship with the dependent variable \\(Y\\) is expressed as:\n",
    "\n",
    "# \\[\n",
    "# Y = \\beta_0 + \\beta_1 X + \\epsilon\n",
    "# \\]\n",
    "\n",
    "# In **multiple linear regression**, we have more than one independent variable, say \\( X_1, X_2, X_3, \\dots, X_k \\). The equation becomes:\n",
    "\n",
    "# \\[\n",
    "# Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_k X_k + \\epsilon\n",
    "# \\]\n",
    "\n",
    "# Where:\n",
    "# - \\( Y \\) = Dependent variable (the outcome you are trying to predict or explain)\n",
    "# - \\( X_1, X_2, \\dots, X_k \\) = Independent variables (predictors or features)\n",
    "# - \\( \\beta_0 \\) = Intercept (value of \\( Y \\) when all \\( X \\)'s are 0)\n",
    "# - \\( \\beta_1, \\beta_2, \\dots, \\beta_k \\) = Coefficients (slopes) that measure the effect of each independent variable on \\( Y \\)\n",
    "# - \\( \\epsilon \\) = Error term (captures the variation in \\( Y \\) not explained by the model)\n",
    "\n",
    "# ### Purpose of Multiple Linear Regression:\n",
    "# - To understand the relationship between several independent variables and the dependent variable.\n",
    "# - To predict the value of \\( Y \\) based on the values of \\( X_1, X_2, \\dots, X_k \\).\n",
    "# - To determine the strength and nature (positive or negative) of the relationships between the independent variables and the dependent variable.\n",
    "\n",
    "# ### Assumptions of Multiple Linear Regression:\n",
    "# Like simple linear regression, MLR also comes with certain assumptions:\n",
    "# 1. **Linearity**: There is a linear relationship between the dependent variable and each independent variable.\n",
    "# 2. **Independence**: Observations are independent of each other.\n",
    "# 3. **Homoscedasticity**: The variance of the errors is constant across all levels of the independent variables.\n",
    "# 4. **No Multicollinearity**: The independent variables should not be highly correlated with each other. This can be tested using Variance Inflation Factor (VIF).\n",
    "# 5. **Normality of Errors**: The residuals (errors) should be normally distributed.\n",
    "\n",
    "# ### Interpreting the Coefficients:\n",
    "# In MLR, each coefficient \\( \\beta_1, \\beta_2, \\dots, \\beta_k \\) represents the change in \\( Y \\) associated with a **one-unit change** in the respective independent variable, assuming that all other variables are held constant.\n",
    "\n",
    "# For example:\n",
    "# - \\( \\beta_1 \\) represents the change in \\( Y \\) for a one-unit increase in \\( X_1 \\), while keeping \\( X_2, X_3, \\dots, X_k \\) constant.\n",
    "# - \\( \\beta_2 \\) represents the change in \\( Y \\) for a one-unit increase in \\( X_2 \\), while keeping \\( X_1, X_3, \\dots, X_k \\) constant.\n",
    "\n",
    "# ### Example:\n",
    "# Let’s say you are studying how the number of hours studied (\\( X_1 \\)) and the number of practice tests taken (\\( X_2 \\)) influence the exam score (\\( Y \\)):\n",
    "\n",
    "# \\[\n",
    "# Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\epsilon\n",
    "# \\]\n",
    "\n",
    "# Here:\n",
    "# - \\( Y \\) is the exam score.\n",
    "# - \\( X_1 \\) is the number of hours studied.\n",
    "# - \\( X_2 \\) is the number of practice tests taken.\n",
    "\n",
    "# If \\( \\beta_1 = 2 \\), this would mean that for each additional hour studied, the exam score increases by 2 points, holding the number of practice tests constant. Similarly, if \\( \\beta_2 = 3 \\), this would mean that for each additional practice test taken, the exam score increases by 3 points, holding the number of study hours constant.\n",
    "\n",
    "# ### How Multiple Linear Regression is Used:\n",
    "# 1. **Prediction**: MLR can be used to predict the value of \\( Y \\) based on known values of \\( X_1, X_2, \\dots, X_k \\). For example, predicting house prices based on factors like size, location, and number of rooms.\n",
    "# 2. **Insight into Relationships**: MLR helps identify how each independent variable influences the dependent variable, while controlling for the effects of other variables. For instance, understanding how both hours studied and practice tests affect exam scores simultaneously.\n",
    "# 3. **Variable Selection**: MLR can help in identifying which independent variables are most important in explaining the variation in the dependent variable (using methods like stepwise regression or regularization techniques).\n",
    "\n",
    "# ### Conclusion:\n",
    "# Multiple Linear Regression is a powerful statistical tool used for modeling and predicting the relationship between a dependent variable and multiple independent variables. It provides insights into the effects of several predictors on the outcome and is widely used in fields like economics, engineering, social sciences, and marketing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6e74d72-004a-4aaa-a019-eca30f472ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Question 9 >> What is the main difference between Simple and Multiple Linear Regression\n",
    "\n",
    "# The main difference between **Simple Linear Regression** and **Multiple Linear Regression** lies in the number of independent variables used to predict the dependent variable.\n",
    "\n",
    "# ### 1. **Number of Independent Variables**:\n",
    "#    - **Simple Linear Regression** involves **only one independent variable** (predictor) and aims to model the relationship between this single independent variable and the dependent variable.\n",
    "#      - **Equation**: \\( Y = \\beta_0 + \\beta_1 X + \\epsilon \\)\n",
    "#    - **Multiple Linear Regression** involves **two or more independent variables** (predictors) and models the relationship between multiple independent variables and the dependent variable.\n",
    "#      - **Equation**: \\( Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_k X_k + \\epsilon \\)\n",
    "\n",
    "# ### 2. **Complexity**:\n",
    "#    - **Simple Linear Regression** is **less complex**, as it deals with just one predictor variable and is easier to visualize (a straight line in a 2D plot).\n",
    "#    - **Multiple Linear Regression** is **more complex** because it involves multiple predictors, making it harder to visualize but allowing for a more nuanced understanding of how various factors influence the dependent variable.\n",
    "\n",
    "# ### 3. **Interpretation**:\n",
    "#    - In **Simple Linear Regression**, the coefficient \\( \\beta_1 \\) represents the change in \\( Y \\) for a one-unit change in \\( X \\).\n",
    "#    - In **Multiple Linear Regression**, each coefficient \\( \\beta_1, \\beta_2, \\dots, \\beta_k \\) represents the change in \\( Y \\) for a one-unit change in the corresponding independent variable, **holding all other independent variables constant**.\n",
    "\n",
    "# ### 4. **Application**:\n",
    "#    - **Simple Linear Regression** is useful when you want to predict or understand the relationship between a dependent variable and **just one predictor**.\n",
    "#    - **Multiple Linear Regression** is used when you want to predict or understand the relationship between a dependent variable and **multiple predictors**, which can provide more accurate and realistic models in cases where more factors influence the outcome.\n",
    "\n",
    "# ### 5. **Assumptions**:\n",
    "#    - Both models have similar assumptions (linearity, independence, homoscedasticity, and normality of errors), but **Multiple Linear Regression** also assumes that the independent variables are **not highly correlated with each other** (no multicollinearity).\n",
    "\n",
    "# ### Summary Table:\n",
    "\n",
    "# | Feature                        | Simple Linear Regression                  | Multiple Linear Regression              |\n",
    "# |---------------------------------|--------------------------------------------|-----------------------------------------|\n",
    "# | **Number of Predictors**        | One independent variable \\( X \\)           | Two or more independent variables \\( X_1, X_2, \\dots, X_k \\) |\n",
    "# | **Equation**                    | \\( Y = \\beta_0 + \\beta_1 X + \\epsilon \\)   | \\( Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_k X_k + \\epsilon \\) |\n",
    "# | **Model Complexity**            | Less complex (linear relationship)         | More complex (multiple relationships)  |\n",
    "# | **Interpretation of Coefficients** | \\( \\beta_1 \\) shows change in \\( Y \\) per unit change in \\( X \\) | Each \\( \\beta_i \\) shows change in \\( Y \\) per unit change in corresponding \\( X_i \\), holding other variables constant |\n",
    "# | **Visualization**               | Easy to visualize (2D plot)                | Hard to visualize (multidimensional)    |\n",
    "# | **Use Case**                    | When there's one predictor variable        | When there are multiple predictor variables |\n",
    "\n",
    "# In essence, **Simple Linear Regression** is a basic form of regression with one predictor variable, while **Multiple Linear Regression** is an extension that handles multiple predictors, offering a more comprehensive model for predicting the dependent variable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5edb934c-47da-4bfe-bd92-08808a5f177d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # question 10 >> What are the key assumptions of Multiple Linear Regression\n",
    "\n",
    "# Multiple Linear Regression (MLR) makes several assumptions to ensure the model provides reliable, valid, and interpretable results. These assumptions are similar to those in simple linear regression but with a few additional considerations due to the presence of multiple predictors.\n",
    "\n",
    "# ### Key Assumptions of Multiple Linear Regression:\n",
    "\n",
    "# 1. **Linearity**:\n",
    "#    - The relationship between the dependent variable \\( Y \\) and each of the independent variables \\( X_1, X_2, \\dots, X_k \\) should be **linear**. This means that the effect of the independent variables on \\( Y \\) is constant and additive. \n",
    "#    - In other words, the relationship between the predictors and the dependent variable can be modeled by a straight line (or hyperplane in higher dimensions).\n",
    "\n",
    "# 2. **Independence**:\n",
    "#    - The residuals (errors) should be **independent** of each other. This means that the error term for one observation should not be correlated with the error term for another observation.\n",
    "#    - This assumption is critical for ensuring that the model's predictions are unbiased. If the residuals are correlated (for example, in time series data), it could lead to incorrect conclusions.\n",
    "\n",
    "# 3. **Homoscedasticity**:\n",
    "#    - The variance of the residuals (errors) should be **constant** across all levels of the independent variables. In other words, the spread (or variability) of the residuals should remain the same as the predicted values of \\( Y \\) increase or decrease.\n",
    "#    - This assumption ensures that the model is not biased in predicting values at different levels of \\( X \\). If the residuals have non-constant variance (a condition called **heteroscedasticity**), it can lead to inefficient estimates and misleading significance tests.\n",
    "\n",
    "# 4. **Normality of Errors**:\n",
    "#    - The residuals should be **normally distributed** for inference purposes (e.g., hypothesis testing and confidence intervals). While the normality assumption is not strictly required for making predictions, it is important for hypothesis tests related to the model's coefficients (such as t-tests and F-tests).\n",
    "#    - The normality assumption can be assessed visually with a histogram or Q-Q plot of the residuals or with statistical tests like the Shapiro-Wilk test.\n",
    "\n",
    "# 5. **No Multicollinearity**:\n",
    "#    - **Multicollinearity** refers to a situation where the independent variables are highly correlated with each other. If two or more predictors are strongly correlated, it becomes difficult to estimate their individual effects on the dependent variable accurately.\n",
    "#    - High multicollinearity inflates the standard errors of the coefficients and makes it difficult to interpret the significance of individual predictors.\n",
    "#    - This assumption can be tested using measures like the **Variance Inflation Factor (VIF)**. If VIF values are high (typically above 10), it indicates problematic multicollinearity.\n",
    "\n",
    "# 6. **No Auto-correlation (for Time Series Data)**:\n",
    "#    - If the data is time-series data, the residuals should not exhibit any **auto-correlation**. This means that the residual for one time point should not be correlated with residuals from previous time points.\n",
    "#    - Auto-correlation can violate the independence assumption and is especially relevant in time-series analysis. The **Durbin-Watson statistic** is commonly used to test for autocorrelation.\n",
    "\n",
    "# ### Summary of Assumptions:\n",
    "\n",
    "# | Assumption                  | Description                                                                 |\n",
    "# |-----------------------------|-----------------------------------------------------------------------------|\n",
    "# | **Linearity**                | The relationship between \\( Y \\) and \\( X \\) is linear.                    |\n",
    "# | **Independence**             | The residuals are independent of each other.                               |\n",
    "# | **Homoscedasticity**         | The variance of residuals is constant across all levels of \\( X \\).        |\n",
    "# | **Normality of Errors**      | The residuals are normally distributed for statistical inference.          |\n",
    "# | **No Multicollinearity**     | The independent variables are not highly correlated with each other.       |\n",
    "# | **No Auto-correlation**      | The residuals do not exhibit autocorrelation (relevant for time series).   |\n",
    "\n",
    "# ### Why These Assumptions Matter:\n",
    "\n",
    "# - **Linearity** ensures that the model correctly represents the relationships between predictors and the dependent variable.\n",
    "# - **Independence** ensures that there is no influence or bias from one observation to another.\n",
    "# - **Homoscedasticity** ensures that the model's error is consistent across different values of the predictors, allowing for reliable predictions.\n",
    "# - **Normality of Errors** is required for valid hypothesis testing (significance tests for coefficients).\n",
    "# - **No Multicollinearity** allows for clear interpretation of the effects of each predictor independently.\n",
    "# - **No Auto-correlation** in time-series data ensures that observations are not influenced by previous data points.\n",
    "\n",
    "# If any of these assumptions are violated, the reliability and validity of the regression model may be compromised, and alternative methods (like transforming variables, adding regularization, or using time-series models) may be needed to address the issues.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f68dff7-6de8-470d-ac7d-4144232d8464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Question 11 >> What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model\n",
    "\n",
    "# **Heteroscedasticity** refers to a situation where the variance of the residuals (errors) is **not constant** across all levels of the independent variables in a regression model. In other words, as the value of the independent variable(s) changes, the spread or variability of the residuals also changes.\n",
    "\n",
    "# ### Understanding Heteroscedasticity:\n",
    "\n",
    "# In **Multiple Linear Regression (MLR)**, one of the key assumptions is **homoscedasticity**, which means that the variance of the residuals should remain constant for all values of the independent variables. When this assumption is violated, and the residuals show a pattern where their variance increases or decreases as a function of the independent variables, this is known as **heteroscedasticity**.\n",
    "\n",
    "# ### Visualizing Heteroscedasticity:\n",
    "# - **Homoscedasticity**: When you plot the residuals against the predicted values or any of the independent variables, the residuals should be evenly spread out around zero with no clear pattern.\n",
    "  \n",
    "#   Example:\n",
    "#   - A **random scatter** of points across all levels of the independent variable(s).\n",
    "  \n",
    "# - **Heteroscedasticity**: When there is heteroscedasticity, the residuals show a **funnel shape** or some other pattern where their spread increases or decreases as a function of the independent variable(s).\n",
    "  \n",
    "#   Example:\n",
    "#   - If the variance of the residuals increases as the value of \\( X \\) increases (or vice versa), the plot will show a \"fan\" shape.\n",
    "\n",
    "# ### How Heteroscedasticity Affects the Results of Multiple Linear Regression:\n",
    "\n",
    "# 1. **Inefficient Estimates**:\n",
    "#    - Heteroscedasticity causes the **standard errors of the regression coefficients** to be biased and inconsistent. This means that the model's coefficients may not be estimated with high precision.\n",
    "#    - This leads to **inefficient estimates**, and the model may not provide reliable predictions.\n",
    "\n",
    "# 2. **Invalid Significance Tests**:\n",
    "#    - The presence of heteroscedasticity affects the **t-tests** and **F-tests** for the regression coefficients. These tests rely on the assumption of constant variance in the errors to determine whether the coefficients are statistically significant.\n",
    "#    - **P-values** derived from these tests may be misleading because the assumption of constant variance is violated, leading to **incorrect conclusions** about the significance of the predictors.\n",
    "\n",
    "# 3. **Underestimation of Variability**:\n",
    "#    - If heteroscedasticity is present, the model may **underestimate the true variability** in the data, leading to overly optimistic conclusions about how well the model fits the data. This could make the model appear better than it actually is.\n",
    "\n",
    "# 4. **Model Misfit**:\n",
    "#    - Heteroscedasticity suggests that the regression model may not adequately capture the true relationship between the dependent variable and the predictors. The presence of heteroscedasticity may indicate that there are other factors affecting the dependent variable that are not included in the model.\n",
    "\n",
    "# ### How to Detect Heteroscedasticity:\n",
    "# There are several ways to detect heteroscedasticity:\n",
    "\n",
    "# 1. **Residual vs. Fitted Value Plot**:\n",
    "#    - Plot the residuals (errors) against the fitted (predicted) values. If the residuals fan out or contract as the predicted values increase, this is a sign of heteroscedasticity.\n",
    "\n",
    "# 2. **Breusch-Pagan Test**:\n",
    "#    - This statistical test checks for heteroscedasticity. It tests whether the variance of the residuals is related to the values of the independent variables.\n",
    "\n",
    "# 3. **White Test**:\n",
    "#    - Another test used to detect heteroscedasticity, similar to the Breusch-Pagan test but more general.\n",
    "\n",
    "# 4. **Visual Inspection of Residual Plots**:\n",
    "#    - Look for any systematic patterns (like fanning or funneling) in the residual plots. A random scatter of points around zero indicates homoscedasticity, while a pattern indicates heteroscedasticity.\n",
    "\n",
    "# ### How to Address Heteroscedasticity:\n",
    "\n",
    "# 1. **Transformation of Variables**:\n",
    "#    - One common approach to handle heteroscedasticity is to apply a **transformation** to the dependent variable or independent variables. For example, using a logarithmic transformation can often stabilize the variance.\n",
    "#    - Example: If \\( Y \\) shows increasing variance as \\( X \\) increases, you might apply a log transformation to \\( Y \\) (e.g., \\( \\log(Y) \\)).\n",
    "\n",
    "# 2. **Weighted Least Squares (WLS)**:\n",
    "#    - In this method, different weights are assigned to different observations based on the variance of their residuals. Observations with larger residuals (more variance) are given less weight in the estimation process, helping to correct for heteroscedasticity.\n",
    "\n",
    "# 3. **Robust Standard Errors**:\n",
    "#    - If you do not want to transform the data, one alternative is to calculate **robust standard errors** (also called **heteroscedasticity-consistent standard errors**). These adjusted standard errors help correct for the presence of heteroscedasticity in hypothesis testing without needing to modify the regression model itself.\n",
    "\n",
    "# 4. **Add More Predictors**:\n",
    "#    - Sometimes heteroscedasticity arises because the model does not account for important variables that influence the dependent variable. Adding more relevant predictors can help reduce the pattern in the residuals.\n",
    "\n",
    "# ### Conclusion:\n",
    "# Heteroscedasticity is a significant issue in Multiple Linear Regression because it violates one of the core assumptions of the model—constant variance of the residuals. It leads to inefficient estimates, biased standard errors, and invalid statistical tests, which can affect the interpretation and predictive power of the model. Detecting and addressing heteroscedasticity through methods like residual plots, transformations, or robust standard errors is crucial to ensure that the regression model remains reliable and valid.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe42648d-78bf-4894-a634-5660f23c9820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # question 12 >> How can you improve a Multiple Linear Regression model with high multicollinearity\n",
    "\n",
    "# When a **Multiple Linear Regression (MLR)** model exhibits **high multicollinearity**, it means that some of the independent variables are highly correlated with each other. This creates several problems, including unreliable coefficient estimates, inflated standard errors, and difficulty in interpreting the effects of individual predictors. Fortunately, there are several ways to improve a Multiple Linear Regression model with high multicollinearity:\n",
    "\n",
    "# ### 1. **Remove Highly Correlated Variables**:\n",
    "#    - If two or more independent variables are highly correlated, one potential solution is to **remove one of the correlated variables** from the model. This can help eliminate the redundancy and reduce multicollinearity.\n",
    "#    - To identify highly correlated predictors, you can compute a **correlation matrix** or use the **Variance Inflation Factor (VIF)** to assess the level of multicollinearity.\n",
    "#    - **VIF Threshold**: A commonly used rule of thumb is that a VIF greater than **10** suggests high multicollinearity, meaning the corresponding variable should be considered for removal.\n",
    "\n",
    "# ### 2. **Combine or Aggregate Variables**:\n",
    "#    - If two or more predictors are measuring similar concepts, you might **combine** them into a single predictor. This can reduce the multicollinearity by creating a composite variable that captures the shared information.\n",
    "#    - For example, if you have **Height** and **Weight** as predictors and they are highly correlated, you could create a **Body Mass Index (BMI)** variable as a new predictor, which might represent the same concept more succinctly.\n",
    "\n",
    "# ### 3. **Principal Component Analysis (PCA)**:\n",
    "#    - **Principal Component Analysis (PCA)** is a dimensionality reduction technique that can be used to transform the original correlated predictors into a smaller set of uncorrelated components called **principal components**.\n",
    "#    - These principal components can then be used in the regression model as new predictors. PCA helps eliminate multicollinearity by creating independent variables that are linear combinations of the original variables.\n",
    "#    - While PCA is effective at reducing multicollinearity, interpreting the resulting components can be more challenging because they are combinations of the original variables.\n",
    "\n",
    "# ### 4. **Use Ridge Regression (L2 Regularization)**:\n",
    "#    - **Ridge Regression** is a form of **regularized regression** that adds a penalty term to the cost function based on the sum of the squared coefficients. This penalty shrinks the coefficients, especially those of highly correlated predictors, thereby helping to address multicollinearity.\n",
    "#    - Ridge regression doesn't remove variables from the model but rather reduces the influence of correlated variables by **shrinking their coefficients**. The result is a more stable and interpretable model.\n",
    "#    - The key parameter in Ridge Regression is the **regularization strength** (denoted by \\( \\lambda \\) or \\( \\alpha \\)), which controls the amount of shrinkage applied to the coefficients. You can tune this parameter using cross-validation.\n",
    "\n",
    "# ### 5. **Use Lasso Regression (L1 Regularization)**:\n",
    "#    - **Lasso Regression** (Least Absolute Shrinkage and Selection Operator) is another regularization technique that adds a penalty term based on the **absolute values** of the coefficients.\n",
    "#    - Unlike Ridge Regression, Lasso has the property of **shrinking some coefficients exactly to zero**, effectively **performing variable selection**. This can help remove less important predictors from the model, which might be highly correlated with other predictors.\n",
    "#    - Like Ridge, the strength of the regularization is controlled by a hyperparameter (often denoted by \\( \\lambda \\) or \\( \\alpha \\)), which can be optimized using cross-validation.\n",
    "\n",
    "# ### 6. **Stepwise Regression**:\n",
    "#    - **Stepwise Regression** is an automatic variable selection procedure that helps to select the most important predictors by iteratively adding or removing predictors based on some criterion, such as AIC (Akaike Information Criterion) or BIC (Bayesian Information Criterion).\n",
    "#    - **Forward Selection** starts with no predictors and adds the most significant variable at each step.\n",
    "#    - **Backward Elimination** starts with all predictors and removes the least significant ones step-by-step.\n",
    "#    - **Stepwise Selection** combines both approaches. While stepwise methods are effective at handling multicollinearity, they can be prone to overfitting, especially when the number of predictors is large.\n",
    "\n",
    "# ### 7. **Increase Sample Size**:\n",
    "#    - Increasing the **sample size** can sometimes mitigate the effects of multicollinearity. A larger sample size helps provide more information, which can reduce the variance of the coefficient estimates and improve the model's ability to distinguish between the effects of correlated predictors.\n",
    "#    - This approach is often not practical in situations where you cannot collect more data, but it can help in cases where the data is limited.\n",
    "\n",
    "# ### 8. **Use Domain Knowledge**:\n",
    "#    - Sometimes multicollinearity arises because the model includes predictors that are conceptually related. Applying **domain knowledge** to understand the relationship between predictors can help in deciding which variables should be kept, combined, or removed.\n",
    "#    - For example, in a marketing model, if the number of social media followers and online engagement are highly correlated, you might choose to keep the one that is more relevant to your specific research question.\n",
    "\n",
    "# ### 9. **Check for Redundant Interactions**:\n",
    "#    - If your model includes interaction terms (e.g., the product of two independent variables), they might create multicollinearity if the main variables themselves are highly correlated. You should evaluate whether the interaction terms are necessary or if they can be simplified to avoid multicollinearity.\n",
    "\n",
    "# ### How to Identify Multicollinearity:\n",
    "\n",
    "# - **Correlation Matrix**: Check pairwise correlations between independent variables. High correlations (e.g., above 0.7 or 0.8) suggest potential multicollinearity.\n",
    "# - **Variance Inflation Factor (VIF)**: A VIF value greater than 10 indicates high multicollinearity. It can be computed for each independent variable.\n",
    "# - **Condition Index**: A high condition index (e.g., above 30) indicates multicollinearity.\n",
    "\n",
    "# ### Conclusion:\n",
    "# High multicollinearity can undermine the reliability of a Multiple Linear Regression model by making coefficient estimates unstable and difficult to interpret. To address multicollinearity, you can remove or combine correlated predictors, apply dimensionality reduction (PCA), use regularization methods like Ridge or Lasso, or perform stepwise regression. The choice of method depends on the nature of your data, the importance of individual predictors, and the goals of your analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3705153f-ef17-46a1-9655-d229c136503f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # question 13 >> What are some common techniques for transforming categorical variables for use in regression models\n",
    "\n",
    "# When working with **categorical variables** in regression models, these variables need to be transformed into a numerical format because most regression models (like **Multiple Linear Regression**) require numeric input. There are several techniques for transforming categorical variables into numerical values, and the appropriate method depends on the nature of the categorical variable (e.g., **nominal** vs. **ordinal**).\n",
    "\n",
    "# Here are some common techniques for transforming categorical variables for use in regression models:\n",
    "\n",
    "# ### 1. **One-Hot Encoding (Dummy Variables)**\n",
    "\n",
    "# **Best for**: **Nominal variables** (categorical variables with no inherent order, such as color, gender, or country).\n",
    "\n",
    "# - **Description**: One-hot encoding creates binary (0 or 1) columns for each category of the categorical variable. For example, if you have a variable `Color` with categories like \"Red\", \"Green\", and \"Blue\", you would create three new binary columns (one for each color).\n",
    "  \n",
    "#   **Example**:\n",
    "#   - Original data:\n",
    "#     | Color |\n",
    "#     |-------|\n",
    "#     | Red   |\n",
    "#     | Green |\n",
    "#     | Blue  |\n",
    "#     | Red   |\n",
    "  \n",
    "#   - One-Hot Encoded:\n",
    "#     | Red | Green | Blue |\n",
    "#     |-----|-------|------|\n",
    "#     | 1   | 0     | 0    |\n",
    "#     | 0   | 1     | 0    |\n",
    "#     | 0   | 0     | 1    |\n",
    "#     | 1   | 0     | 0    |\n",
    "\n",
    "# - **Advantages**:\n",
    "#   - Simple and effective for nominal variables.\n",
    "#   - Does not imply any order or hierarchy between categories.\n",
    "# - **Disadvantages**:\n",
    "#   - Can lead to **high dimensionality** if the categorical variable has many unique categories (this is known as the **curse of dimensionality**).\n",
    "#   - You need to **drop one of the categories** to avoid the **dummy variable trap**, which occurs when all categories are included and there is perfect multicollinearity (e.g., if all variables sum up to 1, it creates perfect linear dependency).\n",
    "\n",
    "# ### 2. **Label Encoding**\n",
    "\n",
    "# **Best for**: **Ordinal variables** (categorical variables with a natural order, such as education level, rating scales, etc.).\n",
    "\n",
    "# - **Description**: Label encoding assigns a unique integer to each category based on its order. This technique is especially useful when the categories have an inherent rank or order.\n",
    "  \n",
    "#   **Example**:\n",
    "#   - Original data:\n",
    "#     | Education Level |\n",
    "#     |-----------------|\n",
    "#     | High School     |\n",
    "#     | College         |\n",
    "#     | Graduate        |\n",
    "#     | College         |\n",
    "  \n",
    "#   - Label Encoded:\n",
    "#     | Education Level | Encoded Value |\n",
    "#     |-----------------|---------------|\n",
    "#     | High School     | 0             |\n",
    "#     | College         | 1             |\n",
    "#     | Graduate        | 2             |\n",
    "#     | College         | 1             |\n",
    "\n",
    "# - **Advantages**:\n",
    "#   - Simple and easy to implement.\n",
    "#   - Works well for ordinal data where the order matters.\n",
    "# - **Disadvantages**:\n",
    "#   - **Linear models** might misinterpret the encoded integers as representing a **linear relationship** between the categories, which might not be true. For example, the difference between \"High School\" and \"College\" is not necessarily the same as between \"College\" and \"Graduate\", but Label Encoding treats them as equally spaced.\n",
    "  \n",
    "# ### 3. **Binary Encoding**\n",
    "\n",
    "# **Best for**: Categorical variables with **many unique categories**.\n",
    "\n",
    "# - **Description**: Binary encoding is a compromise between **one-hot encoding** and **label encoding**. It first converts the category labels into integers, then converts those integers into their **binary equivalents** and splits them into separate columns. This is more memory-efficient than one-hot encoding for variables with many categories.\n",
    "  \n",
    "#   **Example**:\n",
    "#   - Original data:\n",
    "#     | Color   |\n",
    "#     |---------|\n",
    "#     | Red     |\n",
    "#     | Green   |\n",
    "#     | Blue    |\n",
    "#     | Yellow  |\n",
    "  \n",
    "#   - Integer encoding (for example):\n",
    "#     | Color   | Encoded Integer |\n",
    "#     |---------|-----------------|\n",
    "#     | Red     | 1               |\n",
    "#     | Green   | 2               |\n",
    "#     | Blue    | 3               |\n",
    "#     | Yellow  | 4               |\n",
    "  \n",
    "#   - Binary encoding:\n",
    "#     | Color   | Bit1 | Bit2 | Bit3 |\n",
    "#     |---------|------|------|------|\n",
    "#     | Red     | 0    | 0    | 1    |\n",
    "#     | Green   | 0    | 1    | 0    |\n",
    "#     | Blue    | 0    | 1    | 1    |\n",
    "#     | Yellow  | 1    | 0    | 0    |\n",
    "\n",
    "# - **Advantages**:\n",
    "#   - More **compact** than one-hot encoding, especially for categorical variables with many levels (fewer columns).\n",
    "#   - Reduces dimensionality compared to one-hot encoding.\n",
    "# - **Disadvantages**:\n",
    "#   - Slightly more complex to implement and interpret than one-hot encoding.\n",
    "  \n",
    "# ### 4. **Frequency (Count) Encoding**\n",
    "\n",
    "# **Best for**: Categorical variables with a **large number of categories**, especially when the frequencies of categories vary significantly.\n",
    "\n",
    "# - **Description**: In frequency encoding, each category is replaced by the **frequency (or count)** of its occurrence in the dataset. This technique is especially useful when you have many categories and one-hot encoding would result in too many columns.\n",
    "  \n",
    "#   **Example**:\n",
    "#   - Original data:\n",
    "#     | City    |\n",
    "#     |---------|\n",
    "#     | London  |\n",
    "#     | Paris   |\n",
    "#     | London  |\n",
    "#     | Berlin  |\n",
    "  \n",
    "#   - Frequency Encoding:\n",
    "#     | City    | Frequency |\n",
    "#     |---------|-----------|\n",
    "#     | London  | 2         |\n",
    "#     | Paris   | 1         |\n",
    "#     | London  | 2         |\n",
    "#     | Berlin  | 1         |\n",
    "\n",
    "# - **Advantages**:\n",
    "#   - Simple and efficient.\n",
    "#   - Reduces dimensionality while retaining useful information about the distribution of categories.\n",
    "# - **Disadvantages**:\n",
    "#   - May not be suitable for nominal variables with no natural ordering, as the frequency values could be interpreted as having an ordinal relationship.\n",
    "\n",
    "# ### 5. **Target Encoding (Mean Encoding)**\n",
    "\n",
    "# **Best for**: **Nominal categorical variables** when there's a relationship with the target variable.\n",
    "\n",
    "# - **Description**: Target encoding replaces each category with the **mean of the target variable** for that category. For example, for a categorical variable \"City,\" each city could be replaced by the average value of the target variable (e.g., sales, income, etc.) for that city.\n",
    "  \n",
    "#   **Example**:\n",
    "#   - Original data:\n",
    "#     | City    | Sales |\n",
    "#     |---------|-------|\n",
    "#     | London  | 100   |\n",
    "#     | Paris   | 150   |\n",
    "#     | London  | 200   |\n",
    "#     | Berlin  | 120   |\n",
    "  \n",
    "#   - Target Encoding:\n",
    "#     | City    | Target Encoding (Mean Sales) |\n",
    "#     |---------|------------------------------|\n",
    "#     | London  | 150                          |\n",
    "#     | Paris   | 150                          |\n",
    "#     | London  | 150                          |\n",
    "#     | Berlin  | 120                          |\n",
    "\n",
    "# - **Advantages**:\n",
    "#   - Efficient and can capture relationships between categorical variables and the target.\n",
    "#   - Can be particularly useful in situations where categories have a meaningful relationship with the target.\n",
    "# - **Disadvantages**:\n",
    "#   - Prone to **overfitting** if the number of categories is large or if you don't apply proper regularization techniques (e.g., cross-validation).\n",
    "#   - Requires careful handling, such as **mean smoothing** or using validation sets, to avoid data leakage.\n",
    "\n",
    "# ### 6. **Hashing Encoding**\n",
    "\n",
    "# **Best for**: Very high-cardinality categorical variables (variables with a large number of categories).\n",
    "\n",
    "# - **Description**: Hashing encoding involves applying a **hash function** to the categorical values and mapping them to a fixed number of columns (features). This is useful when you have a large number of categories and don't want to create an excessive number of features with one-hot encoding.\n",
    "  \n",
    "#   **Example**:\n",
    "#   - Original data:\n",
    "#     | Category |\n",
    "#     |----------|\n",
    "#     | A        |\n",
    "#     | B        |\n",
    "#     | C        |\n",
    "#     | D        |\n",
    "  \n",
    "#   - After applying a hash function (assuming a hash space of size 2):\n",
    "#     | Category | Hashed Value |\n",
    "#     |----------|--------------|\n",
    "#     | A        | 10           |\n",
    "#     | B        | 11           |\n",
    "#     | C        | 01           |\n",
    "#     | D        | 00           |\n",
    "\n",
    "# - **Advantages**:\n",
    "#   - Useful for categorical variables with very high cardinality (e.g., hundreds or thousands of categories).\n",
    "#   - Avoids exploding dimensionality.\n",
    "# - **Disadvantages**:\n",
    "#   - The hashed values are not interpretable and might lead to **collisions** (two different categories getting mapped to the same hashed value).\n",
    "\n",
    "# ---\n",
    "\n",
    "# ### Conclusion:\n",
    "# The choice of technique for transforming categorical variables depends on the nature of the variable and the context of the model. For **nominal variables**, one-hot encoding or binary encoding works well, while **ordinal variables** are typically handled with label encoding. For variables with many categories, techniques like frequency encoding or target encoding may be more efficient. Regularization methods, such as Ridge or Lasso regression, can also help manage high-dimensional features resulting from encoding.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bdff3665-c380-4725-882c-992268dd22c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # question 14 >> What is the role of interaction terms in Multiple Linear Regression\n",
    "\n",
    "# In **Multiple Linear Regression (MLR)**, **interaction terms** play a crucial role in capturing the **combined effect** of two or more independent variables on the dependent variable, which might not be adequately represented by the individual effects of those variables. Interaction terms allow the model to account for situations where the effect of one predictor on the dependent variable depends on the value of another predictor.\n",
    "\n",
    "# ### What Are Interaction Terms?\n",
    "\n",
    "# An **interaction term** is created by multiplying two (or more) independent variables together. It represents how the effect of one predictor variable is influenced by the presence or level of another predictor variable.\n",
    "\n",
    "# For example, if you have two independent variables \\( X_1 \\) and \\( X_2 \\), an interaction term would be \\( X_1 \\times X_2 \\). In this case, the model assumes that the effect of \\( X_1 \\) on the dependent variable depends on the value of \\( X_2 \\), and vice versa.\n",
    "\n",
    "# ### Role and Importance of Interaction Terms:\n",
    "\n",
    "# 1. **Capture Combined Effects**:\n",
    "#    - Without interaction terms, a **Multiple Linear Regression** model assumes that the effect of each independent variable on the dependent variable is **independent** of the other predictors. This may not always be true. \n",
    "#    - Interaction terms allow the model to account for scenarios where the influence of one variable changes depending on the level or value of another variable.\n",
    "\n",
    "# 2. **Example in Real-World Applications**:\n",
    "#    - **Marketing**: If you're predicting **sales** (Y) based on **advertisement spending** (X1) and **price** (X2), an interaction term between **advertisement spending** and **price** could be important because the effect of advertising on sales might depend on the price level. For instance, advertising may have a stronger effect when prices are lower.\n",
    "#    - **Medical Research**: When studying the effect of **age** (X1) and **exercise intensity** (X2) on **health outcomes** (Y), the impact of exercise intensity on health could depend on the person's age. An interaction term between age and exercise intensity allows the model to account for this.\n",
    "\n",
    "# 3. **Improve Model Fit**:\n",
    "#    - Including interaction terms can **improve the fit** of a regression model by providing a better representation of how variables interact and influence the dependent variable. This can lead to a **better understanding of relationships** between the predictors and the outcome.\n",
    "#    - If the interaction term significantly improves the **R-squared** value or the **likelihood** of the model, it indicates that the interaction between the variables is important and should be included in the model.\n",
    "\n",
    "# 4. **Non-Linear Relationships**:\n",
    "#    - Interaction terms allow you to model **non-linear relationships** in a linear regression framework. By introducing interaction terms, you can capture more complex relationships between the independent variables and the dependent variable that wouldn’t be accounted for by adding the variables individually.\n",
    "\n",
    "# 5. **Model Interpretation**:\n",
    "#    - The presence of interaction terms adds complexity to the interpretation of regression coefficients. When you include interaction terms, the **interpretation of the main effects** (the coefficients of the individual variables) changes because the effect of one variable depends on the other variable(s).\n",
    "#    - For example, if you include an interaction term \\( X_1 \\times X_2 \\), the coefficient of \\( X_1 \\) represents the effect of \\( X_1 \\) when \\( X_2 = 0 \\), and the coefficient of \\( X_2 \\) represents the effect of \\( X_2 \\) when \\( X_1 = 0 \\). The interaction term itself shows how much the effect of one variable changes as the value of the other variable changes.\n",
    "\n",
    "# ### Mathematical Formulation:\n",
    "\n",
    "# In a Multiple Linear Regression model with two variables \\( X_1 \\) and \\( X_2 \\), and their interaction term \\( X_1 \\times X_2 \\), the equation would look like this:\n",
    "\n",
    "# \\[\n",
    "# Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 (X_1 \\times X_2) + \\epsilon\n",
    "# \\]\n",
    "\n",
    "# Where:\n",
    "# - \\( Y \\) is the dependent variable.\n",
    "# - \\( \\beta_0 \\) is the intercept.\n",
    "# - \\( \\beta_1 \\) and \\( \\beta_2 \\) are the coefficients of \\( X_1 \\) and \\( X_2 \\), respectively.\n",
    "# - \\( \\beta_3 \\) is the coefficient of the interaction term \\( (X_1 \\times X_2) \\).\n",
    "# - \\( \\epsilon \\) is the error term.\n",
    "\n",
    "# In this case:\n",
    "# - \\( \\beta_1 \\) represents the effect of \\( X_1 \\) when \\( X_2 = 0 \\).\n",
    "# - \\( \\beta_2 \\) represents the effect of \\( X_2 \\) when \\( X_1 = 0 \\).\n",
    "# - \\( \\beta_3 \\) represents the additional or combined effect of \\( X_1 \\) and \\( X_2 \\) together, showing how the effect of one variable changes as the other variable changes.\n",
    "\n",
    "# ### When Should You Include Interaction Terms?\n",
    "\n",
    "# 1. **When You Suspect a Combined Effect**:\n",
    "#    - If you believe that the effect of one predictor depends on the level of another predictor, you should include interaction terms in your model.\n",
    "   \n",
    "# 2. **When You Have Domain Knowledge**:\n",
    "#    - Sometimes domain knowledge or business expertise suggests that certain factors interact. For example, in economics or healthcare, you might know that certain factors only influence the outcome when combined.\n",
    "\n",
    "# 3. **Model Diagnostics**:\n",
    "#    - After fitting a regression model, if the residual plots or other diagnostics suggest that there might be a non-linear relationship between predictors, you could consider adding interaction terms to capture this non-linearity.\n",
    "\n",
    "# ### Potential Pitfalls of Interaction Terms:\n",
    "\n",
    "# 1. **Overfitting**:\n",
    "#    - Adding too many interaction terms can lead to **overfitting**—where the model becomes too complex and captures random noise rather than meaningful relationships in the data.\n",
    "#    - It's essential to **regularize** the model (using techniques like **Ridge** or **Lasso Regression**) or use cross-validation to ensure that the added complexity improves generalization.\n",
    "\n",
    "# 2. **Multicollinearity**:\n",
    "#    - Interaction terms can lead to **multicollinearity**, especially when the original predictors are highly correlated. Multicollinearity can make it harder to interpret the model and inflate the standard errors of the coefficients.\n",
    "#    - To manage this, you can assess multicollinearity using **Variance Inflation Factor (VIF)** and consider removing problematic predictors.\n",
    "\n",
    "# 3. **Interpretation Complexity**:\n",
    "#    - Including interaction terms increases the complexity of the model, making interpretation harder. It's essential to clearly communicate the results, particularly when interpreting the effect of each variable in the presence of interactions.\n",
    "\n",
    "# ### Conclusion:\n",
    "\n",
    "# Interaction terms are a powerful tool in **Multiple Linear Regression** because they allow the model to account for combined effects between predictors that would otherwise be missed. They can improve model fit and capture more complex relationships between the independent variables and the dependent variable. However, they also add complexity and can lead to overfitting or multicollinearity if not handled properly. It is important to use interaction terms thoughtfully, backed by domain knowledge and diagnostic checks, to improve the model's predictive power while ensuring interpretability.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b25bebe6-fd2c-4f51-9c65-789ed9ee200a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Question 15 >> How can the interpretation of intercept differ between Simple and Multiple Linear Regression\n",
    "\n",
    "# The interpretation of the **intercept** in a regression model differs between **Simple Linear Regression (SLR)** and **Multiple Linear Regression (MLR)** due to the number of predictors and the way they are related to the dependent variable.\n",
    "\n",
    "# ### 1. **Intercept in Simple Linear Regression (SLR)**:\n",
    "\n",
    "# In **Simple Linear Regression**, you have one independent variable (predictor), and the model is represented as:\n",
    "\n",
    "# \\[\n",
    "# Y = \\beta_0 + \\beta_1 X + \\epsilon\n",
    "# \\]\n",
    "\n",
    "# Where:\n",
    "# - \\( Y \\) is the dependent variable.\n",
    "# - \\( \\beta_0 \\) is the **intercept**.\n",
    "# - \\( \\beta_1 \\) is the **slope** (coefficient for the independent variable \\( X \\)).\n",
    "# - \\( X \\) is the independent variable.\n",
    "# - \\( \\epsilon \\) is the error term.\n",
    "\n",
    "# #### **Interpretation of Intercept in SLR**:\n",
    "# - The **intercept** (\\( \\beta_0 \\)) represents the predicted value of \\( Y \\) when the independent variable \\( X \\) is equal to **zero**. \n",
    "# - In other words, it is the **value of the dependent variable when the independent variable has no effect** (i.e., when \\( X = 0 \\)).\n",
    "  \n",
    "#   **Example**: If the model predicts sales based on advertising spending, the intercept would represent the predicted sales when no money is spent on advertising (i.e., \\( X = 0 \\)).\n",
    "\n",
    "#   **Limitations**:\n",
    "#   - The intercept might not always have a meaningful interpretation if \\( X = 0 \\) is outside the range of your data or doesn't make sense in the real-world context.\n",
    "\n",
    "# ---\n",
    "\n",
    "# ### 2. **Intercept in Multiple Linear Regression (MLR)**:\n",
    "\n",
    "# In **Multiple Linear Regression**, the model involves two or more independent variables (predictors), and it is represented as:\n",
    "\n",
    "# \\[\n",
    "# Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_n X_n + \\epsilon\n",
    "# \\]\n",
    "\n",
    "# Where:\n",
    "# - \\( Y \\) is the dependent variable.\n",
    "# - \\( \\beta_0 \\) is the **intercept**.\n",
    "# - \\( \\beta_1, \\beta_2, \\dots, \\beta_n \\) are the **coefficients** of the independent variables \\( X_1, X_2, \\dots, X_n \\).\n",
    "# - \\( X_1, X_2, \\dots, X_n \\) are the independent variables.\n",
    "# - \\( \\epsilon \\) is the error term.\n",
    "\n",
    "# #### **Interpretation of Intercept in MLR**:\n",
    "# - The **intercept** (\\( \\beta_0 \\)) represents the predicted value of \\( Y \\) when **all** independent variables \\( X_1, X_2, \\dots, X_n \\) are equal to **zero**. \n",
    "# - In other words, it is the **value of the dependent variable when all independent variables have no effect** (i.e., when \\( X_1 = X_2 = \\dots = X_n = 0 \\)).\n",
    "\n",
    "#   **Example**: Suppose you are predicting sales based on advertising spending (\\( X_1 \\)) and store size (\\( X_2 \\)). The intercept would represent the predicted sales when both **advertising spending** and **store size** are zero, which might not be a realistic scenario but can still be mathematically interpreted.\n",
    "\n",
    "#   **Limitations**:\n",
    "#   - Just like in SLR, if any of the independent variables are **unlikely to be zero** in real life (e.g., it’s not realistic to have zero store size or zero advertising spending), the intercept might not have a meaningful real-world interpretation.\n",
    "#   - However, the intercept is still important for **mathematical consistency** of the regression equation.\n",
    "\n",
    "# ---\n",
    "\n",
    "# ### Key Differences in Interpretation Between SLR and MLR:\n",
    "\n",
    "# 1. **Number of Predictors**:\n",
    "#    - In **SLR**, the intercept represents the value of \\( Y \\) when the single predictor \\( X \\) is zero.\n",
    "#    - In **MLR**, the intercept represents the value of \\( Y \\) when **all** the predictors are zero simultaneously.\n",
    "\n",
    "# 2. **Context of Zero**:\n",
    "#    - In **SLR**, the interpretation is straightforward as long as \\( X = 0 \\) is within the range of observed values. It represents the baseline value when there is no influence from the independent variable.\n",
    "#    - In **MLR**, the intercept might have a **less meaningful** interpretation because it's considering **multiple predictors**. The intercept assumes that all predictors are zero at the same time, which might not be a practical or realistic scenario (e.g., having zero for multiple predictors like age, income, and education level at the same time may not make sense in real life).\n",
    "\n",
    "# 3. **Real-World Relevance**:\n",
    "#    - In **SLR**, if the predictor has a meaningful **zero value**, the intercept can have a clear real-world interpretation.\n",
    "#    - In **MLR**, the intercept’s practical interpretation depends on the specific predictors. If some of the predictors (like income, store size, or education level) cannot realistically be zero, the intercept's real-world meaning is often limited.\n",
    "\n",
    "# 4. **Role in Model**:\n",
    "#    - In **SLR**, the intercept helps define the starting point or baseline value for the dependent variable when the predictor is zero.\n",
    "#    - In **MLR**, the intercept represents the baseline value of the dependent variable when all predictors are set to zero, but this baseline can be influenced by the combined values of all the predictors.\n",
    "\n",
    "# ---\n",
    "\n",
    "# ### Example to Illustrate the Difference:\n",
    "\n",
    "# - **Simple Linear Regression Example**:\n",
    "#   - Model: \\( Y = 50 + 10X \\), where \\( Y \\) is sales and \\( X \\) is advertising spending.\n",
    "#   - **Interpretation**: The intercept is 50, meaning that if no money is spent on advertising (\\( X = 0 \\)), the predicted sales (\\( Y \\)) would be 50 units.\n",
    "  \n",
    "# - **Multiple Linear Regression Example**:\n",
    "#   - Model: \\( Y = 20 + 5X_1 + 3X_2 \\), where \\( Y \\) is sales, \\( X_1 \\) is advertising spending, and \\( X_2 \\) is store size.\n",
    "#   - **Interpretation**: The intercept is 20, meaning that if both advertising spending and store size are zero (\\( X_1 = 0, X_2 = 0 \\)), the predicted sales would be 20 units. However, this scenario might not be realistic, as both \\( X_1 \\) and \\( X_2 \\) being zero simultaneously may not occur in practice.\n",
    "\n",
    "# ---\n",
    "\n",
    "# ### Conclusion:\n",
    "\n",
    "# - In **Simple Linear Regression**, the intercept has a direct and often more intuitive interpretation, representing the value of \\( Y \\) when the single predictor \\( X \\) is zero.\n",
    "# - In **Multiple Linear Regression**, the intercept represents the value of \\( Y \\) when all the predictors are zero, but this interpretation can be less meaningful, especially when it is not realistic for all predictors to be zero at the same time. The intercept is still a necessary component of the model to establish the baseline prediction before considering the effects of the independent variables.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "048a11e9-9370-424d-82b1-a441486830ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # question 16 >>What is the significance of the slope in regression analysis, and how does it affect predictions\n",
    "\n",
    "# The **slope** in regression analysis represents the **relationship between the independent variable (predictor)** and the **dependent variable (response)**. Specifically, it indicates the amount of **change** in the dependent variable for each unit change in the independent variable. The slope is a crucial parameter in both **Simple Linear Regression (SLR)** and **Multiple Linear Regression (MLR)**, though its interpretation can vary depending on the context of the model.\n",
    "\n",
    "# ### 1. **Slope in Simple Linear Regression (SLR)**\n",
    "\n",
    "# In **Simple Linear Regression**, the model takes the form:\n",
    "\n",
    "# \\[\n",
    "# Y = \\beta_0 + \\beta_1 X + \\epsilon\n",
    "# \\]\n",
    "\n",
    "# Where:\n",
    "# - \\( Y \\) is the dependent variable (the one you're trying to predict).\n",
    "# - \\( X \\) is the independent variable (the predictor).\n",
    "# - \\( \\beta_0 \\) is the intercept.\n",
    "# - \\( \\beta_1 \\) is the **slope** (the coefficient of \\( X \\)).\n",
    "# - \\( \\epsilon \\) is the error term (residuals).\n",
    "\n",
    "# #### **Interpretation of the Slope (\\( \\beta_1 \\)) in SLR**:\n",
    "# - The **slope** \\( \\beta_1 \\) represents the **change in \\( Y \\)** for a **one-unit increase** in \\( X \\).\n",
    "# - If the slope is positive, it means that as \\( X \\) increases, \\( Y \\) also increases (a positive relationship). If the slope is negative, it means that as \\( X \\) increases, \\( Y \\) decreases (a negative relationship).\n",
    "  \n",
    "#   **Example**: Suppose you're using **advertising spending (X)** to predict **sales (Y)**, and the model is:\n",
    "\n",
    "#   \\[\n",
    "#   Y = 50 + 10X\n",
    "#   \\]\n",
    "\n",
    "#   - The slope is 10, which means that for every 1 unit increase in advertising spending, the predicted sales increase by 10 units.\n",
    "\n",
    "#   The slope is the **rate of change** of the dependent variable as the independent variable changes. It quantifies the strength and direction of the relationship between \\( X \\) and \\( Y \\).\n",
    "\n",
    "# ---\n",
    "\n",
    "# ### 2. **Slope in Multiple Linear Regression (MLR)**\n",
    "\n",
    "# In **Multiple Linear Regression**, the model includes more than one independent variable:\n",
    "\n",
    "# \\[\n",
    "# Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_n X_n + \\epsilon\n",
    "# \\]\n",
    "\n",
    "# Where:\n",
    "# - \\( Y \\) is the dependent variable.\n",
    "# - \\( X_1, X_2, \\dots, X_n \\) are the independent variables.\n",
    "# - \\( \\beta_1, \\beta_2, \\dots, \\beta_n \\) are the coefficients (slopes) of the independent variables.\n",
    "# - \\( \\beta_0 \\) is the intercept.\n",
    "\n",
    "# #### **Interpretation of Slopes in MLR**:\n",
    "# - Each **slope** \\( \\beta_i \\) represents the **change in \\( Y \\)** for a **one-unit change** in \\( X_i \\), while keeping all other predictors constant (ceteris paribus). This is a key difference from SLR, where the slope represents the effect of the predictor on \\( Y \\) without considering other variables.\n",
    "  \n",
    "#   **Example**: Suppose you're predicting **sales (Y)** based on **advertising spending (X_1)** and **store size (X_2)**:\n",
    "\n",
    "#   \\[\n",
    "#   Y = 100 + 10X_1 + 5X_2\n",
    "#   \\]\n",
    "\n",
    "#   - \\( \\beta_1 = 10 \\) means that for every 1 unit increase in advertising spending (\\( X_1 \\)), sales (\\( Y \\)) will increase by 10 units, **assuming store size remains constant**.\n",
    "#   - \\( \\beta_2 = 5 \\) means that for every 1 unit increase in store size (\\( X_2 \\)), sales will increase by 5 units, **assuming advertising spending remains constant**.\n",
    "\n",
    "# The **interpretation of the slope** in MLR provides the **marginal effect** of each individual predictor while controlling for the other predictors.\n",
    "\n",
    "# ---\n",
    "\n",
    "# ### 3. **Significance of the Slope in Regression Analysis**:\n",
    "\n",
    "# #### a. **Direction of the Relationship**:\n",
    "# - The sign of the slope (positive or negative) tells us the **direction** of the relationship between the independent variable and the dependent variable:\n",
    "#   - **Positive slope**: As the predictor increases, the response variable also increases.\n",
    "#   - **Negative slope**: As the predictor increases, the response variable decreases.\n",
    "\n",
    "# #### b. **Magnitude of the Slope**:\n",
    "# - The **magnitude of the slope** tells us how **strongly** the independent variable influences the dependent variable. A larger magnitude means a stronger effect, and a smaller magnitude indicates a weaker effect.\n",
    "#   - **Larger absolute slope**: A significant change in \\( Y \\) with a small change in \\( X \\).\n",
    "#   - **Smaller absolute slope**: A small change in \\( Y \\) with a large change in \\( X \\).\n",
    "\n",
    "# #### c. **Prediction**:\n",
    "# - The slope directly affects the **predicted values** of \\( Y \\) for any given value of \\( X \\) (or \\( X_1, X_2, \\dots, X_n \\) in MLR).\n",
    "# - If the slope is large, even small changes in the independent variable will lead to large changes in the predicted value of \\( Y \\), making the model sensitive to variations in the predictor.\n",
    "# - If the slope is small, the model will be less sensitive to changes in the independent variable, and predictions will be more stable.\n",
    "\n",
    "# #### d. **Statistical Significance**:\n",
    "# - The slope’s statistical significance is tested using **hypothesis tests** (e.g., t-tests), and it helps to determine whether the relationship between the independent variable and the dependent variable is statistically meaningful.\n",
    "# - A **significant slope** indicates that the independent variable is a **relevant predictor** of the dependent variable.\n",
    "  \n",
    "#   For example, in a hypothesis test for \\( \\beta_1 \\) in SLR:\n",
    "#   - **Null hypothesis**: \\( H_0: \\beta_1 = 0 \\) (no relationship between \\( X \\) and \\( Y \\)).\n",
    "#   - **Alternative hypothesis**: \\( H_1: \\beta_1 \\neq 0 \\) (a relationship exists).\n",
    "#   - If the p-value for the slope is small (e.g., less than 0.05), you reject the null hypothesis, meaning that the predictor \\( X \\) significantly affects \\( Y \\).\n",
    "\n",
    "# ---\n",
    "\n",
    "# ### 4. **How Slope Affects Predictions**:\n",
    "\n",
    "# In both SLR and MLR, the slope directly impacts the **predicted values** of \\( Y \\) for any given value of \\( X \\) (or \\( X_1, X_2, \\dots, X_n \\) in MLR). \n",
    "\n",
    "# - **In Simple Linear Regression**:\n",
    "#   The predicted value of \\( Y \\) for a given \\( X \\) is:\n",
    "\n",
    "#   \\[\n",
    "#   \\hat{Y} = \\beta_0 + \\beta_1 X\n",
    "#   \\]\n",
    "\n",
    "#   If \\( \\beta_1 \\) is large, the predicted value will change significantly with a small change in \\( X \\). If \\( \\beta_1 \\) is small, the predicted value will not change as drastically with a change in \\( X \\).\n",
    "\n",
    "# - **In Multiple Linear Regression**:\n",
    "#   The predicted value of \\( Y \\) for given values of \\( X_1, X_2, \\dots, X_n \\) is:\n",
    "\n",
    "#   \\[\n",
    "#   \\hat{Y} = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_n X_n\n",
    "#   \\]\n",
    "\n",
    "#   Here, each slope \\( \\beta_i \\) dictates how much \\( Y \\) will change as the corresponding predictor \\( X_i \\) changes, assuming all other predictors remain constant.\n",
    "\n",
    "# ---\n",
    "\n",
    "# ### Conclusion:\n",
    "\n",
    "# The **slope** in regression analysis represents the strength and direction of the relationship between an independent variable and the dependent variable. It has a direct impact on **predictions**, as it dictates how much the dependent variable changes when the independent variable changes. In **Simple Linear Regression**, the slope measures the effect of a single predictor, while in **Multiple Linear Regression**, the slope measures the marginal effect of each predictor while controlling for the others. Understanding the slope is key to interpreting the model, making predictions, and determining the significance of each predictor in the regression analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8569dc6e-60ed-4e1a-ae5e-a4016cddba35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Question 17 >> How does the intercept in a regression model provide context for the relationship between variables\n",
    "\n",
    "# The **intercept** in a regression model provides important context for understanding the **baseline** value of the dependent variable when all the independent variables are equal to zero. It plays a key role in framing the relationship between the independent variables and the dependent variable, although its interpretation can vary depending on the model type and the real-world context.\n",
    "\n",
    "# ### 1. **General Role of the Intercept in Regression Models**:\n",
    "# The **intercept** (\\( \\beta_0 \\)) in any regression model represents the **value of the dependent variable** when **all independent variables** are equal to **zero**. Mathematically, for a simple linear regression model:\n",
    "\n",
    "# \\[\n",
    "# Y = \\beta_0 + \\beta_1 X + \\epsilon\n",
    "# \\]\n",
    "\n",
    "# Where:\n",
    "# - \\( Y \\) is the dependent variable (the outcome you're trying to predict).\n",
    "# - \\( X \\) is the independent variable (the predictor).\n",
    "# - \\( \\beta_0 \\) is the **intercept**.\n",
    "# - \\( \\beta_1 \\) is the **slope** of the regression line.\n",
    "# - \\( \\epsilon \\) is the error term.\n",
    "\n",
    "# ### 2. **Interpretation of the Intercept**:\n",
    "# In this model, the intercept \\( \\beta_0 \\) represents the **predicted value of \\( Y \\)** when the independent variable \\( X \\) is **zero**. \n",
    "\n",
    "# #### Example in Simple Linear Regression (SLR):\n",
    "# - Suppose you are modeling **sales** (Y) based on **advertising spending** (X) using the equation:\n",
    "\n",
    "# \\[\n",
    "# Y = 100 + 20X\n",
    "# \\]\n",
    "\n",
    "# - The **intercept** is **100**, which means that when no money is spent on advertising (\\( X = 0 \\)), the predicted sales (\\( Y \\)) is **100** units. In this case, the intercept sets a **baseline** for the model's predictions.\n",
    "\n",
    "# #### Example in Multiple Linear Regression (MLR):\n",
    "# - For a more complex model with multiple predictors, the intercept still represents the predicted value of \\( Y \\) when all the predictors are zero. For example:\n",
    "\n",
    "# \\[\n",
    "# Y = 50 + 10X_1 + 5X_2\n",
    "# \\]\n",
    "\n",
    "# Where:\n",
    "# - \\( Y \\) = sales\n",
    "# - \\( X_1 \\) = advertising spending\n",
    "# - \\( X_2 \\) = store size\n",
    "\n",
    "# - The **intercept** is **50**, which means that if both **advertising spending** and **store size** are **zero**, the predicted sales will be **50 units**. The intercept here sets a **baseline level** of sales when no advertising is done and the store size is zero.\n",
    "\n",
    "# ### 3. **Context and Meaning of the Intercept**:\n",
    "# While the intercept provides a **baseline** or **starting point** for the model, its real-world interpretation may or may not be meaningful, depending on the context.\n",
    "\n",
    "# #### In Simple Linear Regression:\n",
    "# - The intercept is straightforward to interpret when the predictor (independent variable) can realistically take a value of zero. For instance, in the example of sales and advertising spending, if no money is spent on advertising, the intercept makes sense as a baseline sales figure.\n",
    "# - However, if the independent variable doesn't make sense when equal to zero, the intercept might have little practical significance. For example, if you're predicting **income** based on **education level** and the model includes a variable for education years (where zero years of education isn't realistic), the intercept might be mathematically valid but lack a useful interpretation.\n",
    "\n",
    "# #### In Multiple Linear Regression:\n",
    "# - The intercept represents the predicted value of \\( Y \\) when **all** the independent variables are zero simultaneously. This may not always be a realistic scenario.\n",
    "#   - **Example**: If you are modeling house prices based on **square footage** and **age of the house**, the intercept represents the predicted price when both square footage and house age are zero. While this might not be meaningful (a house with zero square footage or zero age doesn't exist), it still serves as the baseline for the model's predictions.\n",
    "  \n",
    "#   In MLR, the intercept provides context by anchoring the predicted values of \\( Y \\) to a baseline, but the intercept's practical relevance often depends on whether it's reasonable for all predictors to be zero at the same time.\n",
    "\n",
    "# ### 4. **Why the Intercept Is Important**:\n",
    "# The intercept helps in the following ways:\n",
    "\n",
    "# #### a. **Establishes the Baseline**:\n",
    "# It serves as the starting value for the dependent variable when no independent variables are at play. Without the intercept, the model would lack a baseline level and would only predict the change from zero, which might not be realistic or useful.\n",
    "\n",
    "# #### b. **Ensures Mathematical Consistency**:\n",
    "# In regression models, the intercept is necessary to ensure that the relationship between the independent and dependent variables is well-defined and mathematically correct. It helps the model fit the data more accurately, providing a reference point from which the slopes (effects of the predictors) are measured.\n",
    "\n",
    "# #### c. **Supports Predictions**:\n",
    "# Even if the intercept doesn't have a meaningful real-world interpretation, it still plays an essential role in the prediction process. For a given set of independent variables, the intercept allows for accurate predictions of the dependent variable based on the values of the predictors.\n",
    "\n",
    "# ### 5. **Practical Considerations**:\n",
    "# - **Realistic Scenarios**: Sometimes, the intercept doesn’t have a direct real-world interpretation, especially if it's derived from situations that don’t occur in reality (e.g., a zero square footage house). However, it is still important for the mathematical model.\n",
    "  \n",
    "# - **Model Interpretation**: In certain models, the **interpretation of the intercept** may not be as important as the **interpretation of the slope coefficients**, especially if the intercept is not meaningful or if the zero value of the predictors is outside the range of the data. Instead, focus on the slopes (coefficients) that indicate the **marginal effect** of each predictor on the dependent variable.\n",
    "\n",
    "# ---\n",
    "\n",
    "# ### Summary:\n",
    "# - The **intercept** in a regression model represents the predicted value of the dependent variable when all the independent variables are zero.\n",
    "# - In **Simple Linear Regression**, the intercept gives a baseline value for the dependent variable when the predictor is zero.\n",
    "# - In **Multiple Linear Regression**, the intercept represents the predicted value of the dependent variable when all predictors are zero, but its real-world interpretation might be less meaningful if zero values for all predictors are unrealistic.\n",
    "# - The intercept sets the context for understanding the relationship between the independent and dependent variables and provides a mathematical reference point for making predictions.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f86478ad-467a-4b5e-a669-0ed45591f26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # question 18 >> What are the limitations of using R² as a sole measure of model performance\n",
    "\n",
    "# While **R²** (the **coefficient of determination**) is a widely used measure to evaluate the performance of regression models, it has several limitations when used as the sole metric. R² quantifies how much of the variance in the dependent variable is explained by the independent variables, but it doesn’t provide a complete picture of model performance. Below are some of the key limitations of relying solely on R²:\n",
    "\n",
    "# ### 1. **R² Can Be Inflated by Adding More Predictors**:\n",
    "# - **Problem**: R² tends to increase as more independent variables are added to the regression model, regardless of whether those variables are genuinely useful or not.\n",
    "# - **Implication**: A high R² does not necessarily indicate a good model if irrelevant or redundant predictors are included. This can lead to **overfitting**, where the model fits the training data well but fails to generalize to new data.\n",
    "\n",
    "# #### Example:\n",
    "# - If you have a regression model with one independent variable that explains 70% of the variance in the dependent variable, and you add a second predictor that is not meaningful, R² might increase to 75%, even though the second predictor doesn't improve the model’s ability to predict the dependent variable.\n",
    "\n",
    "# ### 2. **R² Doesn’t Indicate the Accuracy of Predictions**:\n",
    "# - **Problem**: A high R² doesn’t necessarily mean the model makes accurate predictions. It only measures the proportion of variance explained by the model, not the **actual errors** in predictions (e.g., how close the predicted values are to the true values).\n",
    "# - **Implication**: You could have a high R² but still have large prediction errors. For example, if the model fits the training data very well but performs poorly on new or unseen data (due to overfitting), the R² will be misleading.\n",
    "\n",
    "# #### Example:\n",
    "# - A model may have an R² of 0.95, indicating that it explains 95% of the variance in the data, but if the predictions are consistently off by a large margin, R² alone doesn't highlight this issue.\n",
    "\n",
    "# ### 3. **R² Cannot Assess Model Fit in Non-Linear Models**:\n",
    "# - **Problem**: R² is specifically designed for linear models and may not be a good fit for non-linear regression models. For non-linear relationships, it may fail to adequately capture the model's performance.\n",
    "# - **Implication**: If you are using a **non-linear regression model** (e.g., polynomial regression or decision tree), R² might not provide a meaningful assessment of the model’s performance. The model could perform well in capturing non-linear relationships, but R² would not reflect this appropriately.\n",
    "\n",
    "# ### 4. **R² Doesn’t Measure Causality**:\n",
    "# - **Problem**: R² measures the strength of the relationship between variables but doesn’t imply causality. A high R² means there is a strong association between the independent and dependent variables, but it doesn't tell you if one causes the other.\n",
    "# - **Implication**: In fields like economics or medicine, where understanding causal relationships is crucial, R² should not be treated as a definitive indicator of the effectiveness of the model or the underlying causal mechanisms.\n",
    "\n",
    "# #### Example:\n",
    "# - A model showing a high R² between ice cream sales and drowning incidents doesn’t mean ice cream consumption causes drowning—it might just indicate a spurious correlation due to a third factor (e.g., summer weather).\n",
    "\n",
    "# ### 5. **R² Cannot Be Used to Compare Models with Different Dependent Variables**:\n",
    "# - **Problem**: R² is specific to each model and the dependent variable used. It is not meaningful to compare R² values across models with different dependent variables because the values are tied to the scale and distribution of the outcome variable.\n",
    "# - **Implication**: If you compare two models where the dependent variables are fundamentally different (e.g., one predicting sales in dollars and the other predicting temperature in degrees), comparing their R² values wouldn’t be informative.\n",
    "\n",
    "# ### 6. **R² Doesn’t Account for the Distribution of Residuals**:\n",
    "# - **Problem**: R² does not provide any insights into the **distribution** or **patterns** of residuals (the differences between observed and predicted values). A good model should have residuals that are randomly scattered, with no clear pattern.\n",
    "# - **Implication**: You could have a high R², but if there is a systematic pattern in the residuals (e.g., a curve or trend), the model might still not be appropriate or may violate assumptions like **homoscedasticity** or **normality**.\n",
    "\n",
    "# #### Example:\n",
    "# - A model with high R² might still exhibit a pattern in residuals that suggests non-linearity, autocorrelation, or heteroscedasticity, which could indicate that the model is missing important features or assumptions.\n",
    "\n",
    "# ### 7. **R² Can Be Misleading in Small Samples**:\n",
    "# - **Problem**: In small sample sizes, R² can be very unstable and might not reflect the true explanatory power of the model. In such cases, R² might give a misleading indication of the model’s ability to generalize.\n",
    "# - **Implication**: With a small dataset, R² could be artificially inflated or deflated due to random variation, leading to misleading conclusions about the model’s effectiveness.\n",
    "\n",
    "# ### 8. **Adjusted R² May Be a Better Metric in Some Cases**:\n",
    "# - **Solution**: In cases where you have multiple predictors, **Adjusted R²** is often preferred over R² because it adjusts for the number of predictors in the model. It helps to penalize the addition of unnecessary variables that don’t improve the model significantly.\n",
    "  \n",
    "# #### Example:\n",
    "# - If adding an additional variable to a model doesn’t improve its predictive power but still increases R², the Adjusted R² will reflect that the new variable didn’t contribute meaningfully to the model.\n",
    "\n",
    "# ### 9. **R² Doesn’t Address Model Bias**:\n",
    "# - **Problem**: R² is concerned with how well the model fits the data in terms of explaining variance, but it does not tell you whether the model is **biased**. A model could have a high R² but still produce biased predictions if it systematically underestimates or overestimates the dependent variable.\n",
    "# - **Implication**: Even with a high R², the model could still be making systematic errors, especially if the model assumptions (e.g., linearity, independence) are violated.\n",
    "\n",
    "# ### 10. **R² Might Not Be Meaningful for Some Types of Data**:\n",
    "# - **Problem**: For certain types of data (e.g., binary outcomes or count data), R² is not a meaningful or interpretable measure of fit.\n",
    "# - **Implication**: For classification problems or models like logistic regression, alternative performance metrics (e.g., **accuracy**, **precision**, **recall**, **F1 score**, or **AUC-ROC** curve) should be used.\n",
    "\n",
    "# ---\n",
    "\n",
    "# ### Conclusion:\n",
    "# While **R²** is a useful metric for assessing the **goodness of fit** in a regression model, it has several limitations and should not be the sole criterion for evaluating model performance. It can be misleading if used in isolation, especially in the presence of **overfitting**, **non-linearity**, or **model bias**. It’s important to complement R² with other diagnostic tools and performance metrics—such as **Adjusted R²**, **cross-validation**, residual analysis, and prediction error metrics (e.g., **RMSE**, **MAE**, **MSE**)—to get a more comprehensive understanding of a model’s true performance and generalization ability.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ffa9628f-e365-4f8b-9ba6-3c1c46d99a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # question 19 >>  How would you interpret a large standard error for a regression coefficient\n",
    "\n",
    "# A large **standard error (SE)** for a regression coefficient indicates that there is a **high level of uncertainty or variability** in the estimate of that coefficient. The standard error measures the precision of the estimated coefficient, so a larger standard error means that the estimated coefficient is less reliable. Here's how you can interpret a large standard error in a regression context:\n",
    "\n",
    "# ### 1. **High Uncertainty in the Estimate**:\n",
    "#    - A **large standard error** suggests that the estimated regression coefficient is **less precise** and could vary widely across different samples of data.\n",
    "#    - This implies that the model might not be able to confidently identify the true effect of the predictor variable on the outcome variable.\n",
    "\n",
    "# #### Example:\n",
    "#    - Suppose you're estimating the effect of **advertising spend** (predictor) on **sales** (outcome). If the standard error for the coefficient of advertising spend is large, it means that there is considerable uncertainty in estimating how much sales would increase for each additional dollar spent on advertising. Your estimate might be inaccurate or unstable.\n",
    "\n",
    "# ### 2. **Lack of Statistical Significance**:\n",
    "#    - A large standard error can make it harder for the regression coefficient to be **statistically significant**. Statistical significance is often determined by **t-tests**, which compare the ratio of the estimated coefficient to its standard error.\n",
    "#    - The **t-statistic** is calculated as:\n",
    "\n",
    "#      \\[\n",
    "#      t = \\frac{\\hat{\\beta}}{SE(\\hat{\\beta})}\n",
    "#      \\]\n",
    "\n",
    "#      Where:\n",
    "#      - \\( \\hat{\\beta} \\) is the estimated regression coefficient.\n",
    "#      - \\( SE(\\hat{\\beta}) \\) is the standard error of the coefficient.\n",
    "   \n",
    "#    - If the **t-statistic** is small (because the standard error is large), the coefficient may not be significantly different from zero, and you may fail to reject the null hypothesis (which states that the coefficient is zero).\n",
    "\n",
    "#    - A **large standard error** could indicate that the predictor variable is not contributing much to explaining the variance in the dependent variable.\n",
    "\n",
    "# #### Example:\n",
    "#    - If you have a regression coefficient of 0.5 for advertising spend with a standard error of 0.3, the t-statistic would be:\n",
    "\n",
    "#      \\[\n",
    "#      t = \\frac{0.5}{0.3} = 1.67\n",
    "#      \\]\n",
    "\n",
    "#      Depending on the degrees of freedom and the significance level, a t-statistic of 1.67 may not be large enough to reject the null hypothesis that the coefficient is zero.\n",
    "\n",
    "# ### 3. **Multicollinearity**:\n",
    "#    - A large standard error can be a sign of **multicollinearity**, which occurs when independent variables in the regression model are highly correlated with each other.\n",
    "#    - Multicollinearity leads to **inflated standard errors** because it becomes harder to isolate the individual effect of each predictor on the dependent variable when the predictors are correlated with each other.\n",
    "\n",
    "# #### Example:\n",
    "#    - If you have two highly correlated predictors, such as **height** and **weight**, in a regression model predicting **BMI**, the standard errors of their coefficients may be large because it's difficult for the model to distinguish their individual effects on BMI.\n",
    "\n",
    "# ### 4. **Small Sample Size**:\n",
    "#    - A **large standard error** can also be due to a **small sample size**. In small samples, estimates of regression coefficients tend to be more variable, leading to larger standard errors. Larger standard errors make it harder to detect true relationships between variables.\n",
    "#    - **Small sample sizes** increase the likelihood of **overfitting** and the model being less reliable in predicting outcomes.\n",
    "\n",
    "# #### Example:\n",
    "#    - If you're working with a small dataset of just 30 data points, the estimated coefficients for predictors may have larger standard errors, reflecting the higher uncertainty in the model’s predictions.\n",
    "\n",
    "# ### 5. **Possible Model Misspecification**:\n",
    "#    - A large standard error could indicate that the **model is misspecified**, meaning the relationship between the independent and dependent variables might not be correctly represented.\n",
    "#    - This could be due to:\n",
    "#      - **Omitted variables** (important predictors are left out of the model).\n",
    "#      - **Non-linear relationships** that are incorrectly modeled as linear.\n",
    "#      - **Inappropriate choice of model** for the type of data (e.g., using linear regression when a different model would be more suitable).\n",
    "\n",
    "#    - In such cases, the large standard error suggests that the model may not be capturing the true underlying relationships, leading to less precise coefficient estimates.\n",
    "\n",
    "# ### 6. **Interpretation of a Large Standard Error in the Context of Confidence Intervals**:\n",
    "#    - The **confidence interval** for a regression coefficient is constructed using the standard error, and a large standard error results in a **wider confidence interval**. This means that the range of plausible values for the coefficient is broader, indicating greater uncertainty.\n",
    "#    - For instance, if the estimated coefficient for a variable is 0.5 with a large standard error of 0.4, the 95% confidence interval might range from -0.3 to 1.3. This interval is wide, suggesting that the true value of the coefficient could be much lower or much higher than the estimate.\n",
    "\n",
    "# ---\n",
    "\n",
    "# ### Summary of Interpretations:\n",
    "\n",
    "# 1. **High uncertainty** in the coefficient estimate.\n",
    "# 2. **Reduced statistical significance** (making it harder to conclude that the predictor is truly influencing the outcome variable).\n",
    "# 3. Potential issues with **multicollinearity** (predictors are highly correlated with each other).\n",
    "# 4. A possible consequence of a **small sample size**, leading to **less precision**.\n",
    "# 5. A potential sign of **model misspecification** or failure to properly capture relationships in the data.\n",
    "# 6. **Wider confidence intervals** for the coefficient, indicating less confidence in the true value of the coefficient.\n",
    "\n",
    "# ---\n",
    "\n",
    "# ### How to Address a Large Standard Error:\n",
    "# - **Check for Multicollinearity**: Use techniques like **Variance Inflation Factor (VIF)** to check if predictors are highly correlated, and if so, consider removing or combining correlated variables.\n",
    "# - **Increase Sample Size**: If possible, collecting more data can reduce the standard error and make the coefficient estimates more reliable.\n",
    "# - **Model Refinement**: Ensure that the model is correctly specified, includes relevant predictors, and reflects the nature of the data (e.g., using a non-linear model when needed).\n",
    "# - **Regularization**: Consider using regularization techniques like **Ridge Regression** or **Lasso** to reduce the impact of large standard errors due to multicollinearity or irrelevant predictors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "90175018-6e5c-49ae-8094-e28ee437669f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # question 20 >> How can heteroscedasticity be identified in residual plots, and why is it important to address it\n",
    "\n",
    "# **Heteroscedasticity** refers to a situation in regression models where the variability (or spread) of the residuals (errors) is not constant across all levels of the independent variable(s). In other words, the variance of the residuals changes as the predicted values of the dependent variable increase or decrease. Identifying and addressing heteroscedasticity is crucial because it can affect the reliability of statistical tests and the efficiency of the regression estimates.\n",
    "\n",
    "# ### How to Identify Heteroscedasticity in Residual Plots:\n",
    "\n",
    "# One of the most common methods to detect heteroscedasticity is by examining **residual plots**. Here's how you can use residual plots to identify heteroscedasticity:\n",
    "\n",
    "# #### 1. **Plotting Residuals vs. Fitted Values**:\n",
    "#    - The **residual plot** (residuals on the vertical axis and fitted values on the horizontal axis) is the most effective way to detect heteroscedasticity.\n",
    "#    - **Ideal Scenario (No Heteroscedasticity)**: If the variance of residuals is constant (i.e., homoscedasticity), the residuals will be **randomly scattered** around the horizontal axis, with no apparent pattern or structure. The spread of residuals will remain roughly constant across all levels of fitted values.\n",
    "   \n",
    "#    - **Heteroscedasticity**: If there is a clear pattern in the residuals, such as increasing or decreasing spread as the fitted values increase, this suggests **heteroscedasticity**. The plot may show:\n",
    "#      - A **funnel shape**, where the spread of residuals increases as the fitted values grow (a common sign of positive heteroscedasticity).\n",
    "#      - A **megaphone shape**, where the residuals spread out more as fitted values increase.\n",
    "#      - A **bow-tie shape**, where the residuals are large for small fitted values and become smaller for larger fitted values (or vice versa).\n",
    "\n",
    "#    - **Example of Heteroscedasticity**: If you're modeling income (Y) based on education years (X), and you see that the residuals are more spread out for high-income individuals compared to low-income individuals, it may suggest heteroscedasticity.\n",
    "\n",
    "# #### 2. **Plotting Residuals vs. Predictor Variables**:\n",
    "#    - Another approach is to plot the residuals against individual independent variables (predictors). If you observe that the residuals spread out or contract in relation to one or more predictors, this could indicate heteroscedasticity.\n",
    "#    - For example, if you're predicting **house price** based on **square footage**, and the residuals become more dispersed as the square footage increases, this would indicate that the error variance is increasing with the predictor.\n",
    "\n",
    "# #### 3. **Scale-Location Plot** (also called the Spread-Location Plot):\n",
    "#    - This plot shows the **square root of the standardized residuals** on the vertical axis and the **fitted values** on the horizontal axis. In this plot, you want to see a **horizontal line with a constant spread**. If the plot shows a pattern (e.g., a trend or funnel shape), this is a sign of heteroscedasticity.\n",
    "   \n",
    "# #### 4. **Normal Q-Q Plot**:\n",
    "#    - While primarily used to check for the normality of residuals, the **Q-Q plot** can also give hints about heteroscedasticity. If the residuals are not normally distributed and exhibit significant skew or heavy tails, it may suggest heteroscedasticity.\n",
    "   \n",
    "# ### Why Is It Important to Address Heteroscedasticity?\n",
    "\n",
    "# Heteroscedasticity violates one of the key **assumptions of linear regression** (constant variance of the residuals) and can lead to several issues:\n",
    "\n",
    "# #### 1. **Inefficient Estimates**:\n",
    "#    - The presence of heteroscedasticity means that the **ordinary least squares (OLS)** estimators of the regression coefficients are still **unbiased**, but they are no longer **efficient**. This means that the estimates of the coefficients may have higher variance than necessary, making them less precise.\n",
    "\n",
    "#    - In practical terms, this means that the model’s predictions will be less reliable, especially for those data points where the residual variance is large.\n",
    "\n",
    "# #### 2. **Incorrect Inference**:\n",
    "#    - The standard errors of the coefficients can become **biased** in the presence of heteroscedasticity. This leads to incorrect **statistical significance** testing (e.g., t-tests for regression coefficients), making it more likely that you’ll draw incorrect conclusions about the relationships between predictors and the outcome variable.\n",
    "\n",
    "#    - For example, if the residual variance increases at higher values of the predictor, the standard errors might be smaller for those higher values, leading to **incorrectly rejecting the null hypothesis** (Type I error) or **failing to reject the null hypothesis** when it should be rejected (Type II error).\n",
    "\n",
    "# #### 3. **Misleading Confidence Intervals**:\n",
    "#    - Confidence intervals for the regression coefficients will be **wider or narrower than they should be** due to the incorrect estimation of the standard errors. This can affect decision-making, as you might be overly confident or overly cautious in your predictions.\n",
    "\n",
    "# #### 4. **Violation of Assumptions**:\n",
    "#    - Heteroscedasticity violates the **assumption of homoscedasticity**, which is part of the classical linear regression model. While OLS estimators remain unbiased, **best linear unbiased estimation (BLUE)**, a property of OLS in the presence of homoscedasticity, no longer holds in heteroscedastic conditions. This means that alternative estimation methods might be needed.\n",
    "\n",
    "# ### How to Address Heteroscedasticity:\n",
    "\n",
    "# If you detect heteroscedasticity in your regression model, there are several strategies to address it:\n",
    "\n",
    "# #### 1. **Transformation of the Dependent Variable**:\n",
    "#    - You can try **transforming** the dependent variable (Y) to stabilize the variance. Common transformations include:\n",
    "#      - **Log transformation** (e.g., \\( Y' = \\log(Y) \\)) to compress large values and spread out smaller values.\n",
    "#      - **Square root** or **inverse transformations** may also work in some cases.\n",
    "   \n",
    "#    - For instance, if the data shows increasing variance with increasing fitted values, applying a log transformation might help stabilize the variance.\n",
    "\n",
    "# #### 2. **Weighted Least Squares (WLS)**:\n",
    "#    - **Weighted Least Squares** is a method that assigns a **weight** to each observation based on the variance of its residuals. It helps to correct for heteroscedasticity by giving less weight to points with larger residuals (i.e., more variance).\n",
    "#    - This method modifies the OLS approach to account for the fact that some observations have larger variability than others.\n",
    "\n",
    "# #### 3. **Robust Standard Errors**:\n",
    "#    - You can compute **robust standard errors** that adjust for heteroscedasticity. These are also known as **Huber-White standard errors**. They provide more accurate standard errors and significance tests in the presence of heteroscedasticity.\n",
    "#    - Using robust standard errors ensures that statistical tests (like t-tests) and confidence intervals are valid even if heteroscedasticity is present.\n",
    "\n",
    "# #### 4. **Model Re-specification**:\n",
    "#    - It may be worth considering whether there is a **non-linear relationship** between the predictors and the dependent variable. In some cases, heteroscedasticity can be addressed by including non-linear terms (e.g., polynomial terms) or interaction terms that better capture the underlying data structure.\n",
    "\n",
    "# #### 5. **Segmentation**:\n",
    "#    - If heteroscedasticity arises from specific subgroups or conditions in the data, you might consider **segmenting** the data into different groups and fitting separate models for each group.\n",
    "\n",
    "# ---\n",
    "\n",
    "# ### Summary:\n",
    "\n",
    "# - **Heteroscedasticity** is identified by inspecting residual plots (such as **residuals vs. fitted values** and **scale-location plots**) for patterns like increasing or decreasing spread of residuals.\n",
    "# - **Why it matters**: Heteroscedasticity can cause inefficiency in regression estimates, lead to incorrect statistical inference, and bias confidence intervals and significance tests.\n",
    "# - **Addressing it**: You can use strategies like transforming the dependent variable, applying weighted least squares, computing robust standard errors, or re-specifying the model to better account for the non-constant variance.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cffeafa8-ee3a-4bbd-9891-931aaf632571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # question 21 >> What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²\n",
    "\n",
    "# If a **Multiple Linear Regression** model has a **high R²** but a **low adjusted R²**, it typically indicates that the model has a good fit to the data in terms of **explaining variance**, but it may not be accounting for the data in the most **efficient** or **appropriate** way. Specifically, this situation suggests that the model might have too many **irrelevant predictors** or features that are **overfitting** the data.\n",
    "\n",
    "# Here's a breakdown of what it means:\n",
    "\n",
    "# ### 1. **High R²**:\n",
    "#    - **R² (Coefficient of Determination)** measures the proportion of the variance in the **dependent variable** that is explained by the independent variables in the model.\n",
    "#    - A **high R²** indicates that a large portion of the variance in the dependent variable is being explained by the model. However, a high R² on its own doesn't necessarily mean that the model is good or that it is generalized well to new data.\n",
    "#    - **Potential Issue**: R² always increases when you add more predictors to the model, even if those predictors have little to no real relationship with the outcome. This can lead to a misleadingly high R² if you're adding irrelevant predictors.\n",
    "\n",
    "# ### 2. **Low Adjusted R²**:\n",
    "#    - **Adjusted R²** is a modified version of R² that accounts for the number of predictors in the model. It adjusts the value of R² by penalizing the model for adding unnecessary predictors, making it a better measure of the model's goodness of fit.\n",
    "#    - Unlike R², which only increases or stays the same when new predictors are added, **Adjusted R²** can decrease if the new predictors do not improve the model enough to justify their inclusion.\n",
    "#    - **Low Adjusted R²** suggests that the model may be overfitting the data by including too many irrelevant predictors or features that do not meaningfully contribute to explaining the dependent variable.\n",
    "\n",
    "# ### Key Insights from High R² but Low Adjusted R²:\n",
    "\n",
    "# 1. **Overfitting**:\n",
    "#    - The model may be **overfitting** the training data. Overfitting occurs when the model becomes too complex, capturing noise or random fluctuations in the data rather than the true underlying relationship. When this happens, the model performs well on the training data (reflected in a high R²) but may not generalize well to new, unseen data.\n",
    "#    - Adding more predictors may artificially inflate R², but the adjusted R² penalizes this and reduces its value.\n",
    "\n",
    "# 2. **Irrelevant Predictors**:\n",
    "#    - A **high R² and low adjusted R²** can indicate that the model includes too many predictors, some of which may be irrelevant or only weakly related to the dependent variable.\n",
    "#    - The inclusion of irrelevant predictors **increases the complexity** of the model without improving its ability to predict the outcome, which causes adjusted R² to drop.\n",
    "\n",
    "# 3. **Model Efficiency**:\n",
    "#    - **Adjusted R²** provides a more reliable indication of model quality by considering both the number of predictors and the goodness of fit. A **low adjusted R²** means that, although the model might have a high R², it isn't necessarily a **parsimonious** model (a model that explains the variance with as few predictors as necessary).\n",
    "#    - Ideally, you want a **high adjusted R²**, which reflects a model that explains a lot of the variance but doesn't have too many predictors.\n",
    "\n",
    "# ### What to Do If You Encounter High R² but Low Adjusted R²:\n",
    "\n",
    "# 1. **Reevaluate the Model’s Predictors**:\n",
    "#    - **Remove irrelevant or highly correlated predictors**. If the model includes predictors that don’t contribute much to explaining the variance in the dependent variable, removing them can improve the adjusted R².\n",
    "#    - **Check for multicollinearity**: Highly correlated predictors can inflate the R² without adding much value to the model. Consider using techniques like **Variance Inflation Factor (VIF)** to check for multicollinearity.\n",
    "\n",
    "# 2. **Model Selection**:\n",
    "#    - Consider using **stepwise regression** or other model selection techniques (such as **LASSO** or **Ridge regression**) that penalize the inclusion of irrelevant predictors. These methods can help you identify the most important predictors and remove the unnecessary ones.\n",
    "   \n",
    "# 3. **Cross-validation**:\n",
    "#    - To assess the model's generalizability, use **cross-validation** techniques (like k-fold cross-validation) to evaluate how the model performs on new, unseen data. This helps in identifying overfitting and ensures that the model is not just fitting noise in the data.\n",
    "\n",
    "# 4. **Transforming Variables**:\n",
    "#    - Consider transforming variables (e.g., using **logarithms**, **polynomials**, or **interaction terms**) if they have a non-linear relationship with the dependent variable. This can improve the model’s performance without adding too many predictors.\n",
    "\n",
    "# ---\n",
    "\n",
    "# ### Summary:\n",
    "\n",
    "# - **High R²** suggests that the model explains a large portion of the variance, but it can be misleading if the model has too many predictors.\n",
    "# - **Low Adjusted R²** indicates that the model may have too many predictors that are not contributing much to the explanation of the dependent variable and that the model might be overfitting the data.\n",
    "# - To improve the model, focus on **removing irrelevant predictors**, addressing **multicollinearity**, using **model selection techniques**, and validating the model's performance with **cross-validation**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2d49dfe0-9440-49a6-af8b-c1dd37b81e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # question 22 >> Why is it important to scale variables in Multiple Linear Regression\n",
    "\n",
    "# **Scaling variables** in **Multiple Linear Regression (MLR)** is important for several reasons, particularly when the independent variables have different units or magnitudes. Scaling involves transforming the variables to a common scale, often through techniques like **standardization** (e.g., subtracting the mean and dividing by the standard deviation) or **normalization** (scaling the data to a range, such as [0,1]).\n",
    "\n",
    "# Here are the key reasons why scaling variables is crucial in MLR:\n",
    "\n",
    "# ### 1. **Ensuring Comparable Units**:\n",
    "#    - In multiple regression, the independent variables (predictors) might have different units of measurement. For instance, one predictor could be in **dollars**, while another might be in **years**. If the variables are on different scales, the regression coefficients will reflect their scale, not necessarily their importance in explaining the dependent variable.\n",
    "#    - Without scaling, a predictor with a larger range or unit (e.g., income in thousands) might dominate the model, even if other variables (e.g., age in years) are equally important but have smaller numerical values.\n",
    "   \n",
    "#    **Example**: \n",
    "#    - Suppose you have two predictors: **Age** (measured in years) and **Income** (measured in thousands of dollars). If you don't scale these variables, the regression model might place more weight on income because its values are much larger in magnitude compared to age, even though both variables are important.\n",
    "\n",
    "# ### 2. **Improving Interpretability of Coefficients**:\n",
    "#    - When variables are scaled, the **regression coefficients** represent the change in the dependent variable for a **one-unit change** in each predictor, but now the unit of measurement is consistent across all predictors.\n",
    "#    - In **standardized regression**, where variables are scaled to have a mean of 0 and a standard deviation of 1, the coefficients can be interpreted in terms of **standard deviations**. This makes it easier to compare the relative importance of each predictor, as all the predictors are now on the same scale.\n",
    "\n",
    "#    **Example**:\n",
    "#    - After scaling, a coefficient of 2 for **Income** means that for each standard deviation increase in income, the dependent variable (e.g., **spending**) will increase by 2 units. Similarly, a coefficient of 1.5 for **Age** means that for each standard deviation increase in age, the dependent variable will increase by 1.5 units. This makes the comparison between variables more meaningful.\n",
    "\n",
    "# ### 3. **Ensuring Numerical Stability and Convergence**:\n",
    "#    - When you have predictors with very different scales, the optimization algorithm used in **regression** (e.g., **gradient descent**) might struggle with **numerical instability**. The algorithm could take longer to converge or may not converge at all.\n",
    "#    - **Gradient descent** relies on the values of the regression coefficients being updated iteratively. If one variable has much larger values than others, the algorithm might update the coefficients of large-scale variables too quickly, causing instability in the process. Scaling the variables ensures that all predictors contribute similarly to the optimization process and that the algorithm converges more efficiently.\n",
    "\n",
    "# ### 4. **Addressing Multicollinearity**:\n",
    "#    - **Multicollinearity** occurs when two or more predictors are highly correlated with each other. While scaling doesn’t directly solve multicollinearity, it can make it easier to detect and address. Without scaling, one predictor might appear to have a stronger or weaker relationship with the dependent variable simply due to its scale, masking potential multicollinearity issues.\n",
    "#    - For example, if two predictors are highly correlated but one has much larger values than the other, their relationship might be obscured, leading to inflated standard errors and unreliable coefficient estimates.\n",
    "\n",
    "# ### 5. **Regularization Methods (Ridge, Lasso)**:\n",
    "#    - When using **regularization techniques** like **Ridge Regression** or **Lasso Regression**, which apply penalties to the size of regression coefficients, scaling becomes critical. These methods penalize large coefficients, and if the variables are not scaled, predictors with larger magnitudes can dominate the penalty term.\n",
    "#    - Scaling ensures that all variables are penalized equally, regardless of their original scale, which leads to more balanced and fair regularization.\n",
    "\n",
    "#    **Example**:\n",
    "#    - Without scaling, a variable with large values (e.g., income in thousands) could have a large coefficient, and Ridge or Lasso might penalize this coefficient more heavily, even if it's not more important in explaining the target variable.\n",
    "\n",
    "# ### 6. **Facilitating Feature Selection**:\n",
    "#    - When features are on different scales, **feature selection** algorithms (such as **stepwise regression** or **Lasso**) might have difficulty identifying the most important predictors because they may wrongly prioritize variables with larger values. Scaling the features ensures that the feature selection process is based on the true relationship between predictors and the dependent variable, rather than their scale.\n",
    "\n",
    "# ---\n",
    "\n",
    "# ### Summary of Why Scaling is Important:\n",
    "\n",
    "# 1. **Comparable Units**: It ensures that all predictors are on the same scale, so no single predictor dominates the model due to its larger unit or range.\n",
    "# 2. **Interpretability**: Scaled variables make it easier to interpret the coefficients, especially when using standardized regression models.\n",
    "# 3. **Numerical Stability**: It helps with the stability and convergence of optimization algorithms, particularly gradient descent.\n",
    "# 4. **Multicollinearity Detection**: Scaling can make it easier to detect and handle multicollinearity, leading to more reliable coefficient estimates.\n",
    "# 5. **Regularization**: Scaling is crucial for regularization methods (Ridge, Lasso) to ensure that all predictors are penalized equally.\n",
    "# 6. **Feature Selection**: It ensures that feature selection methods treat all predictors fairly and don’t bias the selection based on scale.\n",
    "\n",
    "# ---\n",
    "\n",
    "# ### Common Scaling Methods:\n",
    "# 1. **Standardization** (Z-score normalization):  \n",
    "#    - Subtract the mean and divide by the standard deviation for each variable.  \n",
    "#    \\[\n",
    "#    z = \\frac{X - \\mu}{\\sigma}\n",
    "#    \\]\n",
    "#    Where \\( \\mu \\) is the mean and \\( \\sigma \\) is the standard deviation.\n",
    "\n",
    "# 2. **Normalization** (Min-Max scaling):  \n",
    "#    - Scale the variables to a specific range, usually [0, 1].  \n",
    "#    \\[\n",
    "#    X' = \\frac{X - X_{min}}{X_{max} - X_{min}}\n",
    "#    \\]\n",
    "\n",
    "# Choosing between **standardization** and **normalization** depends on the nature of the data and the requirements of the analysis. Standardization is often preferred for **regression models** because it doesn't bound the data within a specific range, which can be more appropriate for many statistical methods.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "deafecf2-27ca-43e2-a5b4-0b8ad8a19cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # question 23 >> What is polynomial regression\n",
    "\n",
    "# **Polynomial Regression** is a type of **non-linear regression** where the relationship between the independent variable \\( X \\) and the dependent variable \\( Y \\) is modeled as an **n-th degree polynomial** rather than a linear function. Essentially, polynomial regression allows for a curved relationship between the variables instead of just a straight line, which can be useful when the data shows a more complex, non-linear pattern.\n",
    "\n",
    "# In its basic form, **polynomial regression** extends **simple linear regression** by adding higher-degree terms of the independent variable. The model can capture **curved relationships** by including powers of the predictor variable as additional features.\n",
    "\n",
    "# ### The General Form of a Polynomial Regression Equation:\n",
    "\n",
    "# For a single predictor variable \\( X \\), the polynomial regression model of degree \\( n \\) can be written as:\n",
    "\n",
    "# \\[\n",
    "# Y = \\beta_0 + \\beta_1 X + \\beta_2 X^2 + \\beta_3 X^3 + \\cdots + \\beta_n X^n + \\epsilon\n",
    "# \\]\n",
    "\n",
    "# Where:\n",
    "# - \\( Y \\) is the dependent variable.\n",
    "# - \\( X \\) is the independent variable.\n",
    "# - \\( \\beta_0, \\beta_1, \\beta_2, \\dots, \\beta_n \\) are the regression coefficients (parameters to be estimated).\n",
    "# - \\( X^2, X^3, \\dots, X^n \\) are the higher-degree terms of \\( X \\).\n",
    "# - \\( \\epsilon \\) is the error term.\n",
    "\n",
    "# ### Key Points:\n",
    "# 1. **Higher Degree Terms**:\n",
    "#    - The inclusion of **higher powers** of the independent variable \\( X \\) (such as \\( X^2, X^3 \\), etc.) allows the regression line to bend or curve, accommodating more complex patterns in the data.\n",
    "   \n",
    "# 2. **Degree of the Polynomial**:\n",
    "#    - The **degree \\( n \\)** of the polynomial determines the **degree of curvature** the model can capture. For instance:\n",
    "#      - A **degree 2 polynomial** (quadratic regression) will fit a **parabolic** curve to the data.\n",
    "#      - A **degree 3 polynomial** (cubic regression) will allow for even more flexible curves with potentially multiple inflection points.\n",
    "   \n",
    "#    - The higher the degree, the more flexible the model becomes, but this also increases the risk of **overfitting**, where the model captures noise rather than the true underlying pattern.\n",
    "\n",
    "# 3. **Non-Linear Relationships**:\n",
    "#    - Polynomial regression is useful when the data exhibits **non-linear relationships** that a straight line cannot accurately model. For example, if the data shows an upward or downward curve, polynomial regression can fit that shape.\n",
    "\n",
    "# ### Example of Polynomial Regression with Degree 2 (Quadratic Regression):\n",
    "\n",
    "# If you have a dataset where the relationship between \\( X \\) and \\( Y \\) is approximately quadratic (e.g., the data follows a U-shape or inverted U-shape), you can fit a **quadratic polynomial regression model**. The equation for a degree 2 polynomial would be:\n",
    "\n",
    "# \\[\n",
    "# Y = \\beta_0 + \\beta_1 X + \\beta_2 X^2 + \\epsilon\n",
    "# \\]\n",
    "\n",
    "# Here, \\( \\beta_2 \\) is the coefficient of the quadratic term \\( X^2 \\), which allows the curve to bend.\n",
    "\n",
    "# ### Advantages of Polynomial Regression:\n",
    "# 1. **Captures Non-Linear Relationships**: It is especially useful when the data exhibits a curvilinear or non-linear relationship that cannot be modeled by a straight line.\n",
    "# 2. **Flexible Fit**: It provides flexibility in modeling data with complex patterns, such as parabolas or more intricate curves.\n",
    "# 3. **Simple Extension of Linear Regression**: Polynomial regression can be viewed as an extension of linear regression, where we add polynomial terms of the predictor variables.\n",
    "\n",
    "# ### Disadvantages of Polynomial Regression:\n",
    "# 1. **Overfitting**:\n",
    "#    - **Higher-degree polynomials** can result in **overfitting**, where the model fits the training data perfectly but fails to generalize to unseen data.\n",
    "#    - As the degree increases, the model can become too \"wiggly,\" capturing noise in the data and leading to poor predictions.\n",
    "   \n",
    "# 2. **Complexity**:\n",
    "#    - The model becomes more complex as the degree of the polynomial increases, making it harder to interpret and explain.\n",
    "   \n",
    "# 3. **Sensitive to Outliers**:\n",
    "#    - Polynomial regression models can be sensitive to outliers. If outliers are present in the data, they can disproportionately affect the fit of the polynomial curve.\n",
    "\n",
    "# 4. **Extrapolation Issues**:\n",
    "#    - Polynomial regression models can behave unpredictably for values of \\( X \\) outside the range of the training data, leading to unreliable predictions (especially for higher-degree polynomials).\n",
    "\n",
    "# ### Example:\n",
    "# Suppose you're analyzing the relationship between **advertising budget (X)** and **sales (Y)**. If you fit a **linear regression model**, it might not capture the relationship accurately if the effect of advertising on sales diminishes after a certain budget threshold. By using **polynomial regression**, you could model the diminishing returns and better fit the data, for example:\n",
    "\n",
    "# \\[\n",
    "# \\text{Sales} = \\beta_0 + \\beta_1 (\\text{Advertising Budget}) + \\beta_2 (\\text{Advertising Budget})^2\n",
    "# \\]\n",
    "\n",
    "# This quadratic model would allow the sales increase to slow down after a certain advertising budget level, capturing a more realistic relationship.\n",
    "\n",
    "# ---\n",
    "\n",
    "# ### Conclusion:\n",
    "# **Polynomial regression** is a powerful tool when the relationship between the independent and dependent variables is non-linear. By adding higher-degree terms, you can model more complex relationships. However, it's important to balance flexibility with the risk of **overfitting** and to carefully choose the degree of the polynomial to avoid unnecessary complexity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4dd95d31-d1d7-4be5-8abd-cc7ff1323312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # question 24 >> How does polynomial regression differ from linear regression\n",
    "\n",
    "# **Polynomial Regression** and **Linear Regression** are both methods used to model the relationship between one or more independent variables (predictors) and a dependent variable. However, they differ in how they model that relationship, especially when the data exhibits non-linear patterns.\n",
    "\n",
    "# ### Key Differences Between Polynomial Regression and Linear Regression:\n",
    "\n",
    "# ---\n",
    "\n",
    "# ### 1. **Model Structure:**\n",
    "\n",
    "# - **Linear Regression**:\n",
    "#   - The relationship between the dependent variable \\( Y \\) and the independent variable(s) \\( X \\) is modeled as a straight line (or hyperplane in the case of multiple predictors).\n",
    "#   - The equation for simple linear regression with one predictor \\( X \\) is:\n",
    "#     \\[\n",
    "#     Y = \\beta_0 + \\beta_1 X + \\epsilon\n",
    "#     \\]\n",
    "#     Where:\n",
    "#     - \\( \\beta_0 \\) is the intercept (constant).\n",
    "#     - \\( \\beta_1 \\) is the slope of the line.\n",
    "#     - \\( X \\) is the independent variable.\n",
    "#     - \\( \\epsilon \\) is the error term.\n",
    "\n",
    "# - **Polynomial Regression**:\n",
    "#   - The relationship between \\( Y \\) and \\( X \\) is modeled as a polynomial (curved) function of degree \\( n \\), which allows the model to capture more complex, non-linear relationships.\n",
    "#   - The equation for polynomial regression (degree 2, quadratic) with one predictor \\( X \\) is:\n",
    "#     \\[\n",
    "#     Y = \\beta_0 + \\beta_1 X + \\beta_2 X^2 + \\epsilon\n",
    "#     \\]\n",
    "#     For higher degrees, the equation includes higher powers of \\( X \\) (e.g., \\( X^3, X^4, \\dots \\)).\n",
    "\n",
    "# ### 2. **Relationship Type (Linear vs. Non-Linear):**\n",
    "\n",
    "# - **Linear Regression**:\n",
    "#   - Assumes a **linear** relationship between the independent variable(s) and the dependent variable. The model fits a straight line to the data points.\n",
    "#   - It works best when the relationship between the variables is approximately linear (i.e., the effect of \\( X \\) on \\( Y \\) is constant).\n",
    "\n",
    "# - **Polynomial Regression**:\n",
    "#   - Assumes a **non-linear** relationship between the independent variable(s) and the dependent variable. The model fits a **curved line** (or a multi-dimensional surface if there are multiple predictors).\n",
    "#   - It can handle data that exhibits **quadratic, cubic**, or higher-order relationships, where the effect of \\( X \\) on \\( Y \\) changes as \\( X \\) increases or decreases (e.g., diminishing returns, U-shaped, or inverted U-shaped patterns).\n",
    "\n",
    "# ### 3. **Flexibility in Curve Fitting:**\n",
    "\n",
    "# - **Linear Regression**:\n",
    "#   - The model can only capture straight-line relationships. It is **less flexible** when the data shows non-linear trends.\n",
    "  \n",
    "# - **Polynomial Regression**:\n",
    "#   - More **flexible** because it can model curves by including higher-degree terms (such as \\( X^2, X^3, \\dots \\)), allowing it to fit non-linear data more accurately.\n",
    "#   - By increasing the degree of the polynomial, it can fit more complex relationships, such as **parabolas**, **cubic curves**, and more intricate patterns in the data.\n",
    "\n",
    "# ### 4. **Degree of the Model:**\n",
    "\n",
    "# - **Linear Regression**:\n",
    "#   - The model is always a **first-degree polynomial** (degree 1), meaning it has no higher powers of \\( X \\) except for the linear term \\( X \\).\n",
    "#   - The equation is simple and fits a straight line.\n",
    "\n",
    "# - **Polynomial Regression**:\n",
    "#   - The model can be of **higher degree** (degree 2 or higher). You can add additional terms such as \\( X^2, X^3, X^4, \\dots \\), which allows the model to capture **curved relationships**.\n",
    "#   - The degree of the polynomial can be adjusted depending on how well the model fits the data. A higher degree can model more complex patterns, but it can also increase the risk of overfitting.\n",
    "\n",
    "# ### 5. **Complexity and Overfitting:**\n",
    "\n",
    "# - **Linear Regression**:\n",
    "#   - Typically, **less prone to overfitting** compared to polynomial regression, as the model is simple and has fewer parameters (just the slope and intercept).\n",
    "#   - However, it may **underfit** the data if the true relationship is non-linear, as it can’t capture curves or complex patterns.\n",
    "\n",
    "# - **Polynomial Regression**:\n",
    "#   - More prone to **overfitting**, especially as the degree of the polynomial increases. If the degree is too high, the model may fit the training data perfectly but fail to generalize to new data.\n",
    "#   - **Overfitting** happens when the model becomes overly flexible and starts capturing noise or small fluctuations in the data, rather than the true underlying trend.\n",
    "\n",
    "# ### 6. **Interpretability:**\n",
    "\n",
    "# - **Linear Regression**:\n",
    "#   - **Easier to interpret** because the model is a simple straight line with only one slope term. Each coefficient directly represents the relationship between a predictor and the outcome variable.\n",
    "#   - Interpretation of the coefficients is straightforward: each unit change in \\( X \\) results in a fixed change in \\( Y \\).\n",
    "\n",
    "# - **Polynomial Regression**:\n",
    "#   - **Harder to interpret**, especially as the degree of the polynomial increases. The coefficients of higher-order terms (like \\( X^2, X^3 \\)) become more abstract, and it can be difficult to explain the relationship between predictors and the dependent variable in a simple manner.\n",
    "#   - For example, in a quadratic regression, the coefficients of both the linear term and quadratic term together determine the curvature, which complicates interpretation.\n",
    "\n",
    "# ### 7. **Use Cases:**\n",
    "\n",
    "# - **Linear Regression**:\n",
    "#   - Used when the relationship between the independent and dependent variables is roughly **linear**. It’s useful for situations where you expect the effect of \\( X \\) on \\( Y \\) to be consistent.\n",
    "#   - Example: Predicting house prices based on square footage, assuming a linear relationship.\n",
    "\n",
    "# - **Polynomial Regression**:\n",
    "#   - Used when you suspect that the relationship between the independent and dependent variables is **non-linear**. It’s ideal for situations where the change in \\( Y \\) with respect to \\( X \\) varies (e.g., increasing at a decreasing rate, or vice versa).\n",
    "#   - Example: Modeling the relationship between advertising spend and sales, where the effect of advertising might diminish after a certain point.\n",
    "\n",
    "# ---\n",
    "\n",
    "# ### Example to Illustrate the Difference:\n",
    "\n",
    "# - **Linear Regression**: If you have data on the relationship between **hours studied** (X) and **exam scores** (Y), and you assume the relationship is linear, you might model it as:\n",
    "#   \\[\n",
    "#   \\text{Score} = \\beta_0 + \\beta_1 (\\text{Hours Studied})\n",
    "#   \\]\n",
    "\n",
    "# - **Polynomial Regression**: If you believe there’s a non-linear relationship (e.g., after a certain number of hours, studying more might have diminishing returns), you could model it as:\n",
    "#   \\[\n",
    "#   \\text{Score} = \\beta_0 + \\beta_1 (\\text{Hours Studied}) + \\beta_2 (\\text{Hours Studied})^2\n",
    "#   \\]\n",
    "#   This would allow the curve to bend, capturing the diminishing returns of studying more.\n",
    "\n",
    "# ---\n",
    "\n",
    "# ### Summary:\n",
    "\n",
    "# - **Linear Regression** is best suited for **straight-line relationships**, while **Polynomial Regression** is used to model **non-linear relationships** by adding higher-degree terms to the model.\n",
    "# - **Polynomial Regression** provides more flexibility and can fit complex curves but also carries the risk of **overfitting** if the degree is too high.\n",
    "# - The choice between **Linear** and **Polynomial** regression depends on the nature of the data and the complexity of the relationship you want to model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5958ecde-c4a8-4ea6-ada9-5e7d707b3531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # question 25 >> When is polynomial regression used\n",
    "\n",
    "# **Polynomial regression** is used when the relationship between the independent variable(s) and the dependent variable is **non-linear** and cannot be adequately captured by a straight line (as in simple linear regression). Polynomial regression introduces higher-degree terms of the independent variables (e.g., \\(X^2, X^3\\)) to allow for curved relationships, making it useful in situations where the data exhibits patterns like **curves, U-shapes**, or **inverted U-shapes**.\n",
    "\n",
    "# Here are some specific scenarios where **polynomial regression** is useful:\n",
    "\n",
    "# ---\n",
    "\n",
    "# ### 1. **When the Relationship Between Variables is Non-Linear**:\n",
    "#    - **Polynomial regression** is ideal when there is an apparent **curved** or **non-linear** relationship between the predictor(s) and the outcome variable.\n",
    "#    - For example, in cases where the effect of the independent variable on the dependent variable changes at different levels of the independent variable, polynomial regression can better model this relationship.\n",
    "\n",
    "#    **Example**:  \n",
    "#    - If you're analyzing **advertising spend** and **sales**, the effect of spending may increase rapidly at first and then level off (diminishing returns). A quadratic or cubic polynomial model would be appropriate here.\n",
    "\n",
    "# ---\n",
    "\n",
    "# ### 2. **When There Are Curves in the Data (U-shaped, Inverted U-shaped)**:\n",
    "#    - Polynomial regression can capture relationships where the data forms a **U-shape** or an **inverted U-shape**.\n",
    "#    - These kinds of curves often appear in economic, biological, and social sciences data, where certain behaviors might increase up to a point and then start decreasing (or vice versa).\n",
    "\n",
    "#    **Example**:  \n",
    "#    - **Employee motivation**: A company's employees may perform well with moderate work hours but become less effective if they work too long. A polynomial regression model can capture this **inverted U-shaped relationship**.\n",
    "\n",
    "# ---\n",
    "\n",
    "# ### 3. **When There is a Need to Model More Complex Patterns**:\n",
    "#    - In cases where a straight line is too simplistic, polynomial regression offers a way to model more **complex** relationships.\n",
    "#    - Adding higher-degree terms (such as \\(X^2\\), \\(X^3\\), etc.) helps the model to \"bend\" and follow the pattern of the data more accurately.\n",
    "\n",
    "#    **Example**:  \n",
    "#    - Modeling the **growth rate of a plant** where the growth accelerates initially and then slows down as the plant matures.\n",
    "\n",
    "# ---\n",
    "\n",
    "# ### 4. **When You Want to Fit Data with Multiple Peaks or Inflection Points**:\n",
    "#    - Polynomial regression is useful when the relationship between the independent and dependent variables has multiple peaks, troughs, or inflection points.\n",
    "#    - For instance, the data might increase, decrease, and then increase again (or the opposite), which can be effectively modeled by higher-degree polynomials.\n",
    "\n",
    "#    **Example**:  \n",
    "#    - **Profit Margins**: The relationship between **marketing expenditures** and **profits** might first increase with spending, then reach a peak, and then decline due to oversaturation, which a polynomial regression model can capture.\n",
    "\n",
    "# ---\n",
    "\n",
    "# ### 5. **In Cases Where You Suspect a Diminishing or Increasing Effect**:\n",
    "#    - Polynomial regression can help model diminishing returns or increasing effects. For example, if the relationship between variables initially increases and then levels off or changes direction, polynomial terms (like \\(X^2\\) or \\(X^3\\)) allow the model to reflect that change.\n",
    "\n",
    "#    **Example**:  \n",
    "#    - The relationship between **education level** and **income** might follow a curve, where income increases with education level but at a decreasing rate beyond a certain point (e.g., master's degree vs. doctorate).\n",
    "\n",
    "# ---\n",
    "\n",
    "# ### 6. **To Improve Fit When Linear Regression Is Insufficient**:\n",
    "#    - If a linear regression model does not provide a good fit (low \\(R^2\\) or high residuals), polynomial regression can sometimes offer a better fit to the data by adding higher-degree terms.\n",
    "#    - However, it’s important to note that using too high a degree can lead to **overfitting**, where the model fits the training data well but does not generalize well to new data.\n",
    "\n",
    "#    **Example**:  \n",
    "#    - In time series analysis, if the data shows fluctuations that are not captured by a linear trend, a polynomial regression model can better represent those changes.\n",
    "\n",
    "# ---\n",
    "\n",
    "# ### 7. **When You Want to Use Regression for Curved Data in **Machine Learning** Applications**:\n",
    "#    - In some machine learning applications where features have non-linear relationships, polynomial regression can be used as a **feature engineering** technique.\n",
    "#    - The features can be transformed into polynomial features (e.g., using \\(X^2, X^3\\), etc.), allowing linear models to learn more complex relationships.\n",
    "\n",
    "#    **Example**:  \n",
    "#    - In **predicting stock prices** based on historical data, polynomial features can capture the non-linear trends in the market.\n",
    "\n",
    "# ---\n",
    "\n",
    "# ### When **Not** to Use Polynomial Regression:\n",
    "\n",
    "# 1. **Overfitting Risk**:\n",
    "#    - If you use a very high-degree polynomial, the model may fit the training data perfectly but fail to generalize to new data. This is called **overfitting**. It's essential to choose the polynomial degree carefully (using cross-validation or regularization techniques).\n",
    "\n",
    "# 2. **Simple Relationships**:\n",
    "#    - If the relationship between the variables is truly linear, polynomial regression may overcomplicate the model and reduce interpretability. **Linear regression** should be used in such cases.\n",
    "\n",
    "# 3. **Large Number of Predictors**:\n",
    "#    - If you have many predictors, polynomial regression with interactions or higher-degree terms can become complex and computationally expensive.\n",
    "\n",
    "# ---\n",
    "\n",
    "# ### Summary:\n",
    "# Polynomial regression is used when:\n",
    "# - The relationship between variables is non-linear.\n",
    "# - You need to capture curved, U-shaped, or inverted U-shaped patterns.\n",
    "# - There are multiple inflection points or peaks in the data.\n",
    "# - Linear regression fails to provide a good fit due to non-linearity.\n",
    "\n",
    "# However, it’s important to balance the complexity of the polynomial degree to avoid **overfitting** and **interpretation difficulties**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "95eb7c69-dfb8-490e-9e1c-d0b441653a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # question 26 >> What is the general equation for polynomial regression\n",
    "\n",
    "# The general equation for **polynomial regression** depends on the degree of the polynomial. Polynomial regression is essentially an extension of linear regression where the independent variable \\(X\\) is raised to higher powers.\n",
    "\n",
    "# ### General Equation for Polynomial Regression\n",
    "\n",
    "# For a **single predictor variable \\(X\\)** and a polynomial of degree \\(n\\), the equation is:\n",
    "\n",
    "# \\[\n",
    "# Y = \\beta_0 + \\beta_1 X + \\beta_2 X^2 + \\beta_3 X^3 + \\dots + \\beta_n X^n + \\epsilon\n",
    "# \\]\n",
    "\n",
    "# Where:\n",
    "# - \\(Y\\) is the dependent variable (the outcome you are predicting).\n",
    "# - \\(X\\) is the independent variable (the predictor).\n",
    "# - \\( \\beta_0 \\) is the **intercept** (the constant term).\n",
    "# - \\( \\beta_1, \\beta_2, \\dots, \\beta_n \\) are the **coefficients** (parameters) for each corresponding power of \\(X\\).\n",
    "# - \\( X^2, X^3, \\dots, X^n \\) are the **higher-degree terms** of the predictor \\(X\\).\n",
    "# - \\( \\epsilon \\) is the **error term** (random noise or residual).\n",
    "\n",
    "# ### Example for Polynomial Regression (Degree 2):\n",
    "\n",
    "# If you have a **quadratic polynomial regression** (degree 2), the equation becomes:\n",
    "\n",
    "# \\[\n",
    "# Y = \\beta_0 + \\beta_1 X + \\beta_2 X^2 + \\epsilon\n",
    "# \\]\n",
    "\n",
    "# This equation fits a **parabola** to the data.\n",
    "\n",
    "# ### Example for Polynomial Regression (Degree 3):\n",
    "\n",
    "# If you have a **cubic polynomial regression** (degree 3), the equation becomes:\n",
    "\n",
    "# \\[\n",
    "# Y = \\beta_0 + \\beta_1 X + \\beta_2 X^2 + \\beta_3 X^3 + \\epsilon\n",
    "# \\]\n",
    "\n",
    "# This equation fits a **cubic curve** to the data.\n",
    "\n",
    "# ### Important Notes:\n",
    "# 1. The **degree \\(n\\)** of the polynomial controls how much flexibility the model has to fit the data. The higher the degree, the more the curve can bend.\n",
    "# 2. As the degree increases, the polynomial can capture more complex patterns but also increases the risk of **overfitting**.\n",
    "# 3. If there are multiple predictor variables \\(X_1, X_2, \\dots, X_k\\), the polynomial regression equation becomes:\n",
    "\n",
    "# \\[\n",
    "# Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_k X_k + \\beta_{k+1} X_1^2 + \\beta_{k+2} X_2^2 + \\dots + \\beta_{k+n} X_k^n + \\epsilon\n",
    "# \\]\n",
    "\n",
    "# In this case, polynomial terms of each predictor variable are included (interaction terms between predictors could also be added if needed).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0940c794-7ebd-44fe-89f6-dff04e9af859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # question 27 >> Can polynomial regression be applied to multiple variables\n",
    "\n",
    "# Yes, **polynomial regression** can be applied to **multiple variables**. When polynomial regression is extended to multiple predictor variables, it is known as **Multiple Polynomial Regression**. This involves creating polynomial terms for each of the predictor variables (and their interactions) to capture more complex relationships between the dependent variable and multiple predictors.\n",
    "\n",
    "# ### Multiple Polynomial Regression\n",
    "\n",
    "# In the case of multiple predictor variables, the general equation for a **multiple polynomial regression** model with polynomial terms of degree \\(n\\) for each predictor is:\n",
    "\n",
    "# \\[\n",
    "# Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_k X_k + \\beta_{k+1} X_1^2 + \\beta_{k+2} X_2^2 + \\dots + \\beta_{k+n} X_k^2 + \\dots + \\epsilon\n",
    "# \\]\n",
    "\n",
    "# Where:\n",
    "# - \\(Y\\) is the dependent variable (target).\n",
    "# - \\(X_1, X_2, \\dots, X_k\\) are the independent variables (predictors).\n",
    "# - \\(\\beta_0\\) is the intercept.\n",
    "# - \\(\\beta_1, \\beta_2, \\dots, \\beta_k\\) are the coefficients for the linear terms of the predictors.\n",
    "# - \\(\\beta_{k+1}, \\beta_{k+2}, \\dots\\) are the coefficients for the polynomial terms (e.g., \\(X_1^2, X_2^2\\)).\n",
    "# - Higher-order terms such as \\(X_1^3, X_2^3, X_1X_2, \\dots\\) can also be added to model interactions between variables.\n",
    "# - \\(\\epsilon\\) is the error term.\n",
    "\n",
    "# ### Steps Involved in Multiple Polynomial Regression:\n",
    "# 1. **Create Polynomial Terms**: For each independent variable, create polynomial terms up to the desired degree. For example, if you have two predictors \\(X_1\\) and \\(X_2\\), you could include \\(X_1^2\\), \\(X_2^2\\), and \\(X_1X_2\\) (interaction term).\n",
    "   \n",
    "# 2. **Interaction Terms**: Include interaction terms between the predictor variables. These terms allow the model to capture the effect of combinations of predictor variables on the dependent variable.\n",
    "   \n",
    "#    Example: Interaction between \\(X_1\\) and \\(X_2\\) would be represented as \\(X_1 \\cdot X_2\\) (product of the two variables).\n",
    "   \n",
    "# 3. **Model Fitting**: Fit the model with all these terms using the least squares method to estimate the coefficients of the polynomial terms.\n",
    "\n",
    "# ### Example for Two Predictor Variables:\n",
    "\n",
    "# If you have two predictor variables, \\(X_1\\) and \\(X_2\\), and want to fit a **quadratic polynomial** model (degree 2), the equation would be:\n",
    "\n",
    "# \\[\n",
    "# Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_1^2 + \\beta_4 X_2^2 + \\beta_5 X_1 X_2 + \\epsilon\n",
    "# \\]\n",
    "\n",
    "# Here:\n",
    "# - \\(\\beta_1 X_1 + \\beta_2 X_2\\) are the linear terms.\n",
    "# - \\(\\beta_3 X_1^2 + \\beta_4 X_2^2\\) are the quadratic terms.\n",
    "# - \\(\\beta_5 X_1 X_2\\) is the interaction term.\n",
    "\n",
    "# This equation models a **curved surface** in a 3D space formed by \\(X_1\\), \\(X_2\\), and \\(Y\\).\n",
    "\n",
    "# ### Adding Higher-Degree Terms:\n",
    "# If you wanted a **cubic** model (degree 3), you could extend it to include higher-degree terms like \\(X_1^3\\), \\(X_2^3\\), and additional interaction terms.\n",
    "\n",
    "# \\[\n",
    "# Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_1^2 + \\beta_4 X_2^2 + \\beta_5 X_1 X_2 + \\beta_6 X_1^3 + \\beta_7 X_2^3 + \\epsilon\n",
    "# \\]\n",
    "\n",
    "# ### Key Points:\n",
    "# 1. **Flexibility**: Multiple polynomial regression is very flexible and can fit complex, curved relationships between multiple predictors and the outcome variable.\n",
    "   \n",
    "# 2. **Overfitting**: As you add higher-degree terms, you increase the complexity of the model, which can lead to **overfitting** if the degree is too high. It’s important to evaluate the model using techniques like cross-validation to avoid overfitting.\n",
    "\n",
    "# 3. **Interpretability**: As you add more polynomial terms and interaction terms, the model becomes more complex and harder to interpret. While polynomial regression can fit complex relationships, it sacrifices **interpretability** as the number of terms increases.\n",
    "\n",
    "# 4. **Interaction Effects**: Including interaction terms between predictors allows the model to account for how the combination of predictors influences the dependent variable, which is often not captured by linear regression alone.\n",
    "\n",
    "# ### Summary:\n",
    "# Polynomial regression can be applied to **multiple variables** by adding polynomial terms for each predictor and including interaction terms between the predictors. This allows the model to capture complex, non-linear relationships between the predictors and the dependent variable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "488bdf4f-dcea-4619-a504-fcbccb722d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # question 28 >> What are the limitations of polynomial regression\n",
    "\n",
    "# While **polynomial regression** can be a powerful tool for modeling non-linear relationships, it also comes with several **limitations** that you should consider before using it. Here are some key limitations of polynomial regression:\n",
    "\n",
    "# ### 1. **Overfitting**:\n",
    "#    - **Overfitting** is one of the most common problems with polynomial regression, especially when using high-degree polynomials.\n",
    "#    - The model can fit the training data **very closely**, capturing not just the underlying relationship but also the **random noise** in the data.\n",
    "#    - This can result in a model that works well on the training data but poorly on new, unseen data (low generalizability).\n",
    "#    - **Solution**: Regularization techniques (like Lasso or Ridge regression) or using **cross-validation** can help reduce overfitting.\n",
    "\n",
    "# ### 2. **Increased Model Complexity**:\n",
    "#    - As the degree of the polynomial increases, the number of terms in the model increases as well.\n",
    "#    - This leads to a **more complex model** that is harder to interpret, especially if you have a large number of predictor variables.\n",
    "#    - Polynomial regression can become difficult to explain to stakeholders or use for decision-making as the model becomes more intricate.\n",
    "\n",
    "# ### 3. **Sensitive to Outliers**:\n",
    "#    - Polynomial regression is **sensitive to outliers** in the data, especially when using higher-degree polynomials.\n",
    "#    - Outliers can significantly influence the curve, causing the model to fit poorly and give misleading predictions.\n",
    "#    - **Solution**: Outlier detection and removal or robust regression techniques can help address this.\n",
    "\n",
    "# ### 4. **Instability with High-Degree Polynomials**:\n",
    "#    - Higher-degree polynomials can become **numerically unstable** when fitting the model, particularly if the predictor variables take on large values.\n",
    "#    - This instability may result in **large coefficients** and erratic behavior, making the model less reliable.\n",
    "#    - **Solution**: **Standardizing** or **scaling** the input variables can help mitigate this problem.\n",
    "\n",
    "# ### 5. **Poor Extrapolation**:\n",
    "#    - Polynomial regression can **extrapolate poorly** beyond the range of the training data. If the model is fitted using a higher-degree polynomial, it may predict unrealistic values for new data points that lie outside the training range.\n",
    "#    - This is because the model may produce extreme outputs as the predictor variable moves away from the training data, particularly when the polynomial degree is high.\n",
    "#    - **Solution**: Limiting extrapolation or using simpler models for extrapolation might help.\n",
    "\n",
    "# ### 6. **Interpretability**:\n",
    "#    - As the degree of the polynomial increases, the **interpretability** of the model becomes more difficult.\n",
    "#    - Unlike linear regression, where the effect of each predictor on the dependent variable is easy to interpret, polynomial regression introduces higher-degree terms that complicate the interpretation of how each variable affects the outcome.\n",
    "#    - **Solution**: If interpretability is important, you might want to stick to lower-degree polynomials or consider simpler models like linear regression.\n",
    "\n",
    "# ### 7. **Multicollinearity**:\n",
    "#    - Polynomial regression can introduce **multicollinearity** (correlation between the predictor variables) when you create higher-degree terms of the same variable.\n",
    "#    - For example, when you add \\(X^2\\) and \\(X\\) to the model, they can become highly correlated, making it difficult to estimate the coefficients reliably and leading to unstable predictions.\n",
    "#    - **Solution**: Use **regularization techniques** like Ridge or Lasso regression to deal with multicollinearity.\n",
    "\n",
    "# ### 8. **Difficulty in Choosing the Right Degree**:\n",
    "#    - Deciding the appropriate degree for the polynomial is **non-trivial**. If you choose too low a degree, the model may **underfit** the data (i.e., fail to capture the true relationship). If you choose too high a degree, the model may **overfit** and become overly complex.\n",
    "#    - Finding the right degree typically requires **trial and error** and the use of model evaluation techniques like **cross-validation** to assess model performance.\n",
    "\n",
    "# ### 9. **Assumption of Smoothness**:\n",
    "#    - Polynomial regression assumes that the relationship between the independent and dependent variables is **smooth**, which might not always be true in real-world data.\n",
    "#    - In cases where the data has abrupt changes or discontinuities, polynomial regression may fail to provide a good fit.\n",
    "#    - **Solution**: Non-parametric models like **decision trees** or **splines** might work better in these cases.\n",
    "\n",
    "# ### 10. **Excessive Influence of Higher Powers**:\n",
    "#    - In some cases, the higher-degree terms (like \\(X^n\\)) can become **dominant** and overly influence the model’s predictions, even if they are not significantly important in capturing the relationship.\n",
    "#    - This can result in models that are dominated by a few high-degree terms, leading to less accurate or biased predictions.\n",
    "#    - **Solution**: Regularization methods like **Ridge** or **Lasso regression** can help to prevent overemphasis on certain terms.\n",
    "\n",
    "# ---\n",
    "\n",
    "# ### Summary of Limitations:\n",
    "# 1. **Overfitting** with high-degree polynomials.\n",
    "# 2. Increased **model complexity** as the degree increases.\n",
    "# 3. **Sensitive to outliers**, affecting model accuracy.\n",
    "# 4. **Instability** with high-degree polynomials leading to large coefficients.\n",
    "# 5. **Poor extrapolation** beyond the data range.\n",
    "# 6. Loss of **interpretability** with higher-degree terms.\n",
    "# 7. Potential for **multicollinearity** due to polynomial terms.\n",
    "# 8. **Difficulty** in selecting the optimal polynomial degree.\n",
    "# 9. Assumption of **smoothness** might not fit real-world data.\n",
    "# 10. **Excessive influence** of higher-degree terms on predictions.\n",
    "\n",
    "# ### Alternatives:\n",
    "# - If you find that polynomial regression is not suitable for your data due to these limitations, you might consider other models, such as **decision trees**, **random forests**, **support vector machines (SVMs)**, or **splines**, depending on the nature of your data and the problem you're solving.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a2b31c8b-c647-4400-97f7-602bc4492462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # question 29 >> What methods can be used to evaluate model fit when selecting the degree of a polynomial\n",
    "\n",
    "# When selecting the degree of a polynomial for a regression model, it's crucial to evaluate how well the model fits the data while avoiding overfitting. Here are several methods you can use to evaluate model fit and select the appropriate degree of the polynomial:\n",
    "\n",
    "# ### 1. **Cross-Validation**\n",
    "#    - **Cross-validation** is one of the most reliable methods for selecting the optimal degree of a polynomial.\n",
    "#    - It involves splitting your data into multiple subsets (folds), training the model on some folds, and validating it on the remaining fold(s). This process is repeated for each fold, and the model’s performance is averaged over all iterations.\n",
    "#    - The idea is to measure the model's ability to generalize to unseen data, which helps you identify the degree that provides the best balance between bias (underfitting) and variance (overfitting).\n",
    "\n",
    "#    **Steps**:\n",
    "#    - Perform **k-fold cross-validation** (typically \\(k = 5\\) or \\(k = 10\\)).\n",
    "#    - Train the polynomial model on different degrees (e.g., degree 1, degree 2, degree 3).\n",
    "#    - Evaluate the model's performance using a metric like **Mean Squared Error (MSE)** or **Root Mean Squared Error (RMSE)** on the validation set.\n",
    "#    - Choose the polynomial degree that minimizes the error on the validation data.\n",
    "\n",
    "# ### 2. **Train-Test Split and Performance Metrics**\n",
    "#    - **Train-test split** involves splitting the data into two sets: one for training the model and one for testing it. \n",
    "#    - You can train the model with different degrees of polynomials, evaluate its performance on the test set, and compare the error metrics.\n",
    "   \n",
    "#    Common performance metrics:\n",
    "#    - **Mean Squared Error (MSE)**: Measures the average squared difference between actual and predicted values.\n",
    "#    - **Root Mean Squared Error (RMSE)**: The square root of MSE, which gives an interpretable error value in the same units as the target variable.\n",
    "#    - **Mean Absolute Error (MAE)**: Measures the average magnitude of the errors without considering their direction.\n",
    "   \n",
    "#    By comparing these error metrics across different polynomial degrees, you can determine which degree produces the lowest error on the test set.\n",
    "\n",
    "# ### 3. **Adjusted \\(R^2\\) (Adjusted Coefficient of Determination)**\n",
    "#    - **\\(R^2\\)** measures the proportion of the variance in the dependent variable that is explained by the model. However, \\(R^2\\) tends to increase as the degree of the polynomial increases, even if the additional complexity doesn’t improve the model.\n",
    "#    - **Adjusted \\(R^2\\)** penalizes models with too many predictors, which helps prevent overfitting.\n",
    "#    - It accounts for the number of predictors in the model, and a higher value indicates a better-fitting model that balances complexity and goodness of fit.\n",
    "\n",
    "#    **Formula for Adjusted \\(R^2\\)**:\n",
    "#    \\[\n",
    "#    R^2_{\\text{adj}} = 1 - \\left(1 - R^2 \\right) \\frac{n - 1}{n - p - 1}\n",
    "#    \\]\n",
    "#    Where:\n",
    "#    - \\(n\\) = number of data points\n",
    "#    - \\(p\\) = number of predictors (polynomial terms)\n",
    "   \n",
    "#    **How to use**:\n",
    "#    - Calculate the adjusted \\(R^2\\) for models with different polynomial degrees.\n",
    "#    - Choose the model with the highest adjusted \\(R^2\\) that doesn’t increase drastically with higher degrees (indicating overfitting).\n",
    "\n",
    "# ### 4. **Akaike Information Criterion (AIC) / Bayesian Information Criterion (BIC)**\n",
    "#    - **AIC** and **BIC** are model selection criteria that consider both the goodness of fit and the complexity of the model.\n",
    "#    - They are useful for comparing models with different degrees of polynomials, as they penalize models with more parameters (higher-degree polynomials).\n",
    "#    - **AIC**: Lower values indicate better models.\n",
    "#    - **BIC**: Similar to AIC, but with a stronger penalty for models with more parameters.\n",
    "\n",
    "#    **Formula for AIC**:\n",
    "#    \\[\n",
    "#    \\text{AIC} = 2p - 2\\ln(L)\n",
    "#    \\]\n",
    "#    Where:\n",
    "#    - \\(p\\) = number of model parameters (polynomial degree)\n",
    "#    - \\(L\\) = likelihood of the model\n",
    "\n",
    "#    **Formula for BIC**:\n",
    "#    \\[\n",
    "#    \\text{BIC} = \\ln(n) \\cdot p - 2\\ln(L)\n",
    "#    \\]\n",
    "#    Where:\n",
    "#    - \\(n\\) = number of observations\n",
    "   \n",
    "#    **How to use**:\n",
    "#    - For each polynomial degree, compute AIC and BIC values.\n",
    "#    - Choose the polynomial degree with the **lowest AIC/BIC**.\n",
    "\n",
    "# ### 5. **Validation Set or Holdout Set**\n",
    "#    - Similar to train-test split, but in this case, you hold out a separate **validation set** (usually 10-20% of your data) that is never used for training.\n",
    "#    - After training the model on the remaining data, evaluate the model on the validation set using performance metrics (like MSE or RMSE).\n",
    "#    - Compare the performance across different polynomial degrees and select the one that performs best on the validation set.\n",
    "\n",
    "# ### 6. **Visual Inspection (Residual Plots)**\n",
    "#    - Plotting **residuals** (the differences between observed and predicted values) for models with different polynomial degrees can give insight into the fit.\n",
    "#    - For a good fit, the residuals should be randomly scattered around zero without any discernible patterns.\n",
    "#    - If you notice patterns (e.g., a curve) in the residual plot, it may indicate that the degree of the polynomial is too low.\n",
    "#    - Visualizing the **actual vs. predicted values** graphically can also help you see how well the model captures the underlying relationship.\n",
    "\n",
    "# ### 7. **Learning Curves**\n",
    "#    - A **learning curve** shows how the model's performance changes as the size of the training data increases. You can use learning curves to check if the polynomial model generalizes well to new data as the training set size increases.\n",
    "#    - If a model’s performance on the validation set improves as the training set size increases, it suggests that the model is not overfitting.\n",
    "#    - By comparing learning curves for models with different polynomial degrees, you can select the degree that provides the best generalization.\n",
    "\n",
    "# ---\n",
    "\n",
    "# ### Summary of Methods:\n",
    "# 1. **Cross-Validation**: Assess the model’s performance on multiple subsets of data to avoid overfitting.\n",
    "# 2. **Train-Test Split and Performance Metrics**: Use metrics like MSE or RMSE to compare models with different polynomial degrees.\n",
    "# 3. **Adjusted \\(R^2\\)**: Choose the model with the highest adjusted \\(R^2\\) that doesn’t overly penalize the model.\n",
    "# 4. **AIC/BIC**: Use AIC and BIC to evaluate models by balancing fit and complexity.\n",
    "# 5. **Validation Set**: Evaluate performance on a holdout set to see how well the model generalizes.\n",
    "# 6. **Residual Plots**: Check for patterns in residuals to detect underfitting or overfitting.\n",
    "# 7. **Learning Curves**: Compare how the model’s performance changes as you increase the training data size.\n",
    "\n",
    "# Using these methods will help you select the polynomial degree that best balances model complexity and generalizability, ultimately leading to a better-performing regression model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f03008f8-01a3-4df4-b68c-85bbb4836653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # question 30 >> Why is visualization important in polynomial regression\n",
    "\n",
    "# **Visualization** plays a critical role in understanding and interpreting **polynomial regression** models. It allows you to visually assess the model’s fit, understand the underlying relationships between variables, and detect potential issues. Here’s why visualization is particularly important in polynomial regression:\n",
    "\n",
    "# ### 1. **Understanding the Shape of the Relationship**\n",
    "#    - **Polynomial regression** is used to model **non-linear** relationships between the independent and dependent variables. Visualizing the data and the fitted polynomial curve helps you better understand the **shape** of the relationship.\n",
    "#    - A **scatter plot** of the data points, overlaid with the polynomial regression curve, can show how well the model captures the underlying trend in the data. \n",
    "#    - For example, if the data shows a **U-shape** or **S-shape** trend, a linear regression model would fail to fit it, but a polynomial regression model can capture these curvatures, and visualization will clearly highlight this.\n",
    "\n",
    "# ### 2. **Detecting Overfitting**\n",
    "#    - Polynomial regression, especially with higher degrees, is prone to **overfitting**. This occurs when the model fits the training data too closely, including noise and outliers, and fails to generalize well to new data.\n",
    "#    - By visualizing the fitted polynomial curve against the data points, you can easily spot if the model is overly **wiggly** or follows the data too tightly. This often indicates overfitting, particularly if the curve oscillates wildly between data points.\n",
    "#    - For instance, a high-degree polynomial might create a curve with unnecessary bends that don't represent the actual underlying trend.\n",
    "\n",
    "# ### 3. **Visualizing Model Fit**\n",
    "#    - Plotting the **predicted values** (from the polynomial regression model) against the **observed values** can help you evaluate how well the model fits the data.\n",
    "#    - **Residual plots** (plotting residuals or errors) are particularly useful for this. If the model fits well, the residuals should be randomly scattered around zero. If there are clear patterns or trends in the residuals, it may indicate that the model isn’t a good fit, or that an inappropriate degree was selected for the polynomial.\n",
    "\n",
    "# ### 4. **Selecting the Degree of the Polynomial**\n",
    "#    - Visualization helps when you need to determine the optimal **degree** for the polynomial. By plotting polynomial curves of different degrees, you can visually inspect which degree captures the underlying trend without overfitting.\n",
    "#    - As the polynomial degree increases, the curve will become more complex, and visualizing the effect of higher-degree terms can help you decide when to stop increasing the degree (to avoid overfitting). \n",
    "#    - You might plot a series of graphs with increasing degrees (degree 1, degree 2, degree 3, etc.) to see how the fit improves or worsens.\n",
    "\n",
    "# ### 5. **Modeling Multivariable Data**\n",
    "#    - When you have multiple independent variables (in **multiple polynomial regression**), you can use **3D plots** to visualize the relationship between two predictors and the dependent variable.\n",
    "#    - For instance, with two predictor variables, a **3D surface plot** can help you visualize how the combination of both predictors influences the dependent variable, allowing you to better understand the complexity of the model.\n",
    "#    - For higher-dimensional data, **contour plots** or **heatmaps** can be used to visualize how polynomial terms interact and affect the outcome variable.\n",
    "\n",
    "# ### 6. **Diagnosing Multicollinearity and Model Issues**\n",
    "#    - Visualization of the **correlation matrix** between predictors can help detect **multicollinearity** (high correlations between predictor variables), which can affect the polynomial regression model’s stability and performance.\n",
    "#    - For polynomial regression, where higher-order terms are added, it's important to ensure that the polynomial terms don't become too correlated with each other, leading to unstable coefficient estimates. Visualizing the correlation structure of the predictors can help identify these issues early.\n",
    "\n",
    "# ### 7. **Checking for Outliers and Influential Points**\n",
    "#    - **Outliers** and **influential points** can significantly affect the polynomial regression model. By visualizing the regression line or curve along with the data points, you can easily spot data points that are far away from the trend, indicating potential outliers or influential points.\n",
    "#    - In polynomial regression, outliers can lead to dramatic changes in the shape of the polynomial curve, particularly in high-degree models.\n",
    "#    - **Leverage plots** and **Cook’s distance plots** can help you identify which data points are most influential in determining the fit of the model.\n",
    "\n",
    "# ### 8. **Visualizing Interaction Effects (For Multiple Polynomial Regression)**\n",
    "#    - In **multiple polynomial regression**, you may have interaction terms between predictors (e.g., \\(X_1 \\times X_2\\)). Visualizing these interactions can help you understand how the combination of predictor variables influences the dependent variable.\n",
    "#    - **3D plots** or **heatmaps** can show how different combinations of predictors affect the outcome, allowing for a more intuitive understanding of the interactions between variables.\n",
    "\n",
    "# ---\n",
    "\n",
    "# ### Types of Visualizations for Polynomial Regression:\n",
    "\n",
    "# 1. **Scatter Plots**: \n",
    "#    - Used to visualize the data points and the fitted polynomial curve. You can see how well the curve fits the data.\n",
    "   \n",
    "# 2. **Line Plot**: \n",
    "#    - A simple line plot showing the **predicted values** (from the polynomial model) against the **actual values** can help assess fit.\n",
    "\n",
    "# 3. **Residual Plots**:\n",
    "#    - Plot the residuals (errors) against the fitted values. Random scattering indicates a good fit; systematic patterns suggest problems.\n",
    "   \n",
    "# 4. **Learning Curves**:\n",
    "#    - These curves show the model’s performance (e.g., MSE or RMSE) on training and validation sets as the polynomial degree increases.\n",
    "\n",
    "# 5. **3D Surface or Contour Plots** (for multiple predictors):\n",
    "#    - Used for visualizing how two predictors influence the dependent variable in a **multiple polynomial regression** model.\n",
    "\n",
    "# 6. **Correlation Matrix Heatmap**:\n",
    "#    - Shows the correlation between predictors and helps detect multicollinearity.\n",
    "\n",
    "# 7. **Leverage or Cook’s Distance Plots**:\n",
    "#    - Used to identify influential data points that may disproportionately affect the model.\n",
    "\n",
    "# ---\n",
    "\n",
    "# ### Conclusion:\n",
    "\n",
    "# Visualization is essential in **polynomial regression** because it allows you to:\n",
    "# - Understand and interpret the relationship between the variables.\n",
    "# - Detect problems such as **overfitting**, **underfitting**, or **multicollinearity**.\n",
    "# - Choose the optimal degree for the polynomial.\n",
    "# - Identify issues like **outliers**, **influential points**, and **model misspecification**.\n",
    "\n",
    "# By leveraging visualization techniques, you can ensure that the polynomial regression model fits the data appropriately and generalizes well to unseen data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1cd38ebc-d9fa-40c9-a243-cd4bfc8664c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.6.1-cp312-cp312-win_amd64.whl.metadata (15 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\rahul\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\rahul\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from scikit-learn) (1.15.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\rahul\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from scikit-learn) (1.4.2)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.6.1-cp312-cp312-win_amd64.whl (11.1 MB)\n",
      "   ---------------------------------------- 0.0/11.1 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/11.1 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 3.7/11.1 MB 14.5 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 7.9/11.1 MB 16.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.1/11.1 MB 17.4 MB/s eta 0:00:00\n",
      "Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scikit-learn\n",
      "Successfully installed scikit-learn-1.6.1 threadpoolctl-3.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: C:\\Users\\Rahul\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9fe7f6ee-3078-4f3a-bf21-0b0e47b69927",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjsAAAHHCAYAAABZbpmkAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAkNlJREFUeJzt3Xd401UXB/Bv2tIBpYUW2gItG9l77ylb2bJkC7I3CshUhqLsqYKAryDKUkCRvZTKFNkIyKZl05bRmfv+cUzaNEmbtmnTpt/P8/TR/FZuCu+b473nnqNRSikQERER2SkHWw+AiIiIKDUx2CEiIiK7xmCHiIiI7BqDHSIiIrJrDHaIiIjIrjHYISIiIrvGYIeIiIjsGoMdIiIismsMdoiIiMiuMdghu9agQQM0aNDA1sOwijVr1kCj0eDmzZtJvrd3794oWLCg1cdkrwoWLIjevXvb7P3nzJmDEiVKQKvVptl7Hjx4EBqNBgcPHkzyvTdv3oRGo8GaNWusPq70okaNGvjggw9sPQxKJgY7lK7ovtB1P66urnjjjTcwdOhQPHjwwNbDs3sNGjQw+P27ubmhXLlyWLBgQZp+8WZmoaGh+Oyzz/Dhhx/CwcEBvXv3NvgzMfdjy+DMXly8eBHTpk0z+R8UH374IZYuXYrg4OC0HxilmJOtB0Bkyscff4xChQohPDwcv//+O5YvX45ff/0V58+fR9asWW09PJvo0aMHunTpAhcXl1R9H39/f8yePRsA8PjxY6xfvx6jRo3Co0ePMHPmzFR97/TiypUrcHCwzX8LfvPNN4iOjkbXrl0BAO+//z6aNGmiP3/jxg1MmTIFAwYMQN26dfXHixQpkqL3rVevHl6/fg1nZ+ck31ugQAG8fv0aWbJkSdEYbO3ixYuYPn06GjRoYDQT2qZNG3h4eGDZsmX4+OOPbTNASjYGO5QutWjRAlWqVAEAvPfee/D29sa8efPw888/678EMhtHR0c4Ojqm+vt4enri3Xff1b8eOHAgSpQogcWLF+Pjjz9OkzHohIeHw9nZOc0Dj9QOKBOyevVqvP3223B1dQUA1KxZEzVr1tSfP3nyJKZMmYKaNWsa/DnF9/LlS2TLls3i93VwcNC/Z1LpZmHtmYODAzp27Ihvv/0W06dPh0ajsfWQKAm4jEUZQqNGjQDIf9UCQHR0ND755BMUKVIELi4uKFiwICZOnIiIiAizz3jx4gWyZcuGESNGGJ27e/cuHB0d9TMauuW0P/74A6NHj0bu3LmRLVs2tGvXDo8ePTK6f9myZShdujRcXFyQN29eDBkyBM+fPze4pkGDBihTpgzOnj2L+vXrI2vWrChatCg2bdoEADh06BCqV68ONzc3FC9eHHv37jW431TOzs8//4xWrVohb968cHFxQZEiRfDJJ58gJiYm8V+qhVxdXVG1alWEhYXh4cOHBue+++47VK5cGW5ubvDy8kKXLl1w584do2csXboUhQsXhpubG6pVq4YjR44Y5VPpckY2bNiASZMmIV++fMiaNStCQ0MBAMeOHUPz5s3h6emJrFmzon79+vjjjz8M3icsLAwjR45EwYIF4eLiAh8fH7z55ps4ffq0/pqrV6+iQ4cO8PPzg6urK/z9/dGlSxeEhITorzGVs/Pvv/+iU6dO8PLyQtasWVGjRg388ssvBtfoPsOPP/6ImTNnwt/fH66urmjcuDGuXbuW6O/6xo0bOHv2rMFMjiV0fzcOHTqEwYMHw8fHB/7+/gCAW7duYfDgwShevDjc3Nzg7e2NTp06GS3VmMrZ0f2dvXjxIho2bIisWbMiX758mDNnjsG9pnJ2evfuDXd3d9y7dw9t27aFu7s7cufOjbFjxxr9/Xzy5Al69OgBDw8P5MiRA7169cLff/9tUR5QVFQUpk+fjmLFisHV1RXe3t6oU6cO9uzZY3Dd5cuX0bFjR3h5ecHV1RVVqlTBtm3bDH6HnTp1AgA0bNhQvzwY9/fx5ptv4tatWzhz5kyCY6L0h8EOZQjXr18HAHh7ewOQ2Z4pU6agUqVKmD9/PurXr4/Zs2ejS5cuZp/h7u6Odu3a4YcffjD6P9vvv/8eSil0797d4PiwYcPw999/Y+rUqRg0aBC2b9+OoUOHGlwzbdo0DBkyBHnz5sXcuXPRoUMHfPnll2jatCmioqIMrn327Blat26N6tWrY86cOXBxcUGXLl3www8/oEuXLmjZsiU+/fRTvHz5Eh07dkRYWFiCv5c1a9bA3d0do0ePxsKFC1G5cmVMmTIF48ePT/gXmkS6L7McOXLoj82cORM9e/ZEsWLFMG/ePIwcORL79u1DvXr1DAK95cuXY+jQofD398ecOXNQt25dtG3bFnfv3jX5Xp988gl++eUXjB07FrNmzYKzszP279+PevXqITQ0FFOnTsWsWbPw/PlzNGrUCMePH9ffO3DgQCxfvhwdOnTAsmXLMHbsWLi5ueHSpUsAgMjISDRr1gx//vknhg0bhqVLl2LAgAH4999/jYLTuB48eIBatWph165dGDx4MGbOnInw8HC8/fbb2Lp1q9H1n376KbZu3YqxY8diwoQJ+PPPP43+bply9OhRAEClSpUSvdaUwYMH4+LFiwZ/B06cOIGjR4+iS5cuWLRoEQYOHIh9+/ahQYMGePXqVaLPfPbsGZo3b47y5ctj7ty5KFGiBD788EPs3Lkz0XtjYmLQrFkzeHt744svvkD9+vUxd+5cfPXVV/prtFot3nrrLXz//ffo1asXZs6ciaCgIPTq1cuizzxt2jRMnz4dDRs2xJIlS/DRRx8hf/78BgHuhQsXUKNGDVy6dAnjx4/H3LlzkS1bNrRt21b/51evXj0MHz4cADBx4kT873//w//+9z+ULFlS/5zKlSsDgFGQTRmAIkpHVq9erQCovXv3qkePHqk7d+6oDRs2KG9vb+Xm5qbu3r2rzpw5owCo9957z+DesWPHKgBq//79+mP169dX9evX17/etWuXAqB27txpcG+5cuUMrtONo0mTJkqr1eqPjxo1Sjk6Oqrnz58rpZR6+PChcnZ2Vk2bNlUxMTH665YsWaIAqG+++cZgLADU+vXr9ccuX76sACgHBwf1559/Go1z9erVRmO6ceOG/tirV6+Mfofvv/++ypo1qwoPD9cf69WrlypQoIDRtfHVr19flShRQj169Eg9evRIXb58WY0bN04BUK1atdJfd/PmTeXo6KhmzpxpcP+5c+eUk5OT/nhERITy9vZWVatWVVFRUfrr1qxZowAY/M4PHDigAKjChQsbfC6tVquKFSummjVrZvBn8erVK1WoUCH15ptv6o95enqqIUOGmP18f/31lwKgNm7cmODvoUCBAqpXr1761yNHjlQA1JEjR/THwsLCVKFChVTBggX1f/a6z1CyZEkVERGhv3bhwoUKgDp37lyC7ztp0iQFQIWFhZm95sSJE2b/btSpU0dFR0cbXG/q70hgYKACoL799lv9Md3YDxw4oD+m+zsb97qIiAjl5+enOnTooD9248YNozH16tVLAVAff/yxwXtXrFhRVa5cWf968+bNCoBasGCB/lhMTIxq1KiR0TNNKV++vMHfTVMaN26sypYta/C/Ca1Wq2rVqqWKFSumP7Zx40aj30F8zs7OatCgQQm+H6U/nNmhdKlJkybInTs3AgIC0KVLF7i7u2Pr1q3Ily8ffv31VwDA6NGjDe4ZM2YMABgtLcR/bt68ebFu3Tr9sfPnz+Ps2bMm8x8GDBhgsDZft25dxMTE4NatWwCAvXv3IjIyEiNHjjTIK+nfvz88PDyMxuLu7m4w+1S8eHHkyJEDJUuWRPXq1fXHdf/+77//mv0sAODm5qb/97CwMDx+/Bh169bFq1evcPny5QTvNefy5cvInTs3cufOjRIlSuDzzz/H22+/bbCcsGXLFmi1Wrzzzjt4/Pix/sfPzw/FihXDgQMHAEh+yZMnT9C/f384OcWmCHbv3h05c+Y0+f69evUy+FxnzpzB1atX0a1bNzx58kT/Xi9fvkTjxo1x+PBh/U6xHDly4NixY7h//77JZ3t6egIAdu3aZdGshs6vv/6KatWqoU6dOvpj7u7uGDBgAG7evImLFy8aXN+nTx+DRF9dInFif55PnjyBk5MT3N3dLR5bXP379zfKqYr7u4yKisKTJ09QtGhR5MiRw2D2wxx3d3eD/204OzujWrVqiX4WnYEDBxq8rlu3rsG9v/32G7JkyYL+/fvrjzk4OGDIkCEWPT9Hjhy4cOECrl69avL806dPsX//frzzzjv6/408fvwYT548QbNmzXD16lXcu3fPovcCgJw5c+Lx48cWX0/pAxOUKV1aunQp3njjDTg5OcHX1xfFixfXBxO3bt2Cg4MDihYtanCPn58fcuTIoQ9ETHFwcED37t2xfPlyvHr1ClmzZsW6devg6uqqX6+PK3/+/AavdV/Qz549048FkKAlLmdnZxQuXNhoLP7+/kaJjZ6enggICDA6Fvd9zLlw4QImTZqE/fv363NbdOLmoCRFwYIF8fXXX0Or1eL69euYOXMmHj16ZJCAevXqVSilUKxYMZPP0O3K0X3++H9WTk5OZuv+FCpUyOC17kssoWWNkJAQ5MyZE3PmzEGvXr0QEBCAypUro2XLlujZsycKFy6sf/bo0aMxb948rFu3DnXr1sXbb7+Nd999V/87N+XWrVsGwaiObonj1q1bKFOmjP54Yn9vUkv83x0AvH79GrNnz8bq1atx7949KKX05yz5O2Lq72zOnDlx9uzZRO91dXVF7ty5je6N+3u4desW8uTJY7TLMv7fGXM+/vhjtGnTBm+88QbKlCmD5s2bo0ePHihXrhwA4Nq1a1BKYfLkyZg8ebLJZzx8+BD58uWz6P2UUkxOzoAY7FC6VK1aNf1uLHOS+384PXv2xOeff46ffvoJXbt2xfr169G6dWuTX3bmdh7F/cJICnPPS877PH/+HPXr14eHhwc+/vhjFClSBK6urjh9+jQ+/PDDZNfFyZYtm0GCbO3atVGpUiVMnDgRixYtAiB5FhqNBjt37jQ59uTOTACGMxG69wKAzz//HBUqVDB5j+793nnnHdStWxdbt27F7t278fnnn+Ozzz7Dli1b0KJFCwDA3Llz0bt3b/z888/YvXs3hg8fjtmzZ+PPP//UJ/WmVHL/3nh7eyM6OhphYWHInj17kt83/u8OkLyz1atXY+TIkahZsyY8PT2h0WjQpUsXi/6OpOR/A2mxc69evXq4fv26/s9z5cqVmD9/PlasWIH33ntP/xnHjh2LZs2amXyGpYEVIP+7y5Url1XGTmmHwQ5lOAUKFIBWq8XVq1cNkgcfPHiA58+fo0CBAgneX6ZMGVSsWBHr1q2Dv78/bt++jcWLFyd7LIDUZdHNHgCSCHvjxo0k76pJioMHD+LJkyfYsmUL6tWrpz+u27FmLeXKlcO7776LL7/8EmPHjkX+/PlRpEgRKKVQqFAhvPHGG2bv1f1+rl27hoYNG+qPR0dH4+bNm/r/+k6Irn6Mh4eHRb/PPHnyYPDgwRg8eDAePnyISpUqYebMmfpgBwDKli2LsmXLYtKkSTh69Chq166NFStWYMaMGWY/x5UrV4yO65YKE/s7Z6kSJUoAkD9DS343lti0aRN69eqFuXPn6o+Fh4cnmJCdlgoUKIADBw7oZ1p1LNm9puPl5YU+ffqgT58+ePHiBerVq4dp06bhvffe0//vMkuWLIn+/UnsP6Du3buHyMhIg//foYyBOTuU4bRs2RIAsGDBAoPj8+bNAwC0atUq0Wf06NEDu3fvxoIFC+Dt7W3wRZgUTZo0gbOzMxYtWmTwX7qrVq1CSEiIRWNJLt1/Ncd938jISCxbtszq7/XBBx8gKipK/ztu3749HB0dMX36dKP/wldK4cmTJwCAKlWqwNvbG19//TWio6P116xbt87iJZ3KlSujSJEi+OKLL/DixQuj87pSADExMUbLMj4+PsibN6++JEFoaKjBOAAJfBwcHBIsW9CyZUscP34cgYGB+mMvX77EV199hYIFC6JUqVIWfZbE6OrpnDx50irPA+TvSfw/o8WLF1u1PEFKNGvWDFFRUfj666/1x7RaLZYuXWrR/bq/azru7u4oWrSo/s/Tx8cHDRo0wJdffomgoCCj++OWktDVJTIXCJ46dQoAUKtWLYvGRukHZ3Yowylfvjx69eqFr776Sr+Uc/z4caxduxZt27Y1mEEwp1u3bvjggw+wdetWDBo0KNmVX3Pnzo0JEyZg+vTpaN68Od5++21cuXIFy5YtQ9WqVRMs+pZStWrVQs6cOdGrVy8MHz4cGo0G//vf/5K9xJaQUqVKoWXLlli5ciUmT56MIkWKYMaMGZgwYQJu3ryJtm3bInv27Lhx4wa2bt2KAQMGYOzYsXB2dsa0adMwbNgwNGrUCO+88w5u3ryJNWvWoEiRIhYtRTo4OGDlypVo0aIFSpcujT59+iBfvny4d+8eDhw4AA8PD2zfvh1hYWHw9/dHx44dUb58ebi7u2Pv3r04ceKEflZj//79GDp0KDp16oQ33ngD0dHR+N///gdHR0d06NDB7BjGjx+P77//Hi1atMDw4cPh5eWFtWvX4saNG9i8ebPVih4WLlwYZcqUwd69e9G3b1+rPLN169b43//+B09PT5QqVQqBgYHYu3evvoyDrbVt2xbVqlXDmDFjcO3aNZQoUQLbtm3D06dPASQ+21KqVCk0aNAAlStXhpeXF06ePIlNmzYZlIhYunQp6tSpg7Jly6J///4oXLgwHjx4gMDAQNy9exd///03AKBChQpwdHTEZ599hpCQELi4uKBRo0bw8fEBAOzZswf58+dHxYoVU+m3QamFwQ5lSCtXrkThwoWxZs0abN26FX5+fpgwYQKmTp1q0f2+vr5o2rQpfv31V/To0SNFY5k2bRpy586NJUuWYNSoUfDy8sKAAQMwa9asVC2f7+3tjR07dmDMmDGYNGkScubMiXfffReNGzc2m5uQEuPGjcMvv/yCxYsXY9q0aRg/fjzeeOMNzJ8/H9OnTwcABAQEoGnTpnj77bf19w0dOhRKKcydOxdjx45F+fLlsW3bNgwfPtziqrsNGjRAYGAgPvnkEyxZsgQvXryAn58fqlevjvfffx8AkDVrVgwePBi7d+/W7xYrWrQoli1bhkGDBgGQQLlZs2bYvn077t27h6xZs6J8+fLYuXMnatSoYfb9fX19cfToUXz44YdYvHgxwsPDUa5cOWzfvt3qs3d9+/bFlClT8Pr1a5M5OEm1cOFCODo6Yt26dQgPD0ft2rWxd+/eVPk7khyOjo745ZdfMGLECKxduxYODg5o164dpk6ditq1ayf6d2T48OHYtm0bdu/ejYiICBQoUAAzZszAuHHj9NeUKlUKJ0+exPTp07FmzRo8efIEPj4+qFixIqZMmaK/zs/PDytWrMDs2bPRr18/xMTE4MCBA/Dx8YFWq8XmzZvRr18/JihnQBqVGv8ZSJQBtGvXDufOnUtSbgBZh1arRe7cudG+fXuD5QuSHVKFCxfGnDlz0K9fP1sPx2Z++ukntGvXDr///jtq165t6+Hgp59+Qrdu3XD9+nXkyZPH1sOhJGLODmVKQUFB+OWXX1I8q0OJCw8PN1pa+/bbb/H06VODdhEkPD098cEHH+Dzzz/PNJ3mX79+bfA6JiYGixcvhoeHR7KrSVvbZ599hqFDhzLQyaA4s0OZyo0bN/DHH39g5cqVOHHiBK5fvw4/Pz9bD8uuHTx4EKNGjUKnTp3g7e2N06dPY9WqVShZsiROnTqVrC7bZF/ee+89vH79GjVr1kRERAS2bNmCo0ePYtasWZgwYYKth0d2gDk7lKkcOnQIffr0Qf78+bF27VoGOmmgYMGCCAgIwKJFi/D06VN4eXmhZ8+e+PTTTxnoEABp9Dt37lzs2LED4eHhKFq0KBYvXmzUh44ouTizQ0RERHaNOTtERERk1xjsEBERkV1jzg5kG+z9+/eRPXt21k8gIiLKIJRSCAsLQ968eRMs7slgB8D9+/eNuk4TERFRxnDnzp0EG/ky2AH03YXv3LkDDw8PG4+GiIiILBEaGoqAgAD997g5DHYQ23vFw8ODwQ4REVEGk1gKChOUiYiIyK4x2CEiIiK7xmCHiIiI7BqDHSIiIrJrDHaIiIjIrjHYISIiIrvGYIeIiIjsGoMdIiIismsMdoiIiMiuMdghIiIiu8Zgh4iIiOwae2MRERFRqoiJAY4cAYKCgDx5gLp1AUfHtB+HTWd2li9fjnLlyukbcNasWRM7d+7Unw8PD8eQIUPg7e0Nd3d3dOjQAQ8ePDB4xu3bt9GqVStkzZoVPj4+GDduHKKjo9P6oxAREVEcW7YABQsCjRvGoHe3CDRsKK+3bEn7sdg02PH398enn36KU6dO4eTJk2jUqBHatGmDCxcuAABGjRqF7du3Y+PGjTh06BDu37+P9u3b6++PiYlBq1atEBkZiaNHj2Lt2rVYs2YNpkyZYquPRERElOlt2QJ07Ahkv3sRMXDCXfjDDa9w754cT+uAR6OUUmn7lgnz8vLC559/jo4dOyJ37txYv349OnbsCAC4fPkySpYsicDAQNSoUQM7d+5E69atcf/+ffj6+gIAVqxYgQ8//BCPHj2Cs7OzRe8ZGhoKT09PhISEwMPDI9U+GxERkb2LiZEZHHX3Lu4iQH/cCVGIgRM0GsDfH7hxI+VLWpZ+f6ebBOWYmBhs2LABL1++RM2aNXHq1ClERUWhSZMm+mtKlCiB/PnzIzAwEAAQGBiIsmXL6gMdAGjWrBlCQ0P1s0OmREREIDQ01OCHiIiIUiYmBli8GHhx95lBoNMa2xHzX5qwUsCdO5LLk1ZsHuycO3cO7u7ucHFxwcCBA7F161aUKlUKwcHBcHZ2Ro4cOQyu9/X1RXBwMAAgODjYINDRndedM2f27Nnw9PTU/wQEBJi9loiIiBKny9GZMOo1nsFLf7wfVuIXtDa6Pigo7cZm82CnePHiOHPmDI4dO4ZBgwahV69euHjxYqq+54QJExASEqL/uXPnTqq+HxERkT3T5egE3Y3Ga2TVH5+ET/AN+pm8J0+etBpdOth67uzsjKJFiwIAKleujBMnTmDhwoXo3LkzIiMj8fz5c4PZnQcPHsDPzw8A4Ofnh+PHjxs8T7dbS3eNKS4uLnBxcbHyJyEiIsp8YmKAESMApRSikUV/fAXex0xMMrpel7NTt27ajdHmMzvxabVaREREoHLlysiSJQv27dunP3flyhXcvn0bNWvWBADUrFkT586dw8OHD/XX7NmzBx4eHihVqlSaj52IiCizOXIEuHsXiIoT6LyGKwZhhdG1Go38c8GCtK23Y9OZnQkTJqBFixbInz8/wsLCsH79ehw8eBC7du2Cp6cn+vXrh9GjR8PLywseHh4YNmwYatasiRo1agAAmjZtilKlSqFHjx6YM2cOgoODMWnSJAwZMoQzN0RERKlIVzBw82bgEkrACTH6c4OxzOQ9/v4S6MSpIpMmbBrsPHz4ED179kRQUBA8PT1Rrlw57Nq1C2+++SYAYP78+XBwcECHDh0QERGBZs2aYdmy2F+go6MjduzYgUGDBqFmzZrIli0bevXqhY8//thWH4mIiMjubdkiS1d37wK/ogVK4Ir+3LfogbXoZXTP/PnAsGG2qaCc7urs2ALr7BAREVlGl4ysFLAcAzEQX+rP7UArtMNWg9wda9bViS/D1dkhIiKi9C02GRn4CDMMAp2jqIlO2GgU6ABpn6MTH4MdIiIisoguGbkvVmEGJuuPX8EbaIlfEQ43g+v9/YFNm9I+Ryc+m289JyIioowhKEiqIa/CewbH6+MQQpBD/3roUKBDB9t1OY+PwQ4RERElSLfzKmzXUWzH2wbnwuCOBzCsbdehA9CgQRoOMBEMdoiIiMgs3c4r97uXcAm1jc63wE79v9uiYKAlGOwQERGRSbqdV3nVXVyCcbHefliJP1AHQPpJRjaFCcpERERkRLfzKod6atDBXGcBRhj0vUovycimcGaHiIiIjBw5Ajy5+wqv4G10bjfexFh8AQCYNAlo3Dj9JCObwmCHiIiIjATficIrZDM6/gA+6IwfEPNfCFGqVPpKRjaFy1hERERkKCYGXXo6mzw1Hp/iOXLqX+fJk1aDSj7O7BAREVEspQAn0+HBBZTCGvQGkH53XpnCmR0iIiKK5W2cowMAr+CGMjgPQJOud16ZwmCHiIiIRLVqwLNnJk9lw0sAEuWk551XpnAZi4iIiIB33gFOnDB5KuZlOA4c1yAoSHJ00vPOK1MY7BAREWV2Y8cCGzeaPhccDMesLul+x1VCuIxFRESUmc2bB8yda/pcuXKAr2/ajicVMNghIiLKrL7+Ghgzxvz5X39Nu7GkIi5jERERZUZLlgDDhpk///w54OmZZsNJTZzZISIiymzGj0840AkPt5tAB+DMDhERUeYRFQV06ABs3272kpjnYYCTC44cRIbdfRUfgx0iIqLMIChIopbr181ekh+38KqIOwDgyZPY4/7+wMKFGaeuTnxcxiIiIrJ3hw8DBQsmGOgAwDPkxJMnhoEOANy7B3TsCGzZknpDTE0MdoiIiOyVUsAXXwD16wORkQle2gff4AWym30MAIwcCcTEWHmMaYDBDhERkT0KDZXpmHHjEr10FOZhDfokeI1SwJ07wJEj1hpg2mHODhERkb05f14SbK5eTfTSmZiIBRhl8aODglIyMNvgzA4REZE9WbcOqF7dokDnW/TAJMxI0uPz5EnuwGyHwQ4REZE9iIwEhg4F3n0XePXKolt6Yw10ncwTo9EAAQGyoSuj4TIWERFRRnfnDtCpE3DsmMW3DMRyKAvnPDT/xUMLFmTMejuc2SEiIsrI9u4FKlVKUqCzGEPxFQaYPOftLT9x+fsDmzZl3Do7nNkhIiLKiLRaYPZsYPLk2L3hFpiAWfgUE0yemzQJmDZN/v3IEVZQJiIiIlt59gzo1SvBtg+mvI8V+Arvmz3fuHFsUNOgQQrGl84w2CEiIspIzpyR/lb//puk2z7M8SW+DhkAmJgE0mhkqSojJh9bgjk7REREGcWaNUDNmkkOdAAg26gBUCo22VgnoycfW4LBDhERUXoXHg4MGAD06SP/nkS70BRTp0risZeX4bmMnnxsCS5jERERpWc3b0rbh1OnknX7GZRHN6wHADx9KsemTweKFbOP5GNLMNghIiJKr3buBLp3l4TkZDiLsmiCvXgK2UuuW8ZauRK4ccP+gxwdLmMRERGlNzExwNSpQKtWhoGOkxOQN69Fj7iOwmiMfXiCXAbHM3JDz+TizA4REVF68uSJzObs2mV4PH9+oGxZ4JdfEn1ECDxQE4F4jNxmr8mIDT2Ti8EOERFRenHihOTn3L5teLxVK8DPD1i1yqLHdMEGPIJPgtdkxIaeycVlLCIiIltTCvjyS6BOHcNAx9ER+OwzIHduiwMdAHiQp6LRFnOdjNzQM7kY7BAREdnSq1dA797AwIHSuVwnb17gwAFg3z6pr2OpGTMwaYkfgMxZU8cUBjtERES2cu2aFAn89lvD42++KUta7dsDu3db/rz69YGPPkL79lI7J18+w9OZoaaOKRqlktA9zE6FhobC09MTISEh8PDwsPVwiIgoM/j5Z+lvFRISe0yjkSI4w4YBOXMm/ZlarcF0TkyMfTX0jM/S728mKBMREaWl6GjpVP7pp4bHfX2B9euBQoWSF+iEhxutWzk62ldDz+TiMhYREVFaefgQaNbMONCpXx/46y+Z5SlcOOnPvXsXcHGxzhjtEIMdIiKitHD0KFCpErB/v+Hxjz4C9u4FJk5MVjJNzP5Dxsk5ZMCmwc7s2bNRtWpVZM+eHT4+Pmjbti2uXLlicE2DBg2g0WgMfgYOHGhwze3bt9GqVStkzZoVPj4+GDduHKKjo9PyoxAREZmmFLB4scze3LsXe9zbW9pBfPCBzMokZcfVf0ZiPo5o6llvrHbKpsHOoUOHMGTIEPz555/Ys2cPoqKi0LRpU7x8+dLguv79+yMoKEj/M2fOHP25mJgYtGrVCpGRkTh69CjWrl2LNWvWYMqUKWn9cYiIiAy9eCHVkIcPl1wdnVq1ZNkqVy7A01MSi5PoJ7TBQozMVJWQk8umCcq//fabwes1a9bAx8cHp06dQr16sZFq1qxZ4efnZ/IZu3fvxsWLF7F37174+vqiQoUK+OSTT/Dhhx9i2rRpcHZ2TtXPQEREZNLly0CHDsDFi4bHx40DZswAFi6UWZ1kCEV2dMBmAJmrEnJypaucnZD/tt95eXkZHF+3bh1y5cqFMmXKYMKECXj16pX+XGBgIMqWLQtfX1/9sWbNmiE0NBQXLlww+T4REREIDQ01+CEiIrKaTZuAqlUNA50cOWS7+dixkqSczEAHAPLjNpTGMdNVQk6udLP1XKvVYuTIkahduzbKlCmjP96tWzcUKFAAefPmxdmzZ/Hhhx/iypUr2LJlCwAgODjYINABoH8dHBxs8r1mz56N6dOnp9InISKiTCsqChg/Hpg3z/B41arAjz8C169LMnEK8kor4jRCNTkAZL5KyMmVboKdIUOG4Pz58/j9998Njg8YMED/72XLlkWePHnQuHFjXL9+HUWKFEnWe02YMAGjR4/Wvw4NDUVAQEDyBk5ERARI5b7OnaWKX1zDhwOzZsX+pMASDMEZVESAvwQ6ma0ScnKli2Bn6NCh2LFjBw4fPgx/f/8Er61evToA4Nq1ayhSpAj8/Pxw/Phxg2sePHgAAGbzfFxcXODCegRERGQthw8D77wD/Pf9AwDw8JDmnVWqSPuHwMAUvUVoyerwnrwEB+ywEnJqs2nOjlIKQ4cOxdatW7F//34UKlQo0XvOnDkDAMjzX0ZWzZo1ce7cOTx8+FB/zZ49e+Dh4YFSpUqlyriJiIgAyLbyL76QbeVxA50KFYBTp6SicYUKKQ50AMDjQiC6dpWKyAx0ksamMztDhgzB+vXr8fPPPyN79uz6HBtPT0+4ubnh+vXrWL9+PVq2bAlvb2+cPXsWo0aNQr169VCuXDkAQNOmTVGqVCn06NEDc+bMQXBwMCZNmoQhQ4Zw9oaIiFJPaCjQty+webPh8fffB2bPBiZMAL780jrv9eqVcQtzsphNG4FqzPzBrV69Gr1798adO3fw7rvv4vz583j58iUCAgLQrl07TJo0yaDh161btzBo0CAcPHgQ2bJlQ69evfDpp5/CycmyWI6NQImIKEkuXJCEmX/+iT2WLRvw1VdA+fJAly7A+fNWeauqfncwYak/83NMsPT7m13PwWCHiIiSYP16oF8/abypU7q0bDc/fBgYORJ4/doqbzUS87FIMxKAPJ4BjyFLv7/TVZ0dIiKidCsyEhg2TCoixw10+vQBdu8GpkyRJSwrBTpb0A4LMRK6KYmRI4GYGKs8OtNJF7uxiIiI0rW7d4FOnYA//4w95uwMrFgBlCwp7R9u3bLa291AQXTAFv1rpYA7d2RXe4MGVnubTIMzO0RERAnZtw+oWNEw0CleHDh+XHZg1alj1UAHAIrhqsnj7IOVPAx2iIiITNFqpQhgkybA48exx7t1A7Zvl7YPEybI2lLOnFZ7W288RoyZhRf2wUoeLmMRERHF9/w50LOnBDVxrVgBFCgA1K4NPHoEuLkBlSoZzvqkQBmcw1N4Gx3XaAB/f/bBSi7O7BAREcV15gxQubJhoFOkiAQ0164BLVpIoFO2LPDWW8Aff1glc7gW/sAFlDE6rqvSwj5YyceZHSIiIp01a2R3VVwdOgATJwIDBwInTsixAQOAsDDg+++t8rb5cBf3kQ+ABDRxYyd/9sFKMQY7RERE4eHSsPPrrw2PL1oE5MolW6DCwiQ3Z+FCYN06YNcuq7y1G15jwnRXFCsmOTm1agFHj0oych72wbIKBjtERJS53bwJdOwovax0AgKAtWuB774DvvlGjtWpA8ydK7V0/uvTmFL5/bVYt1BjNGvD7eXWxWCHiIgyr99+A9q0kYKBOm+9BYweDQwaBFy+LEkzkyZJnZ3GjSVfxwpOrPwbN3prOGuTBhjsEBFR5qPVAh9/DEyfbnh8zhzA1RVo3hyIiADy5pXZHQcH4L8G1NZStUUugIFOmuBuLCIiylyePAFatTIMdHLlArZtk51Vw4dLoNO6NfD331I40NrrSpMmSSBFaYLBDhERZR4nTkh7h99+iz3WrJl0Kx88GPj5Z2kDsWCBBD+rVgFdu1p3DM2aAZ98Yt1nUoK4jEVERPZPKdlp9f77hsenT5clrY4d5Z/FigEbNgDly0vDTyttLdcPI1s2aOIGWpQmOLNDRET27dUrqZ0TN9BxdZVcnP37YwOeHj1kR1aJEkDBglYPdACgQI5QbNmS+HVkXZzZISIi+3X9uuTnXLkSe6xBA5m1GTFC8neyZQOWL5dg59EjwMcnVYaSFS8Rft8BHTsCmzaxSGBa4swOERHZp23bgKJFDQOdDz4ASpcG+veXQKdSJeCvvyTQuXAh1QKdKjiB18gKpeT1yJFW6TBBFmKwQ0RE9iU6Wto7tGljeHzxYmD3bmDpUnk9cqSUKi5WDPjxR6CMcV8qa2iOnTiFKvrXSgF37gBHjqTK25EJXMYiIiL78fChJBvHjSSqVQPatgXGjwdevpRt5mvWyPIWAAwdGhsAWVl/fIVdaG7yXFBQqrwlmcBgh4iI7ENgoDSWimvQIODZM5npASRf57vvgHz5JCk5IAC4fz9VhjMXo7ES/c2ez5MnVd6WTOAyFhERZWxKyRJV/EBn2jRgzx7ZSu7gILVt9u6VQOfZM+mumUqBzkmn6hiLuSbPaTQSY9WtmypvTSZwZoeIiDKuly9lW/nGjbHHSpYEmjYFZs4EoqIksli/Xhp5AlJYsFq12OtdXaXruRVViz5q8rhGI/9csICdzNMSZ3aIiChjunJFZmniBjrdusmxhQsl0GnXTjqU6wKdpUsNA5169awe6HjhCZSZr1d/f247twXO7BARUcazebMkIsc1dKgcDwoCXFyA+fOBgQNlOiUyUrqWb9sWe323bjLjY0WF8C+ewUv/WqORfOj58yUGq1uXMzq2wGCHiIgyjqgoYNw4mbnR8fEBGjeWWRulZBlrw4bYLuX37gFvvCGVlHW6dwfWrbPq0KrhGG6ikMExpaROYb581u8lSpZjsENERBlDUBBQvz5w9WrssWbNgMePY1s7vPeeJMRkyyavDx0yjjIaNbJ6oNMRG3EC1cye5zZz22KwQ0RE6d/hwxLoxNWpk+y2ev4c8PCQzuWdO8s5pYB584CxY2OvDwiQf+7fb9WhTcRMbEbHBK/hNnPbYoIyERGlX0oBn39uHOg0aiSJyc+fS8LxX3/FBjphYcA77xgGOi1bStniO3esOjxtp3fwP/+J+l1W8XGbefrAYIeIiNKn0FBZgvrgg9hj5coBRYrEzs588AHw++9A4cLy+vJl6Xe1aVPsPQMGAL/+av3xFS0Khx826NOH4gc83GaefjDYISKi9OfCBcDTU5avdOrUkXyd69clKXnXLuCzz4AsWeT85s2SnHztWuw9Y8fK8lZqOHcO0GjQvr3EVvnyGZ7mNvP0gzk7RESUvqxbB7z7ruGxMmVkBgcA3nwT+PZbwM9PXkdHAxMmAF98YXjP+PHAp5+mzhgfPZJihP9p3176jh45IsnIefJwm3l6wmCHiIjSh8hIqYYct/ZNjhwyc3P+PODkJFWRx46V9g8A8OAB0KULcPBg7D1eXkDPnqkX6Fy5IsVz4nF05Pby9IrBDhER2d7du0CBAtKcU6dQIUkojo4GChaU2jnVq8eeDwyUHVn37sUeK1ZMrlmwIFWGGfP3eTi+8UaqPJtSD3N2iIjItvbtky1LcQMdHx/gxg0JdDp3lpYPukBHKSkgWL++YaBTo4asH333XaoM0x1hOPK0dKo8m1IXgx0iIrINrRb48EOgSRPD466uwMOHgJsbsHKlFAz09JRzr17JEtXQoVJNWadFC5kdipvQbEUaaPES7iwOmEFxGYuIiNLe8+eSdBx3ZkajkVmb8HCgbFlZtipVKvb8tWtAhw7A2bOGz0qF1g9xZUEkANlHzuKAGRNndoiIKG2dOQPkzGkY6Dg4SKADAIMHA8eOGQY627cDVaoYBzqDB6dqoNMOWxCNLCwOmMEx2CEiorSzZAlQsaLxca1Wdl5t3iz5OG5ucjwmBpg8GXj7bSAkJPZ6Fxdg9Ghg2bJUG+pVFMU2vM3igHaAy1hERJT6wsOlxUNgoOnztWvLDE2BArHHnjwBunUDdu82vDZnTtmiPm9eqg33FCqhCk4C0CDAXwIdFgfMuBjsEBFR6rp1S7aOm6LRAB99BEydKnV0dE6dkvycW7cMry9QQDqdp2Kgo8qWRdiiU1jP4oB2g8EOERGlni1bJGgxJW9e2SbesKHh8VWrgCFDgIgIw+MVKkjQlFrtHwDAywuav/9GAzONPSljYs4OERFZn1YrlY3NBTqtWkmictxAJzwc6N8feO8940CnQQPJ0/npp1Qa8H+Cg407elKGx5kdIiKyridPTLZTACCtH+bMAUaMMAwqbt2SwOjUKeN72rYFTpww3L2VGh49im0qSnaFMztERGQ9v/9uPtApWlQSlEeONAx0du8GKlWSQMfd3fD+Xr1kNie1A51z58yPmzI8BjtERJRySgEffGC+EE2PHsDp00DlyrHHtFpp7Nm8OfD0qQRDuXMDjx9LsvLAgcDatak/9p9/lgKHZLdsGuzMnj0bVatWRfbs2eHj44O2bdviypUrBteEh4djyJAh8Pb2hru7Ozp06IAHDx4YXHP79m20atUKWbNmhY+PD8aNG4fo6Oi0/ChERJnX69dA9uzA558bn8uWTQKWb7+Va3SeP5flqUmTJFBq1kz+eeOGXPfee8CKFcbPs3Y+zccfSw0fsms2DXYOHTqEIUOG4M8//8SePXsQFRWFpk2b4uXLl/prRo0ahe3bt2Pjxo04dOgQ7t+/j/Zxih3ExMSgVatWiIyMxNGjR7F27VqsWbMGU6ZMscVHIiLKXC5dArJmBeL8/7ZexYoym9Ozp+Hxs2elGvL27ZJ0PGyYHLt+XXZotW9vOtABYqssW0ObNhJskf1T6cjDhw8VAHXo0CGllFLPnz9XWbJkURs3btRfc+nSJQVABQYGKqWU+vXXX5WDg4MKDg7WX7N8+XLl4eGhIiIiLHrfkJAQBUCFhIRY8dMQEdm5hQuVkvDD+GfkSKXCw43v+e47pdzc5JoCBZSaPVspd3d5XbKkUu3amX+mNX9Kl1bq1as0/5WRdVn6/Z2ucnZC/isF7uXlBQA4deoUoqKi0CROR9wSJUogf/78CPyvCmdgYCDKli0LX19f/TXNmjVDaGgoLly4YPJ9IiIiEBoaavBDREQWiomRHJcRI4zPeXvLjM38+TJroxMZKTM4774ry17NmgHDh0sriBcvgOrVAR8fYOvWVB/+k+K1JBla15KC7F66CXa0Wi1GjhyJ2rVro8x/iWLBwcFwdnZGjhw5DK719fVFcHCw/pq4gY7uvO6cKbNnz4anp6f+JyAgwMqfhojITt2/L8nDpv5jsn594O+/gdatDY/fuyd1cpYskdeTJgE1agBjxgDR0cCbb0rAc+hQqg9/O1qj4svfEePkkvjFZDfSTbAzZMgQnD9/Hhs2bEj195owYQJCQkL0P3fu3En19yQiyvA2bgTy5TM+7uAgib779hmfP3RItpUHBgKenjJzExQETJ8u59u3l+KCZmbirWkDOuNtbMeduxocOZLqb0fpSLooKjh06FDs2LEDhw8fhr+/v/64n58fIiMj8fz5c4PZnQcPHsDPz09/zfHjxw2ep9utpbsmPhcXF7i4MKonIrKIUjJb8+uvxucCAoD164E6dYzvmTcP+PBDWfYqV052ZU2YAPz2mwRI3bpJuwgr00IDBxgnMndF7H9MBwVZ/W0pHbPpzI5SCkOHDsXWrVuxf/9+FCpUyOB85cqVkSVLFuzbt09/7MqVK7h9+zZq1qwJAKhZsybOnTuHhw8f6q/Zs2cPPDw8UKpUqbT5IERE9iokRAITU4FO27YyKxM/0AkLA955Bxg7VgKdd9+VHll9+0qgkzWrHEuFQOd5+fomA51KMKzMnCeP1d+a0rO0yZc2bdCgQcrT01MdPHhQBQUF6X9excmQHzhwoMqfP7/av3+/OnnypKpZs6aqWbOm/nx0dLQqU6aMatq0qTpz5oz67bffVO7cudWECRMsHgd3YxERmXDggOmdTC4uSi1dqpRWa3zPxYtKlSgh12XJItddvCg7rwClcudWqnNn089M6Q6rSZNMHs+LuwaHcueWTWEHDigVHZ3Gv1OyKku/v20a7AAw+bN69Wr9Na9fv1aDBw9WOXPmVFmzZlXt2rVTQUFBBs+5efOmatGihXJzc1O5cuVSY8aMUVFRURaPg8EOEVE8gwaZDihKlFDqzBnT92zcGLuNPG9epY4eVerwYaVy5JBjhQsr1bKl4fNy5lTK0zPlgc4335g8rkFMgrf5+yu1eXOa/mbJiiz9/tYoZc0KTRlTaGgoPD09ERISAg8PD1sPh4jIdiIiAFdX0+f69QMWLpSqyHFFR0suzhdfyOsGDYANGyQ5uUcP2XZerpz0vTp6NPa+ggVlF9bjxykb86RJwIwZRocL54vAjXvOCd6qK8i8aZPkSlPGYun3d7rZjUVERDZ29qzpQCd7duD774GVK40DnQcPZOu4LtAZNw7Ys0fycTp3lkCndm3pgxU30ClfXnJ7UhroFCtmMtBBaCiu3nLGgQMylNy5Td+u+8/9kSMlvYjsE4MdIiKShpzlyxsfr1ZNkpC7dDE+Fxgo28oPHpRZm02bgNmzgdGjJTkZkOKBd+4A58/H3teggdTjefIk5eO+etX4WFAQkD07HB3lrfLlAx49Mv8IpWSI3I5uv9LF1nMiIrIRrRbw9TU9w/LBB8AnnwDO8ZaClAKWLQNGjQKiooCSJWW3VYECQKdOsVWQ27YF9u8H4lap79hRgqLUcvEiEK/siKXbzLkd3X4x2CEiyqxu3gTilfwAIG0bvv1WZmXie/UKeP/92G3jnToBq1ZJrk+jRsCff0pw9NZbwObNhvcOGAB89ZXVP4bevn0SeMVj6TZzbke3X1zGIiLKjFauNB3oNGkiS0ymAp1r14CaNSXQcXQE5s4FfvgBePgQqFVLAp2cOYGmTY0DnQ8+SN1AZ+VKCbZMqFsX8PePTUaOT6OR2oh166be8Mi2GOwQEWUmSgFVqwL9+xsed3ICPv0U2LXLaBkIgDT3rFJFkph9fGQWZfRo4MQJCYCuXpWIokIFYMeO2Pvc3aWK8pw5qfaRLrUaKzvFzHB0lE1kgHHAo3u9YIFcR/aJwQ4RUWbx8KFUQz550vB4wYKSnfvhh3I+rpgY6Uz+9ttSTblWLeD0aWn6uW2bZAA/egQULy5B0oEDsff6+QEdOgCffZZqH2kvGuPB2M8Tva59e0kVit+6y9+f284zA9bZAevsEFEm8PPPkjAc3zvvAF9+CcTpP6j35In0r9q9W14PGyZbzJ2dgeXLgaFDJcG5YkXJ5blyJfbeYsWkLcTff8cea93acNYnhULggbL+z3HjpsbiWZmYGInrgoIkR6duXc7oZGSWfn8zQZmIyJ4pJTugtmwxPO7mBixaJMs/ppJZTp2SWZlbt+Tar78GuneX4GbCBFnyAqSGzs2bwL17sfdWqWI8ezRnjuTtWJE3nuLHhZYHOgD029Epc2GwQ0Rkr0JCTM/YlCkjicXmmiWvWgUMGSI7rIoWlWTjcuXkdd++0uUckGTmkyeB589j761XDzh82PB5W7cC7dpZ4xPpFfN/jR8XOnL5iSzCYIeIyB4dOmR6CmPgQGDePJmtiS88XJaqVq6U12+9JVvQc+SQgKZdOykg6OQENG4s+TmRkbH316plHOjs3StBkRX9sfUhLr/lyuUnshgTlImI7IlSkksTP9DJkUMycZcvNx3o3LoF1KkjgY5GIxWVf/pJ7rt9W84dPCitI+rXl11bcQOdokUN20EAkp9j5UAHK1agdtvcDHQoSTizQ0RkL168kGAkvlq1ZOmpQAHT9+3eDXTtCjx9Cnh7y7VNm8q5M2eAVq2A+/dly3nhwrLtPL5r1wxff/utJCRb0+TJUtCQKIk4s0NEZA9OnzYd6Hz0kSxpmQp0tFqZwWneXAKdKlUkMVkX6OzZIzk49+/L/b6+UjgwIfXqyTN79kz5Z4qrXz/g44+t+0zKNBjsEBFldJ98AlSubHjM11dmYGbMkByb+J4/l63okybJ0lf//rInWxcUrV0LtGwpnclLlJBtTOfOJTyO998H8ueXAMuaGjSIzSMiSgYuYxERZVSvXgFvvGG47RuQIGXNGiB3btP3nT0rVfSuXwdcXIClS2MrECslAdKUKfK6QgUgOFh+EvLRR7K9/dKllHwiY7lzGxYqJEoGzuwQEWVEly8D2bIZBzrz5klrB3OBznffATVqSKBToADwxx+xgU5UlDTr1AU6NWoA//6beKDz8ceydGXtQOebb6TqM1EKcWaHiCijWbZM6uDE5e8vu6fiL2fpREYCY8YAS5bI62bNgHXrJCEZkOTmTp2A336TlhG1agHHjkkAlJCpU2ODI2upXFnGkSuXdZ9LmRaDHSKijCI8XLZ9Hz9uePzddyUAMpWgDMjsT6dOQGCgvJ48WYIU3f7t4GDZcXX6tGxLL1cO+P33xMczYgQwfXryP48p9erJzBRb95AVMdghIsoIrl+XWjbxrV2b8M6ngweBzp1lOcjTU5ax4m4Jv3QJaNFC6uzkzCkJxseOJT6eDh1iW4lbS4sWUgsoa1brPpcyPebsEBGld+vXGwc6hQpJ3o65QEcpYO5cKer38KHM1pw8aRjoHDkiva1u3ZIO5blzGzbuNKdIEWkhYU2dOskyHAMdSgUMdoiI0qvISODtt6UBZ1zDhsmMTPHipu8LC5Nu5mPHSpvvd9+VJay4AdPGjRIIPXsmicpZsgD//JPweJyd5Z/Xryf/M5mwCv2wpdP3sc8nsjIuYxERpUe3bgEFCxof37ZNelaZc+mSbCu/fFkCmAULgEGDYjubKwXMny/JygBQrJgEPI8fJzweR0fD9hBWMh8jMQbz4D9GgzbtwTYQlCo4s0NElN5s22Yc6BQpAty9m3Cgs2kTUK2aBDp580rl5MGDYwOdmBhg5MjYQKd0aSAoKPFAR3evlU3DVIzGPChocOeOrKoRpQYGO0RE6UVUlNS8adPG8PhHHwFXrgD58pm+LzoaGDdO8l5evJCKw6dPAzVrxl7z+rWcX7RIXleoIMtWL16kxiexyHRMA6DRvw4KstlQyM5xGYuIKD24d09mc6KjDY8fPgzUrWv+vgcPgC5dZNcVIEHPrFmGLSIeP5bcn8BAyYt54w1p8GlDyzDI6FiePPLPmBiZ5QkKkmN163J5i1KGwQ4Rka3t3i1F/uIqWlSabuqK/pkSGAh07CiNOt3dpUVEhw6G11y/Llu6r16Va/LmBc6ft/pHSIpuWIfv0VX/WqORmoh160rHiREjZMVOx99fdrm3b2+DwZJd4DIWEZGtxMRI/kz8QGfWLFliMhfoKCX9rOrXl0CnZEngxAnjQOf4cVnKunoV8PKSn8R2XKWiR8iF3HiE79ENuuUrXTrRggXAzz9L7BY30AFk0qtjRwmEiJKDMztERLYQHAxUrGjcd+qvvySfxpxXr6S7+HffyetOnYBVq4yrJ2/fLsUEX7+WDugAcPu2+ec6ORkvoVnTBx/gSPXP4DoCQLxZmwULJE2pYEGJ4+JTSoKikSPlOi5pUVIx2CEiSmsHDgCNGhkeK1pU8miyZTN/37VrMntz9qx848+ZA4waFTs9orNihfTO0molqfnlS+D5c9PP9PaWGSZz562lTRu0ryXBiql8nIMHjWd04lIK+h1bDRqk7lDJ/nAZi4gorWi10ksqfqAzb54sNSUU6GzfDlSpIoGOjw+wbx8werRhoKPVAhMmSF0drVamSp49Mx/ItGyZ8HlradYMqFoVgAQ2DRoAXbvKP3WzNJbuxOKOLUoOzuwQEaWFx49lGuPyZcPjly+br4QMyKzLtGnAjBnyulYt4McfjbehR0YCfftKJ3MAKFxYlq3MLU198IHMDKW2Tp1kyS1LlgQv0+3ESoyl1xHFxWCHiCi1/fEHUKeO4bGCBaV2TkItEp48Abp1k91agLSJ+OIL43ueP5etSgcOyFRJ/vzAv/+af+7ixfKs1NavH/DllxYl2dStK/k79+6ZztuJu2OLKKm4jEVElFqUAj7/3DjQWbAAuHEj4UDn1CmgcmUJdNzcZHZk0SLje+7ckecfOAC4ukpDzxs3TD/Ty0tmdNIi0BkzBvj6a4uziR0dY5uox09Birtji8nJlBwMdoiIUsPTp5Jj88EHhsevXpVCMglZtSq2G7mu3k78ZqCAdCivUQO4cEHyfbJnl6kRU958Uyo0p8XS1YwZEuTFj1oS0b69dLyIv0Ln7y/HWWeHkovLWERE1nb8OFC9uuGxfPkkeEloaiI8XGZdVq6U12+9BXz7LZAjh/G1e/bIzqywMMDTUwKLR49MP7d1a2DHjmR9lCRbvBgYOjTZt7dvb37HFlFycWaHiMhalJK1lviBzrx5sq86oW/sW7dkOWrlSglcZs4EfvrJdKDz7beykyosTJamoqPN76jy8kpaoGOu/5Ylvv02RYGOjrkdW0TJxZkdIiJrCAkBmjeXJae4/vkHKFYs4Xt375Zv9qdPpe7N+vVA06bG1yklQdDkyfI6Vy4gNFR2Ypnz9Knln6F5c+C33yy/XjcsB0doN22GY7s2iV9MZAOc2SEiSqm//pIZmLiBjre35MgkFOhotRK8NG8em+Nz6pTpQCc6GhgwIDbQ8fWVexIKdJJi1KhkBToA0Eq7DQWHt2E7B0q3GOwQESWXUsCSJUClSobHZ82SujpOCUyeP38OtG0LTJokz+nfXxJVChQwvvbFC+larlviyp1bup1rtSn/DO7u8hnmz0/W7WdRFjvRkv2rKF3jMhYRUXKEhckMTPxlq7NngbJlE7737FnJxL1+HXBxkaae/fqZvjY4GGjVCjh9WoKn7NnNJyInlb+/BDpt2ybr9jXohT5YDYD9qyh948wOEVFSnTsHeHgYBjrZskkPqsQCne++k+3i16/LLM4ff5gPdC5flq7lp09LfR03N2nvkBy5cxu+rlgR2Lgx2YHOYCxFH6yBrns5YNi/iig9YbBDRJQUy5YB5coZHps4UWZ6smY1f19kpGwr79FDOpE3axZbONCUI0ekNcTNm1Is0NFR3iM5WrUynA1q2RL4/nsJpJKhPg5iOQabPc/+VZTeJDnY6dWrFw4fPpwaYyEiSr9evZIE4iFDDI8fPSpJxgkV0Lt3T/ZQL1kirydPBn75RZKYTdm4UYoAPnsmgY5WKwFSckyfLu/1H+37A3G65SSgRIlkPW4iZuIw6id4DftXUXqT5JydkJAQNGnSBAUKFECfPn3Qq1cv5EtJXQYiovTu8mWgZEnDY46OkiRsLmDROXgQ6NwZePhQiv99950U+TNn/nzpZg7IslVkpDQDTY4NG4AuXfQvL3aehv3rIzA0rFayHrcYQzEbE8yeZ/8qSq+SPLPz008/4d69exg0aBB++OEHFCxYEC1atMCmTZsQFRWVGmMkIrKdpUuNA53Bg4GIiIQDHaWAuXOBJk0k0ClXTpatzAU6MTGS3Rs30AkPT16gkzu3VFiOE+j823QgXH9Yi6Fhs5P+PAD7Gs9CrvWLMX26zGCxfxVlKCqFTp06pYYOHapcXV1Vrly51MiRI9U///xj0b2HDh1SrVu3Vnny5FEA1NatWw3O9+rVSwEw+GnWrJnBNU+ePFHdunVT2bNnV56enqpv374qLCwsSZ8hJCREAVAhISFJuo+I7Njr10qVKaOUhC2xP7/8kvi9oaFKdewYe8+77yr18qX561+9Uqp9+9jrXV2N39fSn+7dldq1y+CYtkCB5D8PUDFfrTQY7ubNSvn7G14WECDHidKSpd/fKdp6HhQUhD179mDPnj1wdHREy5Ytce7cOZQqVQpz5szBqFGjErz/5cuXKF++PPr27Yv2Zjq8NW/eHKtXr9a/dnFxMTjfvXt3/TiioqLQp08fDBgwAOvXr0/JRyOizOyff4DixY2P37gBFCyY8L2XLsm28suXgSxZZKpj0CDzOT2PH8te7aNH5bWzs8zoJMc338hW9mbNDA5rbt1K3vMAYOtWOMTbscX+VZThJDWKioyMVJs2bVKtWrVSWbJkUZUrV1bLly83iKq2bNmicuTIkaTnwszMTps2bczec/HiRQVAnThxQn9s586dSqPRqHv37ln83pzZISK9RYuMZze6dJGZnsRs3KiUu7vckzevUkePJnz99etKFSsW+z6OjsmfgTlzRqk5c1I0g2P0U7KkdX6nRKkk1WZ28uTJA61Wi65du+L48eOoUKGC0TUNGzZEDlPN65Lh4MGD8PHxQc6cOdGoUSPMmDED3v+tkwcGBiJHjhyoUqWK/vomTZrAwcEBx44dQ7t27Uw+MyIiAhEREfrXoaGhVhkrEWVgkZFA6dLAtWuGx1evBnr3Tvje6GhgwgTgiy/kdYMGkhzs62v+nhMnJH/n4UN5rdEkPxH56VPZ4bV0afLuN2GHWye0OPcjOFlD9iDJwc78+fPRqVMnuLq6mr0mR44cuHHjRooGBsgSVvv27VGoUCFcv34dEydORIsWLRAYGAhHR0cEBwfDx8fH4B4nJyd4eXkhODjY7HNnz56N6dOnp3h8RGQnzC1b/fUXYOI/6Aw8eCCJwAcPyutx46RdREKtInbskB1ar17FHlMqqaOWQoYPH0oT0W3bkn6/GQOxHF++HogDRyRuI8rokhzs9OjRIzXGYVKXODsJypYti3LlyqFIkSI4ePAgGjdunOznTpgwAaN1Ox4gMzsBAQEpGisRZVBxt3rrNG0K/PCDNPdMSGCgNIS6f196TK1ZA3TokPA9K1ZIrZ6U9rV65x0pcFi9OnD+fMqeFUcNBOIYagBgcUCyHxmqN1bhwoWRK1cuXLt2DY0bN4afnx8e6qaA/xMdHY2nT5/Cz8/P7HNcXFyMEp2JKJOJipJ2DfG/0WfOBMaPBxwSqMyhlAQao0bJc0qWlA6YCRXqUwr46CNgdvK2fhtYsECWwHLlSvmz4qiEU/gLsU1NWRyQ7EWGCnbu3r2LJ0+eIM9//wusWbMmnj9/jlOnTqHyfyXX9+/fD61Wi+rVq9tyqESUnpkqEghIbZomTRK+99Ur4P33pTggAHTqBKxaJQ06zYmMBPr2BdatS/6YdXbulBo/RYum/FlxtMCv+kCHxQHJ3ti0N9aLFy9w5swZnDlzBgBw48YNnDlzBrdv38aLFy8wbtw4/Pnnn7h58yb27duHNm3aoGjRomj237bKkiVLonnz5ujfvz+OHz+OP/74A0OHDkWXLl2QN29eG34yIkq3Pv3UONCpWlU6WCYW6Fy7Jv2kvvtO9lnPnSvLXQkFOiEhQIsW1gl0jh8HFi5MdvNOc0rjPH5DCwAsDkh2Ko12h5l04MABo6KBAFSvXr3Uq1evVNOmTVXu3LlVlixZVIECBVT//v1VcHCwwTOePHmiunbtqtzd3ZWHh4fq06cPiwoSkbGoKKWyZzfeXj10qFIREYnfv22bUp6eco+Pj1IHDyZ+z+3bpgsTJufnp5+su638v5/qfjdZHJAyLEu/vzVKJWcLgH0JDQ2Fp6cnQkJC4OHhYevhEJG1nT8PlC1rfPy774Du3RO+NyYGmDYNmDFDXteqBfz4I5BYT8CzZ6W7+L17yRqygd69JfnZ2h48QIy3D4sDUoZl6fd3hsrZISJKsqlTgY8/NjxWrJgkFJcpk/C9T54A3boBu3fL62HDpJaOs3PC9+3dK2WGw8KSP26d7NlTJdCJeR4GR093OILby8n+MdghIvsUGQm4uhrXr2nfXgoFJjaLe+qUbCO/dUuacn79deKzQADw7bdAr17JH3d81giY4rgDf5zYdBvtPc20ryCyQzZNUCYiShWnT0uPqPiBzhdfAJs2JR7orFoF1K4tgU7RosCffyYe6Cgl29atGehY2bfZBuHE5jto34GBDmUunNkhIvsyblxs2wYdX1/ZNVW/fsL3hofLUtXKlfL6rbdkpiax4oLR0VJFefPmZA87tV0dtgjd5w9jPg5lSgx2iMg+vH4NZM1qfLxOHUkoTqxC3q1bsmx16pTsv54xI/HiggDw4kXCW8/Tg+bNUWzRMFuPgshmuIxFRBnf0aOmA53Ro4H9+xMPdHbvBipVkkDH2xv47Tdg4sTEA51Tp9J/oDNrlhQiJMrEGOwQUcallFQzrl3b8Li7O7BxoxT9y5LF/P1areTZNG8uncOrVJEApmnThN9Xq5VWEVWqpPwzpKZt26QbO1Emx2UsIsqYQkJM59KUKiXbyk11MY/r+XOgZ09g+3Z53b8/sGiR7OBKyD//yHvExFg2TldXyQVKa7//bhwEEmVSnNkhooxnzx7TgU6XLsCxY4kHOmfPyqzM9u2ya2vlSuCrrxIOdKKipNVE8eKWBzoFC9om0AkMZKBDFAdndogo49BqZQv4hg2Gx52cgHnzgKFDY5s7mfPdd8CAAZLQXKCA7KD6r5GwWadPA/36Af/18bNI3rzAzZuWX28l2+ddRfbwoqgbw0rIRDoMdogoY3j4ULaQx5cvn+y2qlUr4fsjI4ExY4AlS+R1s2bSnNPb2/w9r18D06cDc+YY1+xJzP37SbveCrzxGE9Hy+fx95eeoe3bp/kwiNIdLmMRUfq3davpQKdhQ5l1SSzQuXdPeiLoAp3Jk4Fffkk40Dl0CChfHvjss6QHOjbggnA8ReznuXcP6NhR0peIMjsGO0SUfkVFAa1bm56eGD9etoz7+CT8jIMHZVt5YCDg6Sl5Oh9/bH6NJyREdng1aABcvZrST5AmHBCDSLgYHNPFZyNHWp5iRGSvuIxFROnTzZtAoULGxz09gbVrgTZtEr5fKcnj+fBD+bYvV06mOYoUMX/Ptm3AoEE2WYJKLg3MzzopBdy5Axw5wmaflLlxZoeI0p+1a00HOuXKASdPJh7ohIUB77wDjB0rgc6778rMjrlA58EDoHNneW4GCXT+QK0EA524goJSeTBE6RyDHSJKP169AurVA3r3Nj7Xs6cELEWLJvyMS5eAatWk4WeWLMDSpdLfylSFZaUksCpZUpKcM4jP8AHq4A+Lr0+sgDSRveMyFhGlD+fPA2XLGh93dgYWL5aif4ltK9+0CejTR/pV5c0rr2vWNH3tzZuSm7N7d4qHnpb6YSW+QT+LrtVoZFdW3bqpPCiidI4zO0RkW0oBCxaYDnTy55dKwAMGJBzoREdLt/NOnSTQadBAdmmZCnRiYmRPdpkyGS7QCcDtJAU6gPxqWW+HMjvO7BCR7Tx7JoHJ2bPG5yypgwNIvk2XLrLrCpCgZ9YsKTQY34ULUhzw2LGUjjzNeSAEYfCw+Hp/fwl0WGeHiMEOEdnK0aPmWxpMmSI/iU1JBAZKMZn796X555o1QIcOxtdFRACzZ0sQFBWV4qGntSyIRDQSaGgah5eXpB81aMAZHSIdLmMRUdqKiQGmTTMd6OTMKcX+pk9P+JtaKUk8rl9fAp2SJYETJ0wHOn/+KXV2pk83DHQcUvB/fzlzJv/eJHiJrNBAa1Ggo9HIz9dfA40bM9AhiovBDhGlnaAg4I03JPCIr3JlybNp2TLhZ7x6JTuzhg6V4KVTJ1mWKlHC8LoXL6SiXq1awMWLhufq1JGZoOTw85Plt1Q2D6PgjpcADHOV5s+XmRt/f8Pr/f0lH5vLVkTGuIxFRGlj507zgUz//sCiRQl3HQeAa9dk9ubsWZm6mDMHGDXKOHl5925Jar51y/B44cJAt26SzPLiRfI+R3Bw8u5LgobYj4NoaPKcr6/Ed+3bS7HAoCDZWl63LmdziMxhsENEqSsyUor7LV5sfM7VFVi2TLaLJ2b7dqBHD2nn4OMj0xv16xte8/QpMHq01M6JS6MBRowAiheXCsnpWElcxGWUNHteVzPH0ZFVkYksxWUsIko9164BAQGmA53ChSXBOLFAJyZGGne+/bYEOrVqyXJX3EBHKQl+SpY0DnSKF5cpkFy50nWgE1qsEirlCcIVjelAR6ORXyVr5hAlHYMdIkod69cDxYoBDx8an2vdWto+VKiQ8DOePJGlrxkz5PWwYcCBA0C+fLHX3LsHtG0r7R7ivpeDg/TFOnlSKihPmpTST5QqLqIkvPEYv0w/hUlL/AAYr8qxZg5RyjDYISLrevFClpu6dzc+5+AAzJwJ/Pxz4juaTp6UpOXduwE3N+C77ySvx9lZzmu1wJdfAqVKSQPPuMqUkV1YkyZJEPTVV9b5bFZ2EPVRHcfwFN7Ik0fycDZtMozlACYfE6WURillWSc5OxYaGgpPT0+EhITAw8Pyol1EFM+ZM9LbKizM+FyuXMD33wNNmiT+nFWrgCFDpD5O0aLA5s3SBFTn6lVJaj50yPA+Jydg4kT5ef4caNhQemWlJj+/ZCctu+EVIjRu8PcHbtyInbWJiWHyMZElLP3+ZoIyEaWcUsCSJcDw4abPV68ObNwoSScJCQ+XpaqVK+X1229LDk6OHPI6OhqYO1fq9ISHG95bsSLwzTeyNHbliuTvpOJ/y6nadfA0yh3ex39L1v0V8BciNG4AjJenmHxMZF1cxiKilHnyBHjrLfOBzpAhwOHDiQc6t25J/ZuVKyVJZeZMYOvW2EDnr7+km/n48YaBjrOzXHvsmAQ6f/whNXdSMdD546Nfce7462QHOnVxGH+jApeniNIIZ3aIKPkOHQLefNN0C4asWSVXxlTuTny7dwNdu8rWcW9vSW5u2lTOvX4tRQi/+ELWd+KqVk1mc0qXltebNkkRmtRStSr2dfkajcdUSPYj3BEGV2937P2BLR2I0gqDHSJKupgY4JNPTFdCBqRK8ubNkiicEK1WelZNniwzMVWqSMBSoICcP3wYeO89ydGJy9VVdmiNHBkbLXz6KTBhQoo+VkKuDZ6HO25vpCjQ0UALQIOXT2TYDHSI0gaXsYgoae7elcRfc4FO+/bSpyqxQOf5c9kyPmmSBDr9+0tWboECUk9n4ECppRM/0KlbVyoojxkj0YJWK7M5qRToKI0GnXPtw7/LdqLh3NbJesYa9IIGCnFbPwQFWWmARJQozuwQkeW2bQPatDF9ztFRZlfGjDEuFBPf2bMSFF2/Dri4SFPPfv1i32PwYKmfE1e2bPL8wYNjm3i+fi3FCVOphcPzAuUw4NYk/Pi4cbKf4YMHeAQfo+O6SshElPoY7BBR4sLDgQ8+MF0JGZCGTT/8YNy+wZTvvpO+Va9fyyzO5s1ST+fhQ0ly/uEH43saNZLE5UKFYo89eSLb2VOJav0Wft/ngh/xTrKf4YQoxMT7v1mNRurmsBIyUdphsENECbtyRQrz/f236fN16kiAkjdvws+JjJRZnyVL5HWzZsC6dYCXl1Q4HjVKEpTjyp5dtpq/957hbNGVK8Zdzq2pdGnglx1onYIdXQ6IgYqXKcBKyES2wZwdIjJNKWDNGgkqzAU6o0YB+/cnHujcuydbj3SBzuTJwC+/SPHBFi2AXr2MA50WLYALFySXJ26gs2NH6gY6AHDhAjTJDHQm4RM4Itoo0AFYCZnIVjizQ0TGQkOlaeb69abPu7vLlm9LtnkfPBjbt8rTU5axWrSQwOejj4CXLw2vz5FDpj569jTO/Rk3Tragp1NeeIJn8DI4ljs3MH++tIBgJWQi22CwQ0SGTp4EunSR5GFTSpYEtmxJfHZFKWDePGnGGRMj7R62bJEWEHXqSO+q+Nq0AZYvN529m1DSc968wP37CY8nlbniNSLganT80SMJdFgRmch2uIxFREKrlfyYqlXNBzqdOwPHjyce6ISFAe+8A4wdK4HOu+9KAcLvvpMqx/EDnVy5gA0bpGJy/EAnPDzhQMfBwaaBzgtkkx5XJgIdHW4zJ7ItBjtEJEtMrVpJcGKKk5N0HP/+e1nCSsilS1LZeNMmIEsW2VY+dKjM5kybZlxtuXNn4OJF+Wf8oGbjRul4nhCtNuHzifH3lyKIyXB1yAL44CHCkfAYuc2cyLa4jEWU2e3bJzMv5mrV5M0rQUetWok/a9MmoE8f4MULuW/tWklEHjrUuFeVr68sWbVrZ/ycly8TD6oS4+Ym29sTMm6cFDI0taSWmFevUNjZDd4/S/61qXxmbjMnSh84s0OUWUVFSYJwkybmA52GDaUBZ2KBTnS0zAp16iSBToMGwJw5spNqwQLjSKBnT5nNMRXo7NyZskDHyQl4/30pQmjOW29JlefPP09eoPP6NeDmBkdHYOFCORR/UorbzInSD87sEGVGN28C3boBgYHmr/nwQ+k/5ZTI/008eCBLUIcOyet+/STP5t13ja/Nl0+ag7ZsaXwuOFiCoD17LP4YRiZMkCW0nj0lb8iUnTulSWnVqkl/fq5cks/kGpuf0769TGiNGCGdNHT8/SXQ4TZzIttjsEOU2WzaJEX6QkJMn/fwkOWntm0Tf1ZgINCxoyQIu7tL3s/27ZIDFF///jKT4ulpeFyrBb7+WnphJVfbtjLFEhgoidGmurBXrSpLVg0aJG82Z9Qoma0yEfy1by8byY4ckWTkPHm4zZwoXVE2dOjQIdW6dWuVJ08eBUBt3brV4LxWq1WTJ09Wfn5+ytXVVTVu3Fj9888/Btc8efJEdevWTWXPnl15enqqvn37qrCwsCSNIyQkRAFQISEhKf1IROnXq1dKvf++UrKoZPqnXDmlrl5N/FlarVKLFyuVJYvc5+mpVPHipp9ZoIBSe/aYfs65c0rVrJnwmBL6cXZW6vhxedaCBeav27NHqUuXkv8+Bw9a60+BiKzI0u9vm+bsvHz5EuXLl8fSpUtNnp8zZw4WLVqEFStW4NixY8iWLRuaNWuG8PBw/TXdu3fHhQsXsGfPHuzYsQOHDx/GgAED0uojEGUMFy7IzMaXX5q/pkcPmRkpWjThZ716JctEw4bFzqCEhUkLh/iGDgXOn5e8oLhev5Z8obJlE15KS8iGDbJcVrmyLLmNHGn6usePZdanZMmkv0f27NKDy5KeX0SUfqVR8JUoxJvZ0Wq1ys/PT33++ef6Y8+fP1cuLi7q+++/V0opdfHiRQVAnThxQn/Nzp07lUajUffu3bP4vTmzQ3ZLq1Xqyy+VcnVNeHZkxQq5NjFXr8rsT2IzIUWLKnXokOln7N6tVOHCyZ9l6dpVqYgIeVZkpFING5q/du/e5L/PRx9Z9jshIpvJEDM7Cblx4waCg4PRJM5/EXp6eqJ69eoI/O+/BAMDA5EjRw5UqVJFf02TJk3g4OCAY8eOmX12REQEQkNDDX6I7M7z55I4/P77MgNiSv78wO+/yzUJFe4DJBenShXg7Fnz12g0wOjR0kurXj3Dcw8fStJy06bAv/8m6aPo3bsnLSycnWXGxdkZOHDA9LVvvGE8o2SpU6ckOTux3wkRZQjpNtgJ/m8rrK+vr8FxX19f/bng4GD4+PgYnHdycoKXl5f+GlNmz54NT09P/U9AQICVR09kY4GBUql440bz1zRtKl/qie1KiomRxp1vv20+qRmQqspHj0oV5qxZY49rtcCqVXJ+3bokfQw9f38Zh67h6OrVsjMqIf/8k6y3KpbvFbbcrJSse4kofUq3wU5qmjBhAkJCQvQ/d+7csfWQiKxDqwVmz5atQLdumb9uyhTg118TDxiePJFt4jNmmL/G0RGYOFHq8dSoYXju0iXZ/fTee8CzZxZ/DAOffALcvi1tIa5eldybvn2T96xEaKBw/b4bOnaUNl5EZB/SbbDj5+cHAHjw4IHB8QcPHujP+fn54WG8La7R0dF4+vSp/hpTXFxc4OHhYfBDlOEFBclszcSJMgtiSs6cUtF4+vTE90WfPCnJv7t3m7+mXDng2DFg5kyD2jMID5eAqnx52Y+dXN98A0yaJBWVJ0yQpanLl5P/PDO+Q3c4QZKtdfUPR440/2skoowl3QY7hQoVgp+fH/bt26c/FhoaimPHjqFmzZoAgJo1a+L58+c4deqU/pr9+/dDq9WievXqaT5mIpv57TcJLOL878VIpUqybGWqoF98q1bJ8pa52SEnJ+lzdeKEBERx7d8vQdAnn5iud2Op7duB3r2lH1eJEsCnnyb/WQkIwG30wHeIiVN2TCngzp2UxWlElH7YtKjgixcvcO3aNf3rGzdu4MyZM/Dy8kL+/PkxcuRIzJgxA8WKFUOhQoUwefJk5M2bF23/K3ZWsmRJNG/eHP3798eKFSsQFRWFoUOHokuXLsirW9snsmeRkbKF+4svEr7uvfeAxYsNZ19MCQ+XLeUrV5q/pnJlmXEpV87w+OPHwJgxwLffWjb2hBw9Ku0eGjQADh9O+fPMcEE4IuFi9jy7lRPZiTTaHWbSgQMHFACjn169eimlYosK+vr6KhcXF9W4cWN15coVg2c8efJEde3aVbm7uysPDw/Vp08fFhWkzOHaNaWqVjXeMu3oqJRGI//u6qrUqlWWPe/mTaXy5ze/FdvFRanZs5WKijK8T6tVavVqpby9k7/NW/eTM6dSv/+u1JAhSjk4pPx5Cfy44HWilx04YO0/NCKyJku/vzVK6VaoM6/Q0FB4enoiJCSE+TuUMXz/vWwXj9//KVs2KfqnFFCoELB5M1CxYuLP270baNbM/PkaNWQ2J35hvitXpM3DwYNJ/ghGiheXVg/LlklidCopi7M4j7IJXqPrVn7jBls+EKVnln5/p9ucHSIy4eVL2YnUrZthoJMli/ScevlSAp1WrSQ/J7FAR6sFBgwwH+i4uQHz5kktnriBTkQE8PHHspRljUDHwUGW5D75RAKdYsWkR1cCgvJYEMTF44UnFgU6ALuVE9kTBjtEGcWZM5Ivs3q14XFPT8nFCQmRoGHmTGDbNtl5lZCnT+Xb/OuvTZ+vX18KCI4aZfitf+iQJENPnSoBijVotTKN4ukpAc/r10ACxT6PoRryBP1l8eMvoQScEYFn8Er0Wn9/6ZXKbuVE9oPBDlF6pxSwZIksJcXvP+XvL7MsYWFSM2fXLtl67pDI/7Q3bAC8vU2fy5YNWLpUdlXF7ZP19CnQr58kDZvqg5VSfftKMvLkycDduyYv2Y7WAIDqOG7xY4diMUrhEqLgbHROo5Gf6dOlMPOBAxJzMdAhsi823Y1FRIl4+lSCgJ9/Njzu6irVhHVtF6pVk+mIxKqBR0ZK801z1YXffBP46iugYMHYY0pJ5ePRo4FHj5L9UcyqWlWCuRw5JG8nAW9hR5IeHXPtBjreKYjaQVKP8OuvDeMof39ZrmJwQ2TfGOwQpVdHjkhuTvxZDj8/ydHRBTpDhkiLBhfzW6gBSN5N3bqmz3l4SG5O376G/aCuXQMGDQL27k3+5zAnd26pndO7t9QHsmZtrJo1gd9/h6ODAxoUiT380Ufyaw0KAvLkkV8H83KI7B+XsYjSm5gYWVdp0MA40ClVSgr13bkjycPffSezIgkFOi9fSkNQc4FOq1bAhQuyRKULdCIjJfenTJnUCXRGjpTZpb59gTlzpPKzlVxv0E/q9JhYynN0lF9r167yTwY6RJkDZ3aI0pO7d6Uz+KFDhsfd3GT56a+/JNgpVky2lZdNeGcR9u6VpSlTcuYEFi0Cunc3nM3RdUG/eDFln8Wc8+eB0qXl3319pRu6lRTCv1g9tRCKJH4pEWUinNkhSi+2bZNdTvEDnYAAyWU5flwCnXbtpE1DQoHOs2dAnz7mA5327SWYeffd2EDn2TMJcurWTZ1AZ+JE2XVVurTkImk0Vgt0HiI3HKBFTEAhsxNYRJR5MdghsrWICGDECKBNGwkC4qpaVfpQnTkjay6ffy4zOp6epp+llCQq+/sDa9YYn8+dG/jxR7lG1yxXqdj+U199Zc1PFuvsWVkWA2TZzdxOsGRoj83w0zwENBq89558vIMH2cSTiGJxGYvIlq5cAbp0kWAmLhcXoGFDyT0JDZXlnh9+kNo35ty/L8nKP/1k+nzXrsDChRLw6Pz7LzB4sGxZTw05ckiXcl9f4OZNmY168cIqj96M9uiIzQAA7//K50ydGnve318+LndaERFndohsQSlg7VopEhg/0ClQAKhXTzqZh4YCtWsDp0+bD3SUksadpUqZDnTy5JHj69fHBjpRUcBnn0kCcmoFOo0bA7dvA15e8l6FClkt0IkJPA7vA5uxfr3kcj95Ytxh4t49oGNHYMsWq7wlEWVgnNkhSmthYbKde90643N16khvqz175PWoURIoZMli+lnXrkm7hwMHTJ/v3Vu2lMetphwYKLk5586l6GMkqHNnCeYCA2WGypqePoVjzpxoAFmqilsSKC6lJC1o5EhZIeTOK6LMizM7RGnp5EnpVxU/0MmSRZpgXr8uszju7pJ8Mm+e6UAnOlryd8qWNR3o5MkD7NwprSV0gc7z57JkVbt26gY6w4cDM2ZI4UNrBjr160t0EydwO3LEbLFlABLw3Lkj1xFR5sVghygtaLUSuNSqJQFNXPnzA506yXpLUJA03Dx+XI6ZcuaMtI744AMgPNz4/PvvS55M8+byWilg40Z57vLl8jq1vP++BHLFiln1sYXxL7YMP2hUOycoyLL7Lb2OiOwTl7GIUtujR7Kc9OuvxucaNpTdVuvXy+vOnSX/xt3d+NrwcGmS+dln5rca7dsHNGoU+/rmTUlaNvXeqeHLL63+SCdEQatxMrkclSePZc+w9Doisk+c2SFKTfv3S+2c+MGGo6NUDw4KkvwcJyfZOvT996YDnSNHgAoVgFmzTAc6ffpILpAu0ImOBr74Qhp5plWgY2WVcRIaKMTAyexyVN26susqbk3EuDQaKVPE2jtEmRuDHaLUEB0tjZiaNDFeQ8mXT2ZbfvxRlpvy5pXCMMOHG39rh4ZKnk29euY7jR85AnzzTWyQdPw4UKUKMG5c6hSbMVfjx0qmYSo00OI0Khudi/+rdHSUGBEw/tXpXi9YwORkosyOy1hE1nbrljTwPHrU+FyjRrKmsmiRvG7QANiwQerQxLdjh+zaMpeBW726JCe7ucnr0FBg0iRg8WKrfAyzQkJS7dHF8A+uwXy+j6nlqPbtpUbiiBHsaE5EpmmUSs1sxYwhNDQUnp6eCAkJgYeHh62HQxnZ5s3Ae+/Jzqe4HByAgQOlt1VgoBz78EPZteQU7785Hj2Sb+7vv5fXWbPKdvS4tm4F2raVf1dK6uhk4G911agxCl7ZhTv3HU3mT2s0ErzcuGF+liYmhh3NiTIbS7+/ObNDZA2vXwOjRwMrVhif8/WVQGf5cukF5eEhNWh0wYqOUrKTaeRIqZDn4CDbzuMHOmFhsUtWd+5Ivs6+fanxqdLGb79B06wZ5m+RIoAajeGGMUuXo3QdzYmI4mPODlFKXbgAVKtmOtBp0EC6is+YIYFO2bJSayd+oHPrFtCqFdCjhwQ6uXPLdvWIiNhrxo6VKMDdXXKC5s2TbesZOdB5+RJo1gxA7HJUvnyGl/j7y/EMPHFFRDbGZSxwGYuSSdemYcQImdmJS6MBhg2Trd/btsmxHj0kIMqaNfY6rRZYtgwYP16++J2dJYC5ds3weadOAZUqxf57lSqp9rHSxKefyjKeCVyOIiJLcRmLKDU9fy5tGjZuND6XK5cEL19+CVy9KgHMwoVScC/ulqFLlyS/R5fInD8/EBlpGOgULizBTY4c0leqa1dJXM7AauS5hT/G5oe5+IXLUURkbVzGIkqqP/+Ulg+mAp06dWS5afJkCXQCAmSaYuDA2EAnMlKKA1aoIIFOtmyyvHX7NhAcHPusjz+WZ+TIAXz7LZA9e8KBTrZs1vl8Xl7WeU48v6IFNNDiWFB+tm8gojTFmR0iS2m10o/qo49krcXBQY7pjBghycTjx8vrN9+Uysi5csVec/y4zOboelOVLCkdyOP3qtq1C2jaVJbBChVKeFxubjIDdOFCij8iAODpU+s85z/70RCfYDIOIrZPFts3EFFaYrBDZIngYKBnz9hu5FmzxvalypkTmDkTWLNGghkAmDJFfnTJJi9fyusFCyRA8vKS6sZnzxr2t6pUSXpk+ftLb6tduxIeV4cOspXdWoGOleXEUzxHTqPjbN9ARGmJwQ5RYnbtkkDn4UPZCp49e+zsR7Vqkrvz4YeyiypnTuC774CWLWPv37cP6N9fisQAklwcGhobGOn07y/FBjdtkmTmhHToIFvaly2z3ue0ot/QDK3wC7TxMnN09XLYvoGI0hKDHSJzIiOlIvHnn8trXS6LLtAZPlzyafr3l51ZFStKUUHdstOzZ8CYMcDq1fI6b17gjTdkJibulnJnZwlaGjaMrYZsjp+fbGNfskTeKx3qhTX4Fr2MjrN9AxHZChOUiUz5919JNtYFOkWKSIDy9KkUBVy5Erh+XZKIlYrdVaULdDZvBkqVkkBHo5HtRR4e0gMrbqBToIA0C127Vt4jId9/D/TrJ+915kwqfOiUOY/ScMVrk4EOwHo5RGQ7nNkhiu/772WbeFiYBCj+/sDFi3KuYkVJUB43TpalXFxkVqZvXzkfFCRNPrdulddFikhuzv79kogcV9Om0t+qTp2ExzN+PNCihezounTJup/VSmrhDwSilslzI0cCbdqwXg4R2Q6DHSKdly9laeqbb+R10aIya6MLdAYNki3i3bvL7EyhQjKDU7GiXLdqlWw7DwmRflfNm0vNHFNJxm++CezeLT/mFC0qM0FffAHUr2/1j5tSUXDCXIzBdExFOIyX3wIC2IiTiNIHBjtEgOyK6twZuHxZlp1q15ZZlCdPpD3D4sXAH38AgwfL9a1aAf/7nyQkX7smScoHDsi5cuWAggWBX36RLeqm6HZ1mXP6tOT81KsnS2qWmDZNfhLi72++i7pO/C31JnTTfI/vVRf964AA6V6RKxcrHxNR+sNghzI3pWQZaswYma3x85PaNwcPyrmyZYE5c2Tp6vRpCYQ++QSYMEECgi++kC3lr19LcvFbbwF//x3bIiKpPv1UlqvGjpW8IEusXw+cP594oFOunAR15uiCpUQCnT/XXceajoUx4CgDGyLKGBjsUOb19Kkk/P70k7yuVk3yanQzNH37yhbybt1klsXbW/J53nxTApr33pOmngBQq5YESps2SbDg5ib1cyxtPffGG/LMPXsksfn+/cTvcXKSthUVK0ql5cSYC3Tq15dmnBMnJnz/wIHA1Kmo4ecHgC0diCjj4G4sypyOHJF2DT/9JFu/O3YE7twB/vpLApVvvpH22x07SqBTrZrM7NStK7M8VapIoJMjB9CnD/DokRQD1GqBypVl6cvSQOfkSeD33yW4evttywKdjz+W2SR3d8sCHXOmTAHKlEk40BkzBnjwAFi+XAI6IqIMhjM7lLnExACzZsUu1xQtKlWLN2+WcyVKSGfyWbNik4cHD5aElBMnZAfVlStyvHlzKTC4Zo0ENnnzyqzP2rWWjUVXZfnHH+VZjx9bdt+vv0rwlSVLUj99LE9PGefkycatKnQmTpTlOnf35L8PEVE6wGCHMo9794B335V8HECSjF+8kGADkF1W/fpJ9eI7d2SG56uvZLZl9OjYasV+flLB+NdfY6si9+olW9UtCXR0W9EdHIB27YDt2y0bf8mSwM8/yyxL3H5bSVWmjFR5rlDB9PnBg4H582XGi4jIDjDYocxhxw6gd2/ZXZUtmwQn27dLUOPiIm0atFqZYYmMlIBkyxbpRF66dOwOpk6dJEl56VJ5nT8/MHWqBEmW2LABeOcdST4eMMDy8bdpI53P586VJaxkulOnKwK61jEf6OganBIR2REGO2TfIiKkb9XChfK6YkWgRg2ZsYmOlqBm7Vrgyy8lmABktuXzz2WJaf16OVa4sBSM2bAhNvAZNEi2IFkS6PTqJUthz5/L9m9L8nJ0pk2T5aYCBRLfNm5GNByxC83Q6vfvgd+/N33RH38w0CEiu8Rgh+zXP/8AXbpI0jEgMzsPH0qiLSCzNOPHy/Fz5+SL/tNPZS919eoyC+TgIOdfvJBt5oAEPiNGyE9i8uSRejwNGkiys27nlyXc3eXeZs1SvK/bCTFohV9NnrvfegCON5uMHJH+qBvDLeREZH8Y7JB9+vZbyT15+VLyW4YNk0TiGzcksXfePJlhadhQOpD7+Mhszg8/SC4OIHVp3n5blpyCg2X5qksX2QVlSaDzwQfSSPSLL4AmTRK/PiBAdnWFh8uM088/y3tmzZqiX0VCyud5iLM7cgM75LW/v0yCseoxEdkTzlmTfQkLkwTjXr0k0GnYUPpczZwpgU7BgsDhw7Ic1K6dBDo1a0rNnCFDJNBxdpaGTsWKSYfx4GBZQmrZUursJDY7U6mSvIePj/TWsiTHpnt3yR8KD5e8oRMnpLFoqVJW+KUY+7XmJ3CAFmeDchscv3dPJqC2bEmVtyUisgmNUpYWA7FfoaGh8PT0REhICDw8PGw9HEquU6dk5uXaNVl+GjtWApyNG+X8229LNeRBg2ILBzZvLsUFjx+X17Vry/byJUtklsXBQZKQX7+WXVAJcXOLzePRdUtPTI8e8lzdNvfx4yXA8veXIMsC/6AY3oBltXZOoyKa4zeoXD5md7prNPL2N25wSYuI0jdLv7+5jEUZn1LScfLDD6UCckCABA0LF0rejpOT5OLUqAE0aiTJwc7OQPHisgU8MlLyY0aNktydqVNjn+3gANy8mfgY8uSRKsjz5lk+7tWrJbC5fl2WqlavlqKFTpb9z3I4FuJDfGZxoFMJp3BGUwm5cgGPH5m/TimZZDpyhFWSicg+MNihjO3RI0kg1uXZtGsny1JjxsiSkL+/5OGcPCnf3NHRcp1SscX0WraUGZ0vvpBqyXHprk9MUJD8WKJ5c0mOHjpUltoKFpSlsfnzpRlpInbjTWxAF3wDy7a7j8ACLMJwaDQaALJitmBB4vdZ+nGIiNI75uxQxnXgAFC+vAQ6Li4SrLi7S2JweDjQooVMTyxaJAnFcQOXqChJXJ4zR9ZtPvrIONBJDZ9/Lq0m+vWTQKdRI2ksWqGCRQUJv802CE2xx6JAZ7rnPGigsAgjAGjg7y+tu9q0sWyoefJYdh0RUbqn0rGpU6cqAAY/xYsX159//fq1Gjx4sPLy8lLZsmVT7du3V8HBwUl+n5CQEAVAhYSEWHP4lFqiopSaNEkpjUYpQKmSJZXasEGpUqXktYODUrNmKXXhgpyTeRzDn65dlZo9WykPD9Pnrf2TN69Sv/2m1FtvxR6rVy/13u/VKxUdrdSBA0qtXy//jI6Wn717lfLyMn+rRqNUQIBcS0SUnln6/Z3ul7FKly6NvXv36l87xclnGDVqFH755Rds3LgRnp6eGDp0KNq3b48//vjDFkOltHD7tnQh1/0Zv/ceULWqNNF89UpaOWzYIPV0qleX+jhx6fJ5tm6Vvk/JlSWLzA5ZokUL2YLerx9w+XLs8cOHk//+ZpxYdRbX3MoizzFJ/4mbc7Nli0xwJVSX8L+VLixYwORkIrIjaRR8JcvUqVNV+fLlTZ57/vy5ypIli9q4caP+2KVLlxQAFRgYmKT34cxOBrF5s1I5csj0g4eHUmvWKNW/f+yUROPGSt25o9SoUaanLAYPltmcbNmSP2NSvLhSLVrEjiOxn1mzlNq2LdVnkA5MPaD8/Q0P+/vLr0z3q9NNhCX0ExAQew8RUXpn6fd3us/ZuXr1KvLmzYvChQuje/fuuH37NgDg1KlTiIqKQpM4xdpKlCiB/PnzIzAwMMFnRkREIDQ01OCH0rHXr6VAYIcO0m6hWjVp3jl/PvD11zIdMXWqFA3s0kWOx1W8uJw7f15mc16+TPoYatSQ9ypdGti5U8aREF9faTiq1cqW99T6O/bFF9iyMQaNPm5gNGOjq5mzcaPM6Chl/jHe3sDevbLdnAUFicjepOtlrOrVq2PNmjUoXrw4goKCMH36dNStWxfnz59HcHAwnJ2dkSNHDoN7fH19EZxIfZLZs2dj+vTpqThyspqLFyWA0e2c+uADqWzcqZMUEMydG1i3TraSV64sy1c6Tk7AuHFA9uyy3GXpzqq4ypWTIOfKFSk0+PRp4vc0biy9tkqXlt5cqWHvXqBxY8TEACMKmg5klJI4cMgQ2bSWkCdPZNmKS1dEZI/SdbDTokUL/b+XK1cO1atXR4ECBfDjjz/Czc0t2c+dMGECRo8erX8dGhqKgICAFI2VrEwpYNUqYPhwmdnx8ZG2Dbt2Ae++K9fUrSsVjdevlyAoripVpKjgggXAn38m/f1dXIALFyQ3Z+BAmc2xxJQpgKurtHtIBcfWXkb1nsX1r48cSTgHR6nEAx0dbjUnInuVroOd+HLkyIE33ngD165dw5tvvonIyEg8f/7cYHbnwYMH8PPzS/A5Li4ucHFxSeXRUrKFhAADBshSFSAVjadPl7o0p07JsQkTJJjp0wfYts3w/k8/leThLl2S9/7nzkmbhi+/lCAqfpKzKTlzAm+9ZVlriGSogUAcQw2szwJUj3PcmgEKt5oTkb1K9zk7cb148QLXr19Hnjx5ULlyZWTJkgX79u3Tn79y5Qpu376NmjVr2nCUlCLHjgEVK0qg4+QEfPaZzKw0by6BjpcX8MsvsiPL29sw0GnYUHZZLVoETJ6c9Pdev16mQlxcpP7N4MGWBTqAzD59+23S3zOxIaErNNDiGGoAMA5ILA1QcueO3WkVn0Yjm9Tq1k3BQImI0rF0HeyMHTsWhw4dws2bN3H06FG0a9cOjo6O6Nq1Kzw9PdGvXz+MHj0aBw4cwKlTp9CnTx/UrFkTNWrUsPXQKam0Wgls6tSJbdh54ID0jWrfXmZ7atQA/vpLekaVLWt4//LlsgW9XTtpB5EU/ftLQcFOnaQwYblywKFDSXtGeHjSrk/AJZTAMgxCSVxEd6wHoDEbkNStK0WiEwtkli2LfR3/PMCt5kRk59Jod1iydO7cWeXJk0c5OzurfPnyqc6dO6tr167pz+uKCubMmVNlzZpVtWvXTgUFBSX5fbj13MaCgpR6883Y/c+dOyt17pxSNWrEHhs9WqmwMKUqVDDcK12vnmztTs6W7aJFlTp0SMZw9qxSVaum6vbwxH5eevurvlilnBBlcEqjkR9zW8J128rjby2Pf9/mzcpoezq3mhNRRmbp93e6DnbSCoMdG9q1SykfH/nmdXNTauVKpX75JbbEr6enUlu3SjXk+AHCt98qVb9+8oKLSZOUev1aqYgIpaZOVSpLFtsFOjlzKvX550q9epXsgMTS+0xVVSYiyqgs/f7WKJVQ9Y3MwdIW8WRFUVFSVXjOHHldtqxsIV+/XhKMAdlK/uOPwNKlht3EfX2BadOAQYOS/r65ckmn87JlgRMnpPLy+fOmr61SRRqIphbd1vgPPgDiJNnHxMguq6AgycmpW9eyJabk3kdElFFZ+v3NYAcMdtLcv/8CXbsCx4/L68GDpUt5nz6xLRSGDJG2DvFLAsyYIUFScsyeLcFFRIRsEZ871/R1AwcCX30leUSpZeBAGQO3QBERJZul398Zaus52YEffpBt5aGhMpvxzTdS9K9GDSkIkz271NN5/Ng40NH1mEqOmzeBAgUk8fi994Br14yvqVlT6uOsWJG897DABnTGMr9PcGBJMc66EBGlkXS9G4vsyMuXEmR06SKBTu3awOnTwN9/Sx2dR49kF9TWrcCoUTKzo6Orm5RYYb9ChWRpKK7hw2WGJmdOWfZq0MB0oDN/PhAYCPzvfyn6mObsQlNUxkl0xQYcCS6GI0dS5W2IiMgEzuxQ6jt7FujcWTp+azTARx/JMk6vXoCuTlLfvlLIL06vM71E2n/AxQV4/32prxPXxYtAyZLAr78CvXubLiXcsqXk7owalayPlpjlGIjv0RVHUM/gOKsVExGlHQY7lHqUkvo3o0dLnkzevMB33wEODpL8GxwMZM0qgcauXbKklVTvvy/BVNxAp0IFCWBCQoAePeQ9TSlVSgKhVDAUi7EMg6HMTJ4yVYeIKO0w2KHU8fSpLFtt3SqvW7WSYGbVKsm70WqBYsWkg/nMmUl/ft26kufz+eeGx7/9Vnpnbdwos0kJuXgx6e+bgH1ohMOoh6/RH0HIa/IajUaKALJaMRFR2mGwQ9b3++/SzuHOHWmkOWcO0L27LFvp8m5KlQIiI2W7eVLkzAkMHAgV+Cc08QOdS5cAT0/JAdq71zqfxUJeeIJn8ErwGlYrJiKyDSYok/XExMjW8Pr1JdApWlSSfqtVAypVig10cuaUWRVTicIJ6dxZcntmz4bm4AH94S1ohxJ5Q3Fq8VFZKkvDQOdzjIUG2kQDHUBmdDZtku4XRESUdjizQ9Zx754sHx08KK979ACWLAG+/lrq5URHx1777FnSnl2kiPSv+uEH+YljNOZiK9rh2P0i8FlmIgE5lSzOPhFjw6YgEi6JXjtpEtC4MYv8ERHZCoMdSrkdO2S305MnQLZs0nXyrbeAnj2Bn39O/nOdnIChQ6Xa8kcfyczRfx7AB12wAfVwGDdQOOWfwUI7mi1Ci1+GYTCAUgeBd96R9CRTdPk506YxyCEisiUGO5R8EREya7NggbyuWBHYsEHq6FSqJIX8kqtWLelCvnixVFyO4w/UwlRMxwE0Sv7zk2ABRmASZuAl3IFdgH9BYOFCWY76+mugY0e5Lm4tcubnEBGlH8zZoeS5elUCEl2gM3Kk5Ofs3i0FA5Mb6Hh6ArNmydLVqFFGgc4yDMILuGMv3kzJ6C2yCR3ghCiMwgIJdP5z754EOFu2SMCzaROQL5/hvczPISJKP9gbC+yNlWT/+5/0s3rxAvD2BtasAerVk63mGzcm/7mdOwPVq0sPq/gFAN3cEFynA/z2mKmZYwXXUAQrMBCr0QdP4Z3gtbolqhs3ZOaGTTiJiNIeG4EmAYMdC4WFSRsHXUuF+vVl6/ijR7LklNTdVToFCwITJkh+j6kif76+wIMHyR62Ja6jMIrhqtkigOYcOCAdKIiIKO1Z+v3NZSyyzOnTQOXKEug4OAAffyytHn79FahaNWmBjrOz/NPRERg7Vqogjx5tvppxKgY6szABufAIRXE9yYEOwLYPREQZAROUKWFKSTbuBx/IrqiAAGD9emnJ0Lu3+VYMphQsKLk8kZGyXDV8uLR5OHZMzvv7y9amV6+s/znieQc/YCPeSfFz2PaBiCj948wOmff4MfD225IoHBUFtG0LnDkjScTZs1se6JQuLbM4N28CHh7A3LlSeKZXLwl0smeXZ794kbqBTrduQGgoDh5QFgU6uXLF7qqKT6ORuI9tH4iI0j/O7JBpBw9Ki4f796Wr+Lx5MpPTpInsurJE9uxSLfnCBXndsaP8TJ0KXLkCAFCt30KQJi/y/PwVNKmQPvbAIQ9yPf0Hjp7u+iTie/ckkHn82PQ9uuTjuXMlZ1qj4bZyIqKMjDM7ZCg6GpgyBWjUSAKdEiWAX34Bbt2SgoGWBjrVqklC8+3bQP78Mgvk7Q106SKBjp8fTg1aifs7zyDv9i+tHuh8iE+hgYKf9j6O/OWOLVtkFa1hQyn0nFCgA0gg06kTt5UTEdkD7sYCd2Pp3b4tszm//y6v69UDChSI3X1liTJlpC9WSIgkMo8aBZQvD3z4YWw2b9myiLh5Hy5hT6z+Ed7D11iNPtAidspl5EhJO7Lkb3pAgAQ6cQMZbisnIkqfuPU8CRjsANi6FejXz7BvVfz1m8QUKCAzQABQpYr0SfjmG6m+l4qWYAiGYTEA0wk2uXMbl+3R0WhkSWv+fJnBYSBDRJRxWPr9zZydzC48HBgzRvpZxWdpoOPsDGi1Eui4uwOffCLPbd3aumONa+pUxHR9FwUbF8G9+xrAxFB1gYy5QAeQj/jokQQ6rJdDRGSfGOxkZpcuyW6r5BYDBABXVwlsAMnTCQiQpStr8/CQPKI+fYBWrQBHRzgCWLhIcp7NJRF37x7b0SIhrJdDRGS/mKCcGSklLRlKlTIMdDw9k/4sXaADAMePA5s3p3x8/4nwzgMEB8t4Q0Jkqe3ttw3WmRLrTdWmjWXvxXo5RET2izk7yHg5OylKmD1+XAr6xZU1K1C4MHD+vNXHmhyzMBFr803ExVvZLO47Ze6amBjZhXXvnulVufg9roiIKONgzo6d2rIFGDECuHs39pi/v+w2MrsVWinZYfXOOzJTEteMGcCkSeki0OmJtdiArojWZMGmRRJ8WPp5HR1N59w4Osq1CS11sV4OEZF94zJWBrJli3xpx/3iB2TWomNHE5ueYmKkC3m1arKNPG6gM3myVDGeNCnVx52Q/2V7H8XwDzTQ4n/oCb+ALPoaNkn+vGYkttTFejlERPaNy1iw3TJWUpajdMsx8b/4dQyWY8JfAqtXS9XjGzcMrntRqAxuVGyPsls+tu6HSYIVeB+fYDIc/PNh3jzZGm5u+cmiz2vhrAzr5RAR2RfW2UkCWwQ7SV2OOnhQqv8mxBfB2FB7CWqfXYYsYc+Mzu9w64jWrzelbODJ1BD7cRANELcWjm4ZydTsiiWfFwAOHOCWcSKizMrS728uY9lAcpZnEtoaXQKX8BX64xYKoMEfM40CHa2jE57DM00DnfexAq54DQ0UNFA4iIaIX/RPF2aPHCmzLnFZuhWcW8aJiCgxDHbSWEyMzOiYmk9L6MvfeGu0Qj0cwja8hUsohf5YCRdEIgh+Rs91iIlGDoRYY/jmNWoEDBmCg+N+gQYKX+F9RMA10duUku4SR44YHrd0Kzi3jBMRUWIY7KSxI0fM56EA5r/869aVZS4nROMd/IDjqIZDaIC3sANaaLAF7bAQw5EVr1L3A9SvL8UIf/8deOst4Isv5APt2wcsWQK0bJmsx8afodF9Xo3pDhDQaKR+Yd26yXo7IiLKRBjspLHkLs84vn6BHU0X4R8Uww/ogqo4iddwxTIMQkX8hSfwxggsgidCrT7mMLjj2JgfpSP6wYPSCb12bWDbNmk1EWebU2JBijnxZ2h0W8YB42dxyzgRESUFg500FBMDPHhg2bX6L/+gIGDiRCAgAOW/GYFCuInHDrkxBdORH7dxDNXxNyqgP1ameHznURr9sBKXURwAcAzV8Ba2wROh6PRjJ8TAfGQREyNx0I8/Av37ywyVJQFPQjM03DJORETWwN1YSJvdWKZ2X5mi31K94wIcF8wF1q0DIiPlZLFiwIgRiPEvgKC1u+C/dYlVxtbecx+2hsQmEOfAM+THbZxFOcRNKja388nUZ/P2ln8+eWL+fRPajRUXt4wTEZEprKCcjuh2XyUWVmqg0FAdwLpcX8Cx/E79cVWhAh7kLovwlzHIN34Ssrx4Dn8rjKsOfsdRTW2M6ANgQezx58iJ58hpdL2pJThzn+3pU/nn9OkSo129Cnz9tfFW+wULEp+hMVcdmYiIyBKc2UHqzuwkVhwPAJwQhY7YhIlZvkDZqNMG5yKye0OFvYArIlI8lgmYhddwQzhc8Q36wi/AGQsWAF5eSatpo5tpuXdPGpw/emT6+viF/zhDQ0RE1sSZnXQiod1X7gjDe1iJkViAArgNRBlf4xKWwDpQAnaiOY6jGvLhHraiHX5DcyiNI3LlAubPB7rkM6xW7O+feLPMunUtX44DDHeWNWjAGRoiIrINBjupRDeLsXmz8bm8uIdhWIyBWGGd+jd9+iDmi/k42eVzbN3jjv+hB+4jn/F1SmZh8uUzDDosbZb588+WLcfFx8J/RERkSwx2UoG52Y8yOIcxmItuWA9nU9M4FoiAM+rjENzwGnVxBM03D0Ct9n5wBPBvnxn4bE/izzAVfOh2PplqYbFgAdCmjSzHJWfRk4X/iIjIlhjsWJlxwq5CY+zDWHyB5tiVpGdtRnv8glY4jmqoiyM4jHq4iNL68wfRECUjgFr/vU5p1eH27SWoMZVXc/CgZUtXccVd/iIiIrIVBjtWFL8VRG38jsUYhoo4Y9kD6tYF+vQBGjTAwZsF0bFR7LbvCyhj8pa4gYuuoJ8luTfmmMurSepSFAv/ERFResGiglYUPxl5FibqA50reANLMRgvkA0AcBEl0SvXL9iySQuEhgJaLXD4sAQ7hQqhbj1NktslpGbV4aQuRbHwHxERpRcMdqwo/uzHcCzCUCxGCVxCCVzBUCxFdrzAsKEKDw9cxDfBLdG+gwbInt0oOklu4JJaVYctaQOROzfw3XeyRf3GDQY6RESUPthNsLN06VIULFgQrq6uqF69Oo4fP57mY4g/+/E3KmAphuIKShgc79Ahdit2QpIbuLRvD9y8KUHH+vXWCT4SC740GmDFCqB7d8s+GxERUVqxi6KCP/zwA3r27IkVK1agevXqWLBgATZu3IgrV67Ax8cn0futVVRQV0AwsZwZXZG9pDw3vRTjM7XTLCDAskrIRERE1mTp97ddBDvVq1dH1apVsWSJ9IrSarUICAjAsGHDMH78+ETvt2YFZd1uLMB0vRp7yGNJT8EXERFlXpZ+f2f4ZazIyEicOnUKTZo00R9zcHBAkyZNEBgYaPKeiIgIhIaGGvxYS2bo1K3bsdW1K5esiIgo/cvwW88fP36MmJgY+Pr6Ghz39fXF5cuXTd4ze/ZsTJ8+PdXGlFC9GiIiIkpbGT7YSY4JEyZg9OjR+tehoaEICAiw6nuwDxQREVH6kOGDnVy5csHR0REPHjwwOP7gwQP4+fmZvMfFxQUuLi5pMTwiIiKysQyfs+Ps7IzKlStj3759+mNarRb79u1DzZo1bTgyIiIiSg8y/MwOAIwePRq9evVClSpVUK1aNSxYsAAvX75Enz59bD00IiIisjG7CHY6d+6MR48eYcqUKQgODkaFChXw22+/GSUtExERUeZjF3V2UsqadXaIiIgobWSaOjtERERECWGwQ0RERHaNwQ4RERHZNQY7REREZNfsYjdWSulytK3ZI4uIiIhSl+57O7G9Vgx2AISFhQGA1VtGEBERUeoLCwuDp6en2fPceg6puHz//n1kz54dGo3G1sOxOV2vsDt37nArfjrBP5P0iX8u6Q//TNKf1PwzUUohLCwMefPmhYOD+cwczuwAcHBwgL+/v62Hke54eHjw/yzSGf6ZpE/8c0l/+GeS/qTWn0lCMzo6TFAmIiIiu8Zgh4iIiOwagx0y4uLigqlTp8LFxcXWQ6H/8M8kfeKfS/rDP5P0Jz38mTBBmYiIiOwaZ3aIiIjIrjHYISIiIrvGYIeIiIjsGoMdIiIismsMdkhv9uzZqFq1KrJnzw4fHx+0bdsWV65csfWwKI5PP/0UGo0GI0eOtPVQMrV79+7h3Xffhbe3N9zc3FC2bFmcPHnS1sPKtGJiYjB58mQUKlQIbm5uKFKkCD755JNE+yWRdR0+fBhvvfUW8ubNC41Gg59++sngvFIKU6ZMQZ48eeDm5oYmTZrg6tWraTI2Bjukd+jQIQwZMgR//vkn9uzZg6ioKDRt2hQvX7609dAIwIkTJ/Dll1+iXLlyth5Kpvbs2TPUrl0bWbJkwc6dO3Hx4kXMnTsXOXPmtPXQMq3PPvsMy5cvx5IlS3Dp0iV89tlnmDNnDhYvXmzroWUqL1++RPny5bF06VKT5+fMmYNFixZhxYoVOHbsGLJly4ZmzZohPDw81cfGredk1qNHj+Dj44NDhw6hXr16th5OpvbixQtUqlQJy5Ytw4wZM1ChQgUsWLDA1sPKlMaPH48//vgDR44csfVQ6D+tW7eGr68vVq1apT/WoUMHuLm54bvvvrPhyDIvjUaDrVu3om3btgBkVidv3rwYM2YMxo4dCwAICQmBr68v1qxZgy5duqTqeDizQ2aFhIQAALy8vGw8EhoyZAhatWqFJk2a2Hoomd62bdtQpUoVdOrUCT4+PqhYsSK+/vprWw8rU6tVqxb27duHf/75BwDw999/4/fff0eLFi1sPDLSuXHjBoKDgw3+P8zT0xPVq1dHYGBgqr8/G4GSSVqtFiNHjkTt2rVRpkwZWw8nU9uwYQNOnz6NEydO2HooBODff//F8uXLMXr0aEycOBEnTpzA8OHD4ezsjF69etl6eJnS+PHjERoaihIlSsDR0RExMTGYOXMmunfvbuuh0X+Cg4MBAL6+vgbHfX199edSE4MdMmnIkCE4f/48fv/9d1sPJVO7c+cORowYgT179sDV1dXWwyHIfwhUqVIFs2bNAgBUrFgR58+fx4oVKxjs2MiPP/6IdevWYf369ShdujTOnDmDkSNHIm/evPwzIQBcxiIThg4dih07duDAgQPw9/e39XAytVOnTuHhw4eoVKkSnJyc4OTkhEOHDmHRokVwcnJCTEyMrYeY6eTJkwelSpUyOFayZEncvn3bRiOicePGYfz48ejSpQvKli2LHj16YNSoUZg9e7ath0b/8fPzAwA8ePDA4PiDBw/051ITgx3SU0ph6NCh2Lp1K/bv349ChQrZekiZXuPGjXHu3DmcOXNG/1OlShV0794dZ86cgaOjo62HmOnUrl3bqCTDP//8gwIFCthoRPTq1Ss4OBh+nTk6OkKr1dpoRBRfoUKF4Ofnh3379umPhYaG4tixY6hZs2aqvz+XsUhvyJAhWL9+PX7++Wdkz55dv47q6ekJNzc3G48uc8qePbtRzlS2bNng7e3NXCobGTVqFGrVqoVZs2bhnXfewfHjx/HVV1/hq6++svXQMq233noLM2fORP78+VG6dGn89ddfmDdvHvr27WvroWUqL168wLVr1/Svb9y4gTNnzsDLywv58+fHyJEjMWPGDBQrVgyFChXC5MmTkTdvXv2OrVSliP4DwOTP6tWrbT00iqN+/fpqxIgRth5GprZ9+3ZVpkwZ5eLiokqUKKG++uorWw8pUwsNDVUjRoxQ+fPnV66urqpw4cLqo48+UhEREbYeWqZy4MABk98hvXr1UkoppdVq1eTJk5Wvr69ycXFRjRs3VleuXEmTsbHODhEREdk15uwQERGRXWOwQ0RERHaNwQ4RERHZNQY7REREZNcY7BAREZFdY7BDREREdo3BDhEREdk1BjtERERk1xjsEJFdiYmJQa1atdC+fXuD4yEhIQgICMBHH31ko5ERka2wgjIR2Z1//vkHFSpUwNdff43u3bsDAHr27Im///4bJ06cgLOzs41HSERpicEOEdmlRYsWYdq0abhw4QKOHz+OTp064cSJEyhfvryth0ZEaYzBDhHZJaUUGjVqBEdHR5w7dw7Dhg3DpEmTbD0sIrIBBjtEZLcuX76MkiVLomzZsjh9+jScnJxsPSQisgEmKBOR3frmm2+QNWtW3LhxA3fv3rX1cIjIRjizQ0R26ejRo6hfvz52796NGTNmAAD27t0LjUZj45ERUVrjzA4R2Z1Xr16hd+/eGDRoEBo2bIhVq1bh+PHjWLFiha2HRkQ2wJkdIrI7I0aMwK+//oq///4bWbNmBQB8+eWXGDt2LM6dO4eCBQvadoBElKYY7BCRXTl06BAaN26MgwcPok6dOgbnmjVrhujoaC5nEWUyDHaIiIjIrjFnh4iIiOwagx0iIiKyawx2iIiIyK4x2CEiIiK7xmCHiIiI7BqDHSIiIrJrDHaIiIjIrjHYISIiIrvGYIeIiIjsGoMdIiIismsMdoiIiMiuMdghIiIiu/Z/ATfi42zTv6YAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# question 31 >>  How is polynomial regression implemented in Python\n",
    "\n",
    "# Polynomial regression can be implemented in Python using libraries such as NumPy, Scikit-learn, and Matplotlib for visualization. Below is a step-by-step guide to implementing polynomial regression in Python:\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Step 1: Create or load the dataset\n",
    "np.random.seed(42)\n",
    "X = np.linspace(1, 10, 100).reshape(-1, 1)  # Independent variable (1D array reshaped to 2D)\n",
    "y = 3 * X**2 + 2 + np.random.randn(100, 1) * 5  # Dependent variable (polynomial relationship with noise)\n",
    "\n",
    "# Step 2: Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Apply Polynomial Transformation\n",
    "degree = 2  # Degree of the polynomial\n",
    "poly = PolynomialFeatures(degree=degree)\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "X_test_poly = poly.transform(X_test)\n",
    "\n",
    "# Step 4: Train the Polynomial Regression Model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train_poly, y_train)\n",
    "\n",
    "# Step 5: Make Predictions\n",
    "y_train_pred = model.predict(X_train_poly)\n",
    "y_test_pred = model.predict(X_test_poly)\n",
    "\n",
    "# Step 6: Visualize the Results\n",
    "\n",
    "# Visualizing the Polynomial Regression result (Training set)\n",
    "plt.scatter(X_train, y_train, color='blue')  # Actual data points\n",
    "plt.plot(X_train, y_train_pred, color='red')  # Polynomial regression curve\n",
    "plt.title('Polynomial Regression (Training set)')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.show()\n",
    "\n",
    "# Visualizing the Polynomial Regression result (Test set)\n",
    "plt.scatter(X_test, y_test, color='blue')  # Actual data points\n",
    "plt.plot(X_test, y_test_pred, color='red')  # Polynomial regression curve\n",
    "plt.title('Polynomial Regression (Test set)')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Output:\n",
    "# Training Set Visualization: The plot shows the original data points (in blue) and the fitted polynomial regression curve (in red) for the training set.\n",
    "# Test Set Visualization: Similar to the training set, but here the polynomial curve is plotted against the test set.\n",
    "# Key Points:\n",
    "# PolynomialFeatures: This is the key to transforming the input data into polynomial features (e.g., quadratic, cubic).\n",
    "# LinearRegression: Polynomial regression is built on linear regression, so after transforming the features, you use the same LinearRegression model to fit the data.\n",
    "# Visualization: Helps in assessing how well the polynomial regression fits the data and identifying potential overfitting.\n",
    "# Conclusion:\n",
    "# Polynomial regression in Python is straightforward to implement using Scikit-learn. By transforming the data into polynomial features and applying linear regression, you can model non-linear relationships. Visualization aids in evaluating the model's fit and detecting overfitting or underfitting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53587987-7f05-4b12-b827-4dbdc198a42a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
